<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>K8S一些进阶运维知识</title>
      <link href="/2025/08/15/K8S%E4%B8%80%E4%BA%9B%E8%BF%9B%E9%98%B6%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86/"/>
      <url>/2025/08/15/K8S%E4%B8%80%E4%BA%9B%E8%BF%9B%E9%98%B6%E8%BF%90%E7%BB%B4%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h1 id="一、基础组件"><a href="#一、基础组件" class="headerlink" title="一、基础组件"></a>一、基础组件</h1><blockquote><p>Master节点上有:</p></blockquote><h2 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h2><p>整个系统唯一入口，面向用户的kubectl，还是所有其他组件都必须走apiserver进行访问或修改集群状态<br>其实就是etcd的前端，提供watch机制，提供给其他组件通过watch api实时获取资源对象的状态，这是实现声明式api和控制器模式的关键</p><h2 id="etcd-可能单独部署"><a href="#etcd-可能单独部署" class="headerlink" title="etcd (可能单独部署?)"></a>etcd (可能单独部署?)</h2><p>强一致性分布式数据库，所有资源状态存储，操作资源其实就是操作etcd中的数据，高可用部署、监控、备份都是重中之重</p><h2 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h2><p>k8s 中一系列控制器 (Controllers) 的守护进程。这些控制器负责确保集群的当前状态 (Current State) 向用户声明的期望状态 (Desired State) 收敛。<br>常见的控制器有 node、rs、deployment、statefulset、job、service等</p><h2 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h2><p>负责将新创建、未分配状态的pod调度到适合的节点上，有评分优先级可以去官仓看</p><blockquote><p>worker节点上有</p></blockquote><h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2><p>节点网络代理，配合service实现集群内负载均衡、代理<br>配合iptables或ipvs这种系统基础设施维护网络规则</p><h2 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h2><p>节点Agent，控制面与节点沟通的桥梁<br>负责节点上pod的生命周期维护、上报、挂载<br>通过runtime与容器交互</p><h2 id="容器运行时"><a href="#容器运行时" class="headerlink" title="容器运行时"></a>容器运行时</h2><p>containerd、docker<br>原理， 从同道就开始玩容器</p><h1 id="二、k8s资源创建流程"><a href="#二、k8s资源创建流程" class="headerlink" title="二、k8s资源创建流程"></a>二、k8s资源创建流程</h1><h2 id="1-kubectl"><a href="#1-kubectl" class="headerlink" title="1. kubectl"></a>1. kubectl</h2><p>本地资源格式校验, 转换资源为http请求<br>获取鉴权信息(x509 token basic) 加入http请求<br>与apiserver协商api版本, (apiserver&#x2F;aps有openapi scahme)<br>提交http请求给apiserver</p><h2 id="2-apiserver"><a href="#2-apiserver" class="headerlink" title="2. apiserver"></a>2. apiserver</h2><ol><li>鉴权信息验证</li><li>Admission control验证(确保符合广泛的集群规则和限制) (包括资源管理 安全管理 默认值管理 引用一致性等类型)</li><li>post写入etcd, etcd中的key默认格式是namespace&#x2F;name</li><li>再get获取etcd中该key,确保写入成功</li><li>返回生成的http response</li></ol><p>另外apiserver通过list-watch监听etcd中的pod资源, etcd的key创建好会返回给apiserver一个create事件, apiserver会将pod信息更新到它自己的in-memory cache里</p><p>这一套走完, etcd中有了, 但是kubectl 还get不到</p><h2 id="3-Initializers"><a href="#3-Initializers" class="headerlink" title="3. Initializers"></a>3. Initializers</h2><p>etcd中有了, apiserver先不回将其设置为对外可见, 而是先运行Initializers<br>Initializers可以设置一些通用的启动初始化任务, 比如 (sidecar注入 端口暴露 私钥注入等)<br>可以通过创建Kind: InitializerConfiguration 来维护, 后面就会被追加到每个pod的 metadata.initializers.pending字段</p><h2 id="4-控制器"><a href="#4-控制器" class="headerlink" title="4. 控制器"></a>4. 控制器</h2><p>下一步是设置资源拓扑, 其实一个deployment就是一组rs, 而一个rs就是一组pod<br>k8s通过大量内置的控制器维护这个层级关系</p><p>K8s 中大量使用 “controllers”，</p><ul><li>一个 controller 就是一个异步脚本（an asynchronous script）</li><li>不断检查资源的当前状态（current state）和期望状态（desired state）是否一致</li><li>如果不一致就尝试将其变成期望状态，这个过程称为 reconcile。</li></ul><p>控制器处理完, etcd中会有对应拓补下所有资源, 例如 deployment * 1 + rs * 1 + pods * 2<br>此时kubectl可以get到, 但是pod是Pending的</p><h2 id="5-调度器-scheduler"><a href="#5-调度器-scheduler" class="headerlink" title="5. 调度器 scheduler"></a>5. 调度器 scheduler</h2><p>scheduler 会循环寻找所有 nodeName 字段为空的 pod，为它们选择合适的 node</p><p>优先级 首先是直接指定nodename 污点容忍 亲和性<br>符合后再通过 显式 Resource requests&#x2F;limits 过滤出可调度节点列表</p><ol><li>只看pod的request 不看limit</li><li>所有符合资源需求的节点加入list</li><li>通过内置算法给节点打分, 得分最高的 node 会被选中</li><li>请求创建v1.Binding对象</li><li>apiserver收到请求后, 更新pod的NodeName字段</li></ol><p>此时, pod已经被指定调度到一个节点, etcd中的元数据准备好了, 下一步就是将这个pod的状态同步到对应节点上.<br>接下来就是节点上的kubelet开始干活</p><h2 id="6-kubelet"><a href="#6-kubelet" class="headerlink" title="6. kubelet"></a>6. kubelet</h2><ol><li>通过 ListWatch 接口，从 kube-apiserver 根据spec.nodeName过滤属于本节点的 Pod 列表</li><li>与自己缓存的 pod 列表对比，如果有 pod 创建、删除、更新等操作，就开始同步状态</li><li>如果是 pod 创建事件，会记录一些 pod latency 相关的 metrics</li><li>生成一个 v1.PodStatus 对象 代表当前pod阶段的状态, 并且异步通过 apiserver 更新 etcd 记录</li><li>运行一系列 admission handlers，确保 pod 有正确的安全权限</li><li>创建容器数据目录, 比如默认的&#x2F;var&#x2F;lib&#x2F;kubelet下的pod目录, volumes目录</li><li>从apiserver获取image pull secert, 注入容器</li><li>runtime创建container</li></ol><h2 id="7-runtime"><a href="#7-runtime" class="headerlink" title="7. runtime"></a>7. runtime</h2><p>kubelet 从 v1.5.0 开始，使用 CRI（Container Runtime Interface）与具体的容器运行时交互<br>k8s集群版本从1.22 - 1.24(完全移除) 过渡中, 完全移除了docker作为容器运行时的支持, 必须使用cri兼容的运行时 如contaienrd<br>CRI 提供了 kubelet 和具体 runtime implementation 之间的抽象接口</p><ol><li>CRI create sandbox(pause) ,用于pod中容器的ns创建(资源隔离), 并作为Pod生命周期的锚点，确保Pod在各种状态下都能保持其网络环境和基本生命周期(也就是所谓的占坑)</li><li>调用 CNI 插件为容器设置网络</li><li>创建 init 容器及业务容器, pod内的容器对应挂载之前创建好的工作目录 \ 注入环境变量 secert等初始化任务</li><li>遵循pod的readiness liveness startup Probe 开始维护pod生命周期</li></ol><h1 id="三、升级"><a href="#三、升级" class="headerlink" title="三、升级"></a>三、升级</h1><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul><li>etcd备份</li><li>集群资源快照</li><li>pvc挂载情况快照</li><li>api弃用情况, 我们是通过每个版本的Changelog 加上历史维护集群资源使用的apiversion对比</li><li>helm 通过Helm MapkubeAPIs插件来检查</li><li>可以提前用kubectl convert工具来迁移api版本, 也可以手搓加深更新了解</li></ul><h2 id="升级策略"><a href="#升级策略" class="headerlink" title="升级策略"></a>升级策略</h2><ol><li>先挨个升master (master上没有业务pod)</li><li>worker 挨个 cordon排水, 升级, 取消污点恢复ready状态</li><li>资源检查 业务检查</li></ol><h1 id="四、进阶用法"><a href="#四、进阶用法" class="headerlink" title="四、进阶用法"></a>四、进阶用法</h1><h2 id="helm"><a href="#helm" class="headerlink" title="helm"></a>helm</h2><p>helm基于jinja2的Template , 配合argocd的自动化实现</p><h2 id="network"><a href="#network" class="headerlink" title="network"></a>network</h2><p>NetworkPolicy,podselector选择给哪个服务配置, Ingress和egress控制出入流量, 每种gress下还有namespace和pod的selector, 以及ipblock控制src或dst</p><h2 id="hpa"><a href="#hpa" class="headerlink" title="hpa"></a>hpa</h2><p>通过容器指标(一般通过summary-Exporter暴露pod cpu\内存基础指标)<br>自定义指标, 例如Springboot exporter这种是需要通过Prometheus Adapter进行转换, 转换为kubernetes metrics api才可以使用</p>]]></content>
      
      
      <categories>
          
          <category> 云原生 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> k8s </tag>
            
            <tag> cka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux进程与CPU相关基础</title>
      <link href="/2025/08/15/Linux%E8%BF%9B%E7%A8%8B%E4%B8%8ECPU%E7%9B%B8%E5%85%B3%E5%9F%BA%E7%A1%80/"/>
      <url>/2025/08/15/Linux%E8%BF%9B%E7%A8%8B%E4%B8%8ECPU%E7%9B%B8%E5%85%B3%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Linux-线程-进程-协程"><a href="#一、Linux-线程-进程-协程" class="headerlink" title="一、Linux 线程 进程 协程"></a>一、Linux 线程 进程 协程</h2><ul><li>进程： 对正在运行程序的抽象， 比如一个web浏览器、一个vscode 都是进程，进程是程序运行资源的集合，是系统资源分配的最小单位。</li><li>线程：一个进程环境中的多个执行流，这些执行流很大程度上相对独立，在进行中，程序执行的最小单位（执行流）就是线程。可以并行运行在进程中，就像他们是单独的”线程“一样 只是它们共享相同的地址空间、代码、数据、信号处理、打开文件、全局变量等<ul><li>线程栈1M</li></ul></li><li>协程：简历在线程之上的抽象，它需要线程来承载运行，一个线程可以有多个协程<ul><li>协程很小只有几kb</li><li>比如GO语言的goroutine，一个<code>go</code>关键字就可以运行一个协程程序，go语言中协程通过go提供的runtime来控制和调度，通过context包进行上下文传输。</li><li>协程通过语言提供的runtime来调度，用户空间直接调度不需要在内核空间和用户空间来回切换，效率低</li><li>能更好地利用cpu地多核，提高程序执行能力</li><li>避免阻塞，如果协程所在地线程发生了阻塞，协程调度器可以把运行在阻塞线程上地协程 调度到其他线程上继续运行。</li></ul></li></ul><p>Golang中地协程与线程的关系</p><p>go中的协程相当于一个微线程，由Go Runtime调度使用。 goroutine的协程都是运行在线程上的。</p><p>它的调度模型是一个GMP模型：</p><ul><li>G： goroutine，表示go的一个协程，也就是”微线程“</li><li>M： machine， 表示线程，G在M上运行</li><li>P： processor ，它包含了运行goroutine所需资源，如果一个M想运行一个goroutine，那么要先获取 processor</li></ul><h2 id="二、Linux僵尸进程"><a href="#二、Linux僵尸进程" class="headerlink" title="二、Linux僵尸进程"></a>二、Linux僵尸进程</h2><p>ps 看到STAT为 Z 或者Zs的进程</p><p>简单来说：僵尸进程的出现时间是在子进程终止后，但是父进程尚未读取这些数据之前</p><p>当你运行一个程序时，它会产生一个父进程以及很多子进程。 所有这些子进程都会消耗内核分配给它们的内存和 CPU 资源。<br>这些子进程完成执行后会发送一个 Exit 信号然后死掉。这个 Exit 信号需要被父进程所读取。父进程需要随后调用 wait 命令来读取子进程的退出状态，并将子进程从进程表中移除。<br>若父进程正确第读取了子进程的 Exit 信号，则子进程会从进程表中删掉。<br>但若父进程未能读取到子进程的 Exit 信号，则这个子进程虽然完成执行处于死亡的状态，但也不会从进程表中删掉。</p><p>那么此时如果父进程是一个循环不会结束，子进程就会一直保持僵尸状态。</p><p>僵尸进程不做任何事情，不会使用任何资源也不会影响其他进程，因此也没什么坏处， 但是进程表中的退出状态以及其他一些进程信息是存在内存里的，太多也会是个问题</p><p>kill -9杀不掉子进程的， 必须杀掉父进程</p><h2 id="三、CPU上下文切换"><a href="#三、CPU上下文切换" class="headerlink" title="三、CPU上下文切换"></a>三、CPU上下文切换</h2><p>是指CPU从一个进程(或线程)切换到另一个进程(或线程)时，必须保存当前运行任务的上下文(状态)，并恢复下一个任务的上下文的过程。</p><p>上下文切换的类型：</p><ul><li>进程上下文切换：不同进程间切换，开销较大</li><li>线程上下文切换：同一进程的线程间切换，开销较小</li><li>模式切换：用户态和内核态之间的切换</li></ul><p>上下文切换会导致性能开销，因为：</p><ul><li>需要保存和恢复大量寄存器状态</li><li>可能导致CPU缓存失效(TLB、数据缓存等)</li><li>频繁切换会减少实际任务执行时间</li></ul><p>在性能敏感的应用中，减少不必要的上下文切换是优化的重要方向。</p><p>如何诊断频繁上下文切换带来的性能问题？</p><p>确认上下文切换是否过高， 通过vmstat 1 关注cs列、通过dstat -c -y –top-cpu查看详情、pidstat -w 查看每个进程的上下文切换情况</p><ul><li>正常情况上下文切换次数在每秒几千次以内</li><li>超过1万通常表示可能存在问题</li><li>超过10万基本是肯定有问题的</li></ul><h2 id="四、Linux-CFS调度器"><a href="#四、Linux-CFS调度器" class="headerlink" title="四、Linux CFS调度器"></a>四、Linux CFS调度器</h2><p>完全公平调度器， 2.6版本成为默认调度器取代之前的O（1）调度器</p><p>CFS的核心是”完全公平“地分配CPU时间给所有可运行地进程。与传统的基于时间片的调度器不同，CFS不是直接分配固定的时间片，而是通过虚拟运行时间（vruntime）的概念来实现公平性</p><p>如何实现的公平：</p><ol><li>虚拟运行时间（vruntime），每个进程维护一个vruntime变量，表示该进程已获得的cpu时间，但经过优先级加权后的值。CFS总是选择vruntime最小的进程来运行，vruntime的计算公式：<ul><li><code>vruntime = 实际使用的CPU时间 * （优先级为0（默认优先级）的权重 / 基于进程优先级的权重）</code></li></ul></li><li>红黑树数据结构， cfs使用红黑树来组织可运行进程，以vruntime为键值。这允许：<ul><li>高效地找到vruntime最小地进程</li><li>高效地插入和删除进程</li></ul></li><li>调度周期， cfs定义了一个调度周期通常为几毫秒，目的是让所有可运行进程在这个周期内都能运行一次，每个进程分配的时间为：<ul><li><code>time_slice = sched_latency * (weight / total_weight)</code></li></ul></li><li>动态时间片，cfs不采用固定时间片，而是：<ul><li>当进程数少时，每个进程获得更长的时间</li><li>当进程数多时，调度周期自动扩展，但不超过sched_latency_max</li></ul></li><li>优先级处理，通过<code>nice</code>值影响进程的权重：<ul><li>高优先级（nice值小）的进程获得更多CPU时间</li><li>低优先级（nice值大）的进程获得较少CPU时间</li><li>权重差异被严格控制（避免优先级反转问题）</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> Ops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ISO七层模型与TCP连接基础</title>
      <link href="/2025/08/15/ISO%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%E4%B8%8ETCP%E8%BF%9E%E6%8E%A5%E5%9F%BA%E7%A1%80/"/>
      <url>/2025/08/15/ISO%E4%B8%83%E5%B1%82%E6%A8%A1%E5%9E%8B%E4%B8%8ETCP%E8%BF%9E%E6%8E%A5%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id="一、ISO7层模型"><a href="#一、ISO7层模型" class="headerlink" title="一、ISO7层模型"></a>一、ISO7层模型</h2><ol><li>物理层</li><li>数据链路层</li><li>网络层</li><li>传输层 （TCP、UDP）</li><li>会话层</li><li>表示层</li><li>应用层</li></ol><h2 id="二、TCP三层（传输层）"><a href="#二、TCP三层（传输层）" class="headerlink" title="二、TCP三层（传输层）"></a>二、TCP三层（传输层）</h2><h3 id="2-1-TCP三次握手"><a href="#2-1-TCP三次握手" class="headerlink" title="2.1 TCP三次握手"></a>2.1 TCP三次握手</h3><p>建立一个 TCP 连接需要“三次握手” 就能确认双方收发功能都正常， 缺一不可：</p><ul><li><strong>一次握手</strong>:客户端发送带有 SYN（SEQ&#x3D;x） 标志的数据包 -&gt; 服务端，然后客户端进入 <strong>SYN_SEND</strong> 状态，等待服务端的确认；</li><li><strong>二次握手</strong>:服务端发送带有 SYN+ACK(SEQ&#x3D;y,ACK&#x3D;x+1) 标志的数据包 –&gt; 客户端,然后服务端进入 <strong>SYN_RECV</strong> 状态。</li><li><strong>三次握手</strong>:客户端发送带有 ACK(ACK&#x3D;y+1) 标志的数据包 –&gt; 服务端，然后客户端和服务端都进入<strong>ESTABLISHED</strong> 状态，完成 TCP 三次握手。 其实一旦完成了前两次握手，TCP 协议允许数据在第三次握手时开始传输。如果第三次握手的ACK确认包丢失，但是客户端已经发送了携带数据并且包含ACK标记的包，服务端会将其视为有效的第三次握手确认并正常数据传输。</li></ul><ol><li><strong>第一次握手</strong>：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常</li><li><strong>第二次握手</strong>：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常</li><li><strong>第三次握手</strong>：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常</li></ol><p>tcp三次握手过程中内核维护两个队列管理连接请求</p><ul><li><p>半连接队列：服务端收到客户端的syn请求时双方还没有完全建立连接，它会把这种半连接的状态的连接放到半连接队列中</p></li><li><p>全连接队列：服务端收到客户端ACK相应时意味着三次握手完成，此时服务端会将半连接状态中的该连接移动到全连接队列中。 如果没有收到客户端的ack响应 将会进行重传，重传的等待时间是指数增长的， 如果超出内核配置的最大连接数（tcp_syn_retries、tcp_synack_retries、tcp_retries2），系统将从半连接队列中删除该连接信息。</p><ul><li><p><code>tcp_syn_retries</code>  控制客户端在发起TCP连接时，发送SYN报文的最大重传次数。默认值通常为5。</p></li><li><p><code>tcp_synack_retries</code> 控制服务端在收到客户端的SYN报文后，发送SYN-ACK报文的最大重传次数。</p><p>默认值通常也为5。</p></li><li><p><code>tcp_retries2</code> 控制普通数据包在没有收到确认的情况下，TCP层进行重传的最大次数。默认值通常为15</p></li></ul></li></ul><h3 id="2-2-四次挥手"><a href="#2-2-四次挥手" class="headerlink" title="2.2 四次挥手"></a>2.2 四次挥手</h3><p>断开一个 TCP 连接则需要“四次挥手”，缺一不可：</p><ol><li><strong>第一次挥手</strong>：客户端发送一个 FIN（SEQ&#x3D;x） 标志的数据包-&gt;服务端，用来关闭客户端到服务端的数据传送。然后客户端进入 <strong>FIN-WAIT-1</strong> 状态。</li><li><strong>第二次挥手</strong>：服务端发送一个 ACK （ACK&#x3D;x+1）标志的数据包-&gt;客户端 。然后服务端进入 <strong>CLOSE-WAIT</strong> 状态，客户端进入 <strong>FIN-WAIT-2</strong> 状态。</li><li><strong>第三次挥手</strong>：服务端发送一个 FIN (SEQ&#x3D;y)标志的数据包-&gt;客户端，请求关闭连接，然后服务端进入 <strong>LAST-ACK</strong> 状态。</li><li><strong>第四次挥手</strong>：客户端发送 ACK (ACK&#x3D;y+1)标志的数据包-&gt;服务端，然后客户端进入<strong>TIME-WAIT</strong>状态，服务端在收到 ACK (ACK&#x3D;y+1)标志的数据包后进入 CLOSE 状态。此时如果客户端等待 <strong>2MSL</strong> 后依然没有收到回复，就证明服务端已正常关闭，随后客户端也可以关闭连接了。</li></ol><p><strong>第一次挥手</strong>：A 说“我没啥要说的了”</p><p><strong>第二次挥手</strong>：B 回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话</p><p><strong>第三次挥手</strong>：于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”</p><p><strong>第四次挥手</strong>：A 回答“知道了”，这样通话才算结束。</p><h3 id="2-3-常见问题"><a href="#2-3-常见问题" class="headerlink" title="2.3 常见问题"></a>2.3 常见问题</h3><p><strong>为什么不能把服务端发的 ACK 和 FIN 合并起来，变成三次挥手？</strong></p><p>因为服务端收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复 ACK，表示接收到了断开连接的请求。等到数据发完之后再发 FIN，断开服务端到客户端的数据传送。</p><p><strong>如果第二次挥手时服务端的 ACK 没有送达客户端，会怎样？</strong></p><p>客户端没有收到 ACK 确认，会重新发送 FIN 请求。</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> Ops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch运维进阶</title>
      <link href="/2025/08/15/Elasticsearch%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/"/>
      <url>/2025/08/15/Elasticsearch%E8%BF%90%E7%BB%B4%E8%BF%9B%E9%98%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="高可用架构"><a href="#高可用架构" class="headerlink" title="高可用架构"></a>高可用架构</h1><h2 id="一、节点类型"><a href="#一、节点类型" class="headerlink" title="一、节点类型"></a>一、节点类型</h2><h3 id="1-1-主节点"><a href="#1-1-主节点" class="headerlink" title="1.1 主节点"></a>1.1 主节点</h3><p>协调集群内管理任务，比如索引创建、删除、集群状态更新等</p><h3 id="1-2-数据节点"><a href="#1-2-数据节点" class="headerlink" title="1.2 数据节点"></a>1.2 数据节点</h3><p>存储实际数据，并执行相关的CRUD操作和搜索请求</p><h3 id="1-3-协调节点"><a href="#1-3-协调节点" class="headerlink" title="1.3 协调节点"></a>1.3 协调节点</h3><p>接收客户端请求，将请求分发到数据节点并汇总结果，不存储数据</p><h3 id="1-4-负载节点"><a href="#1-4-负载节点" class="headerlink" title="1.4 负载节点"></a>1.4 负载节点</h3><p>在数据被索引之前，对数据进行预处理，比如解析、变换等</p><h2 id="二、高可用"><a href="#二、高可用" class="headerlink" title="二、高可用"></a>二、高可用</h2><ol><li>ElasticSearch通过分片（Shard）机制来实现数据的水平扩展。每个索引可以被分成多个分片，每个分片是一个完整的搜索引擎，可以被分布在集群的多个节点上。为了提高数据的可用性和容错性，每个分片可以有零个或多个副本分片, 分片大于1即可满足基础高可用</li><li>ElasticSearch集群的高可用性是通过多个节点和分片副本来实现的。当一个节点发生故障时，集群能够自动将请求重定向到其他节点，并且可以自动将故障节点上的分片重新分配到其他节点上。</li><li>ElasticSearch使用Zen Discovery模块来进行节点发现和主节点选举。当集群启动或者主节点故障时，会进行一个新的主节点选举过程。这个过程确保了集群始终有一个主节点来管理集群状态。</li></ol><h2 id="三、故障转移"><a href="#三、故障转移" class="headerlink" title="三、故障转移"></a>三、故障转移</h2><p>当数据节点发生故障时，副本分片会接管请求，并且集群会自动将新的分片分配到其他健康的节点上。这个过程是自动的，无需人工干预。</p><h2 id="四、数据一致性"><a href="#四、数据一致性" class="headerlink" title="四、数据一致性"></a>四、数据一致性</h2><p>在分布式系统中，数据一致性是一个挑战。ElasticSearch通过以下机制来确保数据的一致性：</p><h3 id="4-1-乐观并发控制"><a href="#4-1-乐观并发控制" class="headerlink" title="4.1 乐观并发控制"></a>4.1 乐观并发控制</h3><p>ElasticSearch使用乐观并发控制（Optimistic Concurrency Control, OCC）来处理文档版本的冲突。每个文档都有一个版本号，当文档被更新时，版本号会增加。如果两个并发的更新尝试发生冲突，ElasticSearch会拒绝旧版本的更新。</p><h3 id="4-2-刷新与提交"><a href="#4-2-刷新与提交" class="headerlink" title="4.2 刷新与提交"></a>4.2 刷新与提交</h3><p>ElasticSearch的索引数据不是立即写入磁盘的，而是先存储在内存中，通过定期的刷新（Refresh）操作写入磁盘。这确保了数据的快速响应，但也意味着在刷新间隔内的数据可能会在故障时丢失。为了减少这种风险，可以调整刷新间隔或使用副本分片来提供数据冗余。</p><h2 id="五、分片"><a href="#五、分片" class="headerlink" title="五、分片"></a>五、分片</h2><p>一个index创建出来, 根据不同es版本有默认(主)分片数量, 7.0之前默认为5 , 7.x及之后默认为1<br>而且还会默认创建副本分片数量为1</p><p>比如一个index在7.6版本创建, 那么它默认一共会产生1个主分配 + 1个副本分片</p><h1 id="常见运维场景问题"><a href="#常见运维场景问题" class="headerlink" title="常见运维场景问题"></a>常见运维场景问题</h1><h2 id="一、分片与集群设计"><a href="#一、分片与集群设计" class="headerlink" title="一、分片与集群设计"></a>一、分片与集群设计</h2><p>如何计算主分片数？副本数设置依据是什么？</p><ol><li><p><strong>主分片数</strong>：</p><ul><li>计算公式：<code>max( min(节点数 × 1.5, 数据总量/50GB ), 3 )</code></li><li>示例：3节点+300GB数据 → <code>min(4.5, 6) ≈ 5分片</code></li><li><strong>硬规则</strong>：创建后不可修改，需预留20%增长空间</li></ul></li><li><p><strong>副本数</strong>：</p><ul><li>基础容灾：≥1（允许宕机1节点）</li><li>读性能提升：每增加1副本，读吞吐提升30-50%</li><li>特殊场景：<ul><li>单节点集群：必须设为0（副本无法分配）</li><li>跨AZ部署：副本数≥AZ数量-1</li></ul></li></ul></li></ol><h2 id="二、写入性能优化"><a href="#二、写入性能优化" class="headerlink" title="二、写入性能优化"></a>二、写入性能优化</h2><p>如何提升Elasticsearch的写入吞吐量？</p><h3 id="2-1-Bulk-调优"><a href="#2-1-Bulk-调优" class="headerlink" title="2.1 Bulk 调优"></a>2.1 Bulk 调优</h3><ul><li>单批次大小：10-15MB（过大引发GC，过小降低吞吐）</li><li>并发线程数：<code>CPU核数×2</code>（观察 <code>bulk_rejected</code> 调整）</li></ul><h3 id="2-2-Refresh-策略"><a href="#2-2-Refresh-策略" class="headerlink" title="2.2 Refresh 策略"></a>2.2 Refresh 策略</h3><pre><code class="json">PUT /logs/_settings&#123;&quot;refresh_interval&quot;: &quot;30s&quot;&#125;  // 写入期关闭刷新，完成后恢复1s</code></pre><h3 id="2-3-Translog-优化"><a href="#2-3-Translog-优化" class="headerlink" title="2.3 Translog 优化"></a>2.3 Translog 优化</h3><pre><code class="json">    &quot;index.translog.durability&quot;: &quot;async&quot;  // 异步写入，风险：宕机丢失5s数据    &quot;index.translog.sync_interval&quot;: &quot;5s&quot;</code></pre><h3 id="2-4-硬件级加速"><a href="#2-4-硬件级加速" class="headerlink" title="2.4 硬件级加速"></a>2.4 硬件级加速</h3><ul><li>使用 SSD + 分离 data 和 logs 磁盘路径</li><li>关闭交换分区：sudo swapoff -a</li></ul><h2 id="三、常见排障"><a href="#三、常见排障" class="headerlink" title="三、常见排障"></a>三、常见排障</h2><h3 id="3-1-yellow"><a href="#3-1-yellow" class="headerlink" title="3.1 yellow"></a>3.1 yellow</h3><pre><code class="bash"># 1. 检查未分配分片原因：GET _cluster/allocation/explain?pretty# 2. 查看磁盘水位（&gt;85%将停止分配）：GET _cat/allocation?v&amp;h=node,disk.percent,shards# 3. 检查节点负载：GET _nodes/stats/os,process?filter_path=**.load_5m,**.cpu.percent# 4. 分析分片锁冲突：GET _cluster/state?filter_path=metadata.indices.*.state,*.blocks</code></pre><p>常见根因:</p><ul><li>磁盘不足 → 清理或扩容</li><li>节点配置不一致 → cluster.routing.allocation.enable: all</li><li>分片损坏 → 使用 reroute 强制分配</li></ul><h3 id="3-2-JVM调优"><a href="#3-2-JVM调优" class="headerlink" title="3.2 JVM调优"></a>3.2 JVM调优</h3><p>Old GC持续超过5秒，如何优化？</p><ul><li><p>堆内存设置</p><ul><li>不超过物理内存50%</li><li>不超过32GB（压缩指针失效阈值）</li><li>示例：64GB内存 → <code>-Xms31g -Xmx31g</code></li></ul></li><li><p>GC策略</p></li></ul><pre><code class="bash">-XX:+UseG1GC-XX:G1ReservePercent=25    # 预留内存防Evacuation Failure-XX:InitiatingHeapOccupancyPercent=35  # 提前触发GC</code></pre><ul><li>堆外内存泄漏排查</li></ul><pre><code class="bash"># 关注 count 和 used_in_bytes 持续增长GET _nodes/stats/jvm?filter_path=**.buffer_pools.direct.*</code></pre><h3 id="3-3-如何防止未授权访问？"><a href="#3-3-如何防止未授权访问？" class="headerlink" title="3.3 如何防止未授权访问？"></a>3.3 如何防止未授权访问？</h3><ul><li>网络层 通过配置文件network.host绑定内网ip</li><li>传输加密 通过开启xpack + 自签证书 bin目录下的keysotre二进制创建带密码的公私钥</li><li>认证 开启basic认证, es的bin目录下有个elasticsearch-reset-password二进制</li></ul><h2 id="四、高可用设计"><a href="#四、高可用设计" class="headerlink" title="四、高可用设计"></a>四、高可用设计</h2><h3 id="单机房高可用模式"><a href="#单机房高可用模式" class="headerlink" title="单机房高可用模式"></a>单机房高可用模式</h3><p>3master 3client + 多data<br>日志分等级, 通过template设置主&#x2F;副分片数量, 通过ilm控制生命周期,<br>日志通过logstash配置Datastream类型写入提前配置好template的数据流</p><p>data通过node.role分冷热节点<br>用到的节点角色:<br>master<br>ingest 数据预处理节点<br>data_hot<br>data_warm<br>data_cold</p><p>client设置为空即可作为业务入口</p><h2 id="五、版本升级（生产环境）"><a href="#五、版本升级（生产环境）" class="headerlink" title="五、版本升级（生产环境）"></a>五、版本升级（生产环境）</h2><h3 id="5-1-如何零停机升级ES-7-x到8-x"><a href="#5-1-如何零停机升级ES-7-x到8-x" class="headerlink" title="5.1 如何零停机升级ES 7.x到8.x"></a>5.1 如何零停机升级ES 7.x到8.x</h3><ol><li><p><strong>前置检查</strong>：</p><ul><li>使用 <code>ES 8 Upgrade Assistant</code> 插件扫描</li><li>解决：type冲突、废弃API、证书兼容</li></ul></li><li><p><strong>滚动升级步骤</strong>：</p></li></ol><pre><code class="bash"># 1. 停用分片分配（防恢复风暴）PUT _cluster/settings&#123;&quot;persistent&quot;: &#123;&quot;cluster.routing.allocation.enable&quot;: &quot;none&quot;&#125;&#125;# 2. 逐节点升级：停机 → 装新版本 → 启动 → 重新加入集群# 3. 开启分片分配并等待绿色</code></pre><h2 id="六、描述您解决过的重大ES故障"><a href="#六、描述您解决过的重大ES故障" class="headerlink" title="六、描述您解决过的重大ES故障"></a>六、描述您解决过的重大ES故障</h2><p>集群Red<br>收到集群Red告警,Kibana大量报错,部分日志索引不可用</p><h3 id="6-1-初步诊断"><a href="#6-1-初步诊断" class="headerlink" title="6.1 初步诊断"></a>6.1 初步诊断</h3><pre><code class="bash"># 确认集群状态GET _cluster/health?pretty → status:red # 40个分片未分配# 定位异常节点GET _cat/nodes?v → node-5磁盘使用率98.7%（超过85%默认水位线）</code></pre><h3 id="6-2-止血操作"><a href="#6-2-止血操作" class="headerlink" title="6.2 止血操作"></a>6.2 止血操作</h3><ul><li>扩容磁盘：临时挂载云盘并迁移部分分片</li><li>通过reroute强制分片分配</li><li>降级写入：临时关闭非关键日志写入</li></ul><h3 id="6-3-根因分析"><a href="#6-3-根因分析" class="headerlink" title="6.3 根因分析"></a>6.3 根因分析</h3><p>直接原因：某业务团队误触发全量历史数据重导<br>深层问题：<br>    * 未设置索引分片上限 → 单节点分片数达120（超最佳实践2倍）<br>    * 缺乏自动处理机制 → 水位超85%未自动清理旧索引</p><h3 id="6-4-长期改进"><a href="#6-4-长期改进" class="headerlink" title="6.4 长期改进"></a>6.4 长期改进</h3><pre><code class="json">// 通过low和high的合理区间, 加长yellow时间段, 留出足够的时间操作PUT _cluster/settings&#123;  &quot;persistent&quot;: &#123;    &quot;cluster.routing.allocation.disk.watermark.low&quot;: &quot;80%&quot;,    &quot;cluster.routing.allocation.disk.watermark.high&quot;: &quot;90%&quot;,    &quot;cluster.max_shards_per_node&quot;: 500  // 限制全局分片总数  &#125;&#125;</code></pre><h1 id="配置示例"><a href="#配置示例" class="headerlink" title="配置示例"></a>配置示例</h1><h2 id="ELK日志节点"><a href="#ELK日志节点" class="headerlink" title="ELK日志节点"></a>ELK日志节点</h2><pre><code class="bash">cluster.name: ohmy-lognetwork.host: 0.0.0.0http.port: 9200node.name: es-hotdata-132node.roles: [ data, ingest ]node.attr.box_type: hotpath.data: /app/data/elasticsearchpath.logs: /var/log/elasticsearchbootstrap.memory_lock: true# 所有master ipdiscovery.seed_hosts: [&quot;192.168.11.100&quot;,&quot;192.168.11.101&quot;,&quot;192.168.11.102&quot;]xpack.monitoring.templates.enabled: truexpack.security.enabled: truexpack.license.self_generated.type: basicxpack.security.transport.ssl.enabled: truexpack.security.http.ssl:  enabled: false# 需要预先生成证书xpack.security.transport.ssl:  enabled: true  verification_mode: certificate  keystore.path: elastic-certificates.p12  truststore.path: elastic-certificates.p12</code></pre><h2 id="业务节点"><a href="#业务节点" class="headerlink" title="业务节点"></a>业务节点</h2><pre><code class="bash">cluster.name: ohmy-es8node.name: &quot;bookinfo-1&quot;node.attr.box_type: bookinfonode.attr.node_type: VMnode.attr.zone: ohmy-homepath.data: &quot;/app/data/elasticsearch&quot;path.logs: &quot;/var/log/elasticsearch&quot;network.host: 0.0.0.0discovery.seed_hosts: [&quot;192.168.33.1&quot;,&quot;192.168.33.2&quot;,&quot;192.168.33.3&quot;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CKA试题</title>
      <link href="/2023/08/02/CKA/"/>
      <url>/2023/08/02/CKA/</url>
      
        <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><pre><code class="bash">alias k=&#39;kubectl&#39;export do=&quot;-o yaml --dry-run=client&quot;</code></pre><h1 id="题目一"><a href="#题目一" class="headerlink" title="题目一"></a>题目一</h1><blockquote><p>Use context: kubectl config use-context k8s-c2-AC<br>The cluster admin asked you to find out the following information about etcd running on cluster2-controlplane1:<br>Server private key location<br>Server certificate expiration date<br>Is client certificate authentication enabled<br>Write these information into &#x2F;opt&#x2F;course&#x2F;p1&#x2F;etcd-info.txt<br>Finally you’re asked to save an etcd snapshot at &#x2F;etc&#x2F;etcd-snapshot.db on cluster2-controlplane1 and display its status.</p></blockquote><h2 id="1-获取etcd运行信息-私钥位置、证书过期时间、是否开启客户端证书身份验证"><a href="#1-获取etcd运行信息-私钥位置、证书过期时间、是否开启客户端证书身份验证" class="headerlink" title="1. 获取etcd运行信息: 私钥位置、证书过期时间、是否开启客户端证书身份验证"></a>1. 获取etcd运行信息: 私钥位置、证书过期时间、是否开启客户端证书身份验证</h2><pre><code class="bash">k get nodesk get po -n kube-system -owideetcd-xxx nodessh nodecd /etc/kubernetes/manifestscat etcd.yaml# 私钥位置# cat /etc/systemd/system/etcd.service | grep key-fileServer private key location: /etc/kubernetes/ssl/etcd-key.pem# 过期时间, 从cert-file里解#  cat /etc/systemd/system/etcd.service | grep cert-file/etc/kubernetes/ssl/etcd.pemopenssl x509 -noout -text -in /etc/kubernetes/ssl/etcd.pemServer certificate expiration date: Jun 17 10:22:00 2073 GMT# 是否开启了客户端认证cat etcd.yaml | grep client-cert-authIs client certificate authentication enabled:</code></pre><h2 id="2-创建etcd-快照备份到-x2F-etc-x2F-etcd-snapshot-db-并打印快照信息"><a href="#2-创建etcd-快照备份到-x2F-etc-x2F-etcd-snapshot-db-并打印快照信息" class="headerlink" title="2. 创建etcd 快照备份到&#x2F;etc&#x2F;etcd-snapshot.db, 并打印快照信息"></a>2. 创建etcd 快照备份到&#x2F;etc&#x2F;etcd-snapshot.db, 并打印快照信息</h2><pre><code class="bash"># 创建快照ETCDCTL_API=3 etcdctl snapshot save /etc/etcd-snapshot.db# 打印快照信息ETCDCTL_API=3 etcdctl snapshot status /etc/etcd-snapshot.dbc277e58f, 5503127, 1628, 24 MB# HASH,   reversion, total keys, total size,</code></pre><h1 id="题目二"><a href="#题目二" class="headerlink" title="题目二"></a>题目二</h1><blockquote><p>Use context: kubectl config use-context k8s-c1-H<br>You’re asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster:<br>Create a new Pod named p2-pod with two containers, one of image nginx:1.21.3-alpine and one of image busybox:1.31. Make sure the busybox container keeps running for some time.<br>Create a new Service named p2-service which exposes that Pod internally in the cluster on port 3000-&gt;80.<br>Find the kube-proxy container on all nodes cluster1-controlplane1, cluster1-node1 and cluster1-node2 and make sure that it’s using iptables. Use command crictl for this.<br>Write the iptables rules of all nodes belonging the created Service p2-service into file &#x2F;opt&#x2F;course&#x2F;p2&#x2F;iptables.txt.<br>Finally delete the Service and confirm that the iptables rules are gone from all nodes.</p></blockquote><h2 id="3-创建一个Pod-包含两个容器-一个镜像是nginx-1-21-3-alpine-另一个镜像是busybox-1-31-确保busybox容器保持运行一段时间"><a href="#3-创建一个Pod-包含两个容器-一个镜像是nginx-1-21-3-alpine-另一个镜像是busybox-1-31-确保busybox容器保持运行一段时间" class="headerlink" title="3. 创建一个Pod, 包含两个容器,一个镜像是nginx:1.21.3-alpine,另一个镜像是busybox:1.31, 确保busybox容器保持运行一段时间"></a>3. 创建一个Pod, 包含两个容器,一个镜像是nginx:1.21.3-alpine,另一个镜像是busybox:1.31, 确保busybox容器保持运行一段时间</h2><pre><code class="bash"># 先生成个基础模板,后面好改k run p2-pod --namespace=project-hamster --image=nginx:1.21.3-alpine -oyaml --dry-run=client &gt; p2.yaml</code></pre><pre><code class="yaml">apiVersion: v1kind: Podmetadata:  creationTimestamp: null  labels:    run: p2-pod  name: p2-pod  namespace: project-hamsterspec:  containers:  - image: nginx:1.21.3-alpine    name: p2-pod    resources: &#123;&#125;  - image: busybox:1.31    name: c2    command:      - &quot;sleep&quot;      - &quot;1d&quot;  dnsPolicy: ClusterFirst  restartPolicy: Alwaysstatus: &#123;&#125;</code></pre><pre><code class="bash">k create -f p2.yaml</code></pre><h2 id="4-创建一个service-p2-service-3000转发上面pod的80"><a href="#4-创建一个service-p2-service-3000转发上面pod的80" class="headerlink" title="4. 创建一个service: p2-service, 3000转发上面pod的80"></a>4. 创建一个service: p2-service, 3000转发上面pod的80</h2><pre><code class="bash">k -n project-hamster expose pod p2-pod --name p2-service --port 3000 --target-port 80k get svc -n project-hamster p2-service -oyamlk get po,svc,ep -n project-hamster -owide</code></pre><h2 id="5-在所有节点cluster1-control-plan-e1、cluster1-node1和cluster1-node2上找到Kube-Proxy容器，并确保它正在使用iptabes。为此，请使用命令crictl。"><a href="#5-在所有节点cluster1-control-plan-e1、cluster1-node1和cluster1-node2上找到Kube-Proxy容器，并确保它正在使用iptabes。为此，请使用命令crictl。" class="headerlink" title="5. 在所有节点cluster1-control plan e1、cluster1-node1和cluster1-node2上找到Kube-Proxy容器，并确保它正在使用iptabes。为此，请使用命令crictl。"></a>5. 在所有节点cluster1-control plan e1、cluster1-node1和cluster1-node2上找到Kube-Proxy容器，并确保它正在使用iptabes。为此，请使用命令crictl。</h2><pre><code class="bash"># 主机部署,实际要挨个ssh上去!for i in k get nodes|grep -v NAME|awk &#39;&#123;print $1&#125;&#39;; do    ssh $i &quot;systemctl status kube-proxy; journalctl -u kube-proxy&quot;    ssh $i &quot;systemctl cat kube-proxy&quot;    # 看配置文件里是用的iptables还是ipvs    ssh $i &quot;cat /var/lib/kube-proxy/kube-proxy-config.yaml|grep mode&quot;done# containerd部署,for循环运行不了awk的, 实际要挨个ssh上去!for i in k get nodes|grep -v NAME|awk &#39;&#123;print $1&#125;&#39;; do    ssh $i &quot;crictl ps|grep kube-proxy; crictl logs $(crictl ps|grep kube-proxy|awk &#39;&#123;print $1&#125;&#39;)&quot;done# 容器日志里会打印 Using iptables Proxier#...#I0913 12:53:03.096620       1 server_others.go:212] Using iptables Proxier.#...</code></pre><h2 id="6-将创建的Service-p2-service所属的所有节点的iptable规则写入文件-x2F-opt-x2F-course-x2F-p2-x2F-ipables-txt"><a href="#6-将创建的Service-p2-service所属的所有节点的iptable规则写入文件-x2F-opt-x2F-course-x2F-p2-x2F-ipables-txt" class="headerlink" title="6. 将创建的Service p2-service所属的所有节点的iptable规则写入文件&#x2F;opt&#x2F;course&#x2F;p2&#x2F;ipables.txt"></a>6. 将创建的Service p2-service所属的所有节点的iptable规则写入文件&#x2F;opt&#x2F;course&#x2F;p2&#x2F;ipables.txt</h2><pre><code class="bash">for i in k get nodes|grep -v NAME|awk &#39;&#123;print $1&#125;&#39;; do    ssh $i &quot;iptables-save|grep p2-service &gt;&gt;/opt/course/p2/ipables.txt&quot;done</code></pre><h2 id="7-最后，删除该服务，并确认已从所有节点中删除iptabes规则。"><a href="#7-最后，删除该服务，并确认已从所有节点中删除iptabes规则。" class="headerlink" title="7. 最后，删除该服务，并确认已从所有节点中删除iptabes规则。"></a>7. 最后，删除该服务，并确认已从所有节点中删除iptabes规则。</h2><pre><code class="bash">k delete svc -n project-hamster p2-servicefor i in k get nodes|grep -v NAME|awk &#39;&#123;print $1&#125;&#39;; do    ssh $i &quot;iptables-save | grep p2-service&quot;done</code></pre><h1 id="题目三"><a href="#题目三" class="headerlink" title="题目三"></a>题目三</h1><blockquote><p>Use context: kubectl config use-context k8s-c2-AC<br>Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember&#x2F;output the IP of that Service.<br>Change the Service CIDR to 11.96.0.0&#x2F;12 for the cluster.<br>Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed.</p></blockquote><h2 id="8-default-ns下用httpd-2-4-41-alpine镜像创建一个叫check-ip的pod-暴漏80端口-用clusterip类型的Service-service名叫check-ip-service-打印cluster-ip"><a href="#8-default-ns下用httpd-2-4-41-alpine镜像创建一个叫check-ip的pod-暴漏80端口-用clusterip类型的Service-service名叫check-ip-service-打印cluster-ip" class="headerlink" title="8. default ns下用httpd:2.4.41-alpine镜像创建一个叫check-ip的pod, 暴漏80端口,用clusterip类型的Service, service名叫check-ip-service,打印cluster ip"></a>8. default ns下用httpd:2.4.41-alpine镜像创建一个叫check-ip的pod, 暴漏80端口,用clusterip类型的Service, service名叫check-ip-service,打印cluster ip</h2><pre><code class="bash"># 创建podk run check-ip --image=httpd:2.4.41-alpine# 暴露服务k expose --name check-ip-service pod check-ip --port=80 --type=ClusterIP# 打印cluster ipk get svc check-ip-service</code></pre><h2 id="9-将群集的服务CIDR更改为11-96-0-0-x2F-12。"><a href="#9-将群集的服务CIDR更改为11-96-0-0-x2F-12。" class="headerlink" title="9. 将群集的服务CIDR更改为11.96.0.0&#x2F;12。"></a>9. 将群集的服务CIDR更改为11.96.0.0&#x2F;12。</h2><blockquote><p>需要修改apiserver以及controller-manager的配置文件中–service-cluster-ip-range配置</p></blockquote><pre><code class="bash">ssh master# apiserver## 守护进程运行的形式,可以修改守护进程中的参数配置systemctl cat kube-apiserver|grep service-cluster-ip-range### --service-cluster-ip-range=10.68.0.0/16 修改为--service-cluster-ip-range=11.96.0.0/12### 修改守护进程的配置后重启systemctl daemon-reloadsystemctl restart kube-apiserver## pod运行的形式,修改kube-apiserver的yaml配置,重启podvim /etc/kubernetes/manifests/kube-apiserver.yaml### 修改为--service-cluster-ip-range=11.96.0.0/12### 等待pod重启k -n kube-system get pod | grep api# controller-managersystemctl cat kube-controller-manager### --service-cluster-ip-range=10.68.0.0/16 修改为--service-cluster-ip-range=11.96.0.0/12### 修改守护进程的配置后重启systemctl daemon-reloadsystemctl restart kube-controller-manager## pod运行的形式,修改kube-controller-manager的yaml配置,重启podvim /etc/kubernetes/manifests/kube-controller-manager.yaml### --service-cluster-ip-range=10.68.0.0/16 修改为--service-cluster-ip-range=11.96.0.0/12k get po -n kube-system | grep controller# 最后再看svc ip并没有发生变化k get svc check-ip-service# 最后新建一个check-ip-service2k expose --name check-ip-service2 pod check-ip --port=80 --type=ClusterIP</code></pre><h1 id="题目四"><a href="#题目四" class="headerlink" title="题目四"></a>题目四</h1><p>RBAC鉴权</p><h2 id="10-创建clusterrole，只允许创建deployment-daemonset-statefulset资源"><a href="#10-创建clusterrole，只允许创建deployment-daemonset-statefulset资源" class="headerlink" title="10. 创建clusterrole，只允许创建deployment,daemonset,statefulset资源"></a>10. 创建clusterrole，只允许创建deployment,daemonset,statefulset资源</h2><pre><code class="bash">k create clusterrole test-clusterrole --verb=create --resource=deployment,daemonset,statefulset</code></pre><h2 id="11-在现有的namespace-app-team1中创建一个名为cicd-token的新ServiceAccount"><a href="#11-在现有的namespace-app-team1中创建一个名为cicd-token的新ServiceAccount" class="headerlink" title="11. 在现有的namespace app-team1中创建一个名为cicd-token的新ServiceAccount"></a>11. 在现有的namespace app-team1中创建一个名为cicd-token的新ServiceAccount</h2><pre><code class="bash">k create ns app-team1k create serviceaccount cicd-token -n app-team1</code></pre><h2 id="12-限于namespace-app-team1，将新的ClusterRole-deployment-clusterrole绑定到新的ServiceAccount-cidi-token"><a href="#12-限于namespace-app-team1，将新的ClusterRole-deployment-clusterrole绑定到新的ServiceAccount-cidi-token" class="headerlink" title="12. 限于namespace app-team1，将新的ClusterRole deployment-clusterrole绑定到新的ServiceAccount cidi-token"></a>12. 限于namespace app-team1，将新的ClusterRole deployment-clusterrole绑定到新的ServiceAccount cidi-token</h2><pre><code class="bash">k create clusterrolebinding test-clusterrolebind -n app-team1 --clusterrole=test-clusterrole --serviceaccount=cicd-token</code></pre><h1 id="题目五"><a href="#题目五" class="headerlink" title="题目五"></a>题目五</h1><h2 id="13-将名为ek8s-node-1的node设置为不可用，并重新调度该node上所有运行的pods"><a href="#13-将名为ek8s-node-1的node设置为不可用，并重新调度该node上所有运行的pods" class="headerlink" title="13. 将名为ek8s-node-1的node设置为不可用，并重新调度该node上所有运行的pods"></a>13. 将名为ek8s-node-1的node设置为不可用，并重新调度该node上所有运行的pods</h2><pre><code class="bash"># cordon 禁止调度k cordon ek8s-node-1# drain 排水k drain ek8s-node-1</code></pre><h1 id="题目六"><a href="#题目六" class="headerlink" title="题目六"></a>题目六</h1><p>升级K8s版本</p><h2 id="14-现有的Kubernetes集群正在运行版本为1-20-0，仅将主节点上的所有Kubernetes控制平面和节点组件升级到版本为1-20-1"><a href="#14-现有的Kubernetes集群正在运行版本为1-20-0，仅将主节点上的所有Kubernetes控制平面和节点组件升级到版本为1-20-1" class="headerlink" title="14. 现有的Kubernetes集群正在运行版本为1.20.0，仅将主节点上的所有Kubernetes控制平面和节点组件升级到版本为1.20.1"></a>14. 现有的Kubernetes集群正在运行版本为1.20.0，仅将主节点上的所有Kubernetes控制平面和节点组件升级到版本为1.20.1</h2><p>确保在升级之前drain主节点，并在升级后uncordon主节点</p><pre><code class="bash">ssh ek8s-maste # 登录主节点k drain ek8s-maste --ignore-daemonsets # 排水yum search kubeadm # 查找kubeadm 包名yum install kubeadm-1.20.1-0 # 安装kubeadm 1.20.1版本kubeadm upgrade plan # 验证升级计划kubeadm upgrade apply v1.20.1 --etcd-upgrade=false # 应用升级计划, 排除etcd,题目没有要求kubectl uncordon ek8s-maste # uncordon主节点yum install kubectl-1.20.1-0 kubelet-1.20.1-0 # 安装kubelet kubectl 1.20.1版本systemctl restart kubelet # 重启kubeletk get nodes # 查看当前节点版本信息</code></pre><h1 id="题目七"><a href="#题目七" class="headerlink" title="题目七"></a>题目七</h1><p>etcd备份与恢复</p><h2 id="15-首先，为运行在https-127-0-0-1-2379-上的现有etcd实例创建快照并将快照保存到-x2F-data-x2F-backup-x2F-etcd-snapshot-db"><a href="#15-首先，为运行在https-127-0-0-1-2379-上的现有etcd实例创建快照并将快照保存到-x2F-data-x2F-backup-x2F-etcd-snapshot-db" class="headerlink" title="15. 首先，为运行在https://127.0.0.1:2379 上的现有etcd实例创建快照并将快照保存到 &#x2F;data&#x2F;backup&#x2F;etcd-snapshot.db"></a>15. 首先，为运行在<a href="https://127.0.0.1:2379/">https://127.0.0.1:2379</a> 上的现有etcd实例创建快照并将快照保存到 &#x2F;data&#x2F;backup&#x2F;etcd-snapshot.db</h2><pre><code class="bash">mkdir -p /data/backupsystemctl cat etcd # 查看ca证书位置--trusted-ca-file 客户端证书--cert-file 客户端秘钥--key-fileETCDCTL_API=3 etcdctl --endpoints=&quot;https://127.0.0.1:2379&quot; snapshot save /data/backup/etcd-snapshot.db --cacert=&quot;/etc/kubernetes/ssl/ca.pem&quot; --cert=&quot;/etc/kubernetes/ssl/etcd.pem&quot; --key=&quot;/etc/kubernetes/ssl/etcd-key.pem&quot;</code></pre><h2 id="16-然后还原位于-x2F-data-x2F-backup-x2F-etcd-snapshot-previous-db的现有先前快照-提供了TLS证书和密钥路径，连接etcd服务器"><a href="#16-然后还原位于-x2F-data-x2F-backup-x2F-etcd-snapshot-previous-db的现有先前快照-提供了TLS证书和密钥路径，连接etcd服务器" class="headerlink" title="16. 然后还原位于 &#x2F;data&#x2F;backup&#x2F;etcd-snapshot-previous.db的现有先前快照(提供了TLS证书和密钥路径，连接etcd服务器)"></a>16. 然后还原位于 &#x2F;data&#x2F;backup&#x2F;etcd-snapshot-previous.db的现有先前快照(提供了TLS证书和密钥路径，连接etcd服务器)</h2><pre><code class="bash">systemctl cat etcd | grep data # 确认数据目录位置# --data-dir=/var/lib/etcdsystemctl stop etcd # 停止etcdmv /var/lib/etcd /var/lib/etcd-bak # 备份ETCDCTL_API=3 etcdctl snapshot restore /data/backup/etcd-snapshot-previous.db --data-dir=/var/lib/etcd #恢复数据chown etcd:etcd -R /var/lib/etcd # 数据目录所属etcd用户, ubuntu可能会这样, systemctl cat etcd可以看到启动用户systemctl start etcd</code></pre><h1 id="题目八"><a href="#题目八" class="headerlink" title="题目八"></a>题目八</h1><blockquote><p>题意机翻不明确<br>网络策略<br>在现有的namespace my-app中创建一个名为allow-port-from-namespace的新NetworkPolicy<br>确保新的NetworkPolicy允许namespace my-app中的Pods来连接到namespace big-corp中的端口8080<br>进一步确保新的NetworkPolicy<br>不允许对没有在监听端口8080的Pods的访问<br>不允许不来自namespace my-app中的Pods的访问</p></blockquote><p>猜测题目:</p><h2 id="17-现有namespace-my-app-创建一个名为allow-port-from-namespace的新NetworkPolicy"><a href="#17-现有namespace-my-app-创建一个名为allow-port-from-namespace的新NetworkPolicy" class="headerlink" title="17. 现有namespace my-app, 创建一个名为allow-port-from-namespace的新NetworkPolicy"></a>17. 现有namespace my-app, 创建一个名为allow-port-from-namespace的新NetworkPolicy</h2><p>只允许来自big-corp namespace下的pod访问本namespace下的pod的8080端口</p><pre><code class="yaml"># k api-resources|grep networkpoapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:  name: allow-port-from-namespace  namespace: my-appspec:  podSelector: &#123;&#125;  policyTypes:    - Ingress  ingress:    - from:      - namespaceSelector:          matchLabels:            name: big-corp      ports:      - protocol: TCP        port: 8080</code></pre><h1 id="题目九"><a href="#题目九" class="headerlink" title="题目九"></a>题目九</h1><blockquote><p>题意机翻不明确<br>SVC 暴露应用<br>请重新配置现有的部署front-end以及添加名为http的端口规范来公开现有容器nginx的端口80&#x2F;tcp<br>创建一个名为front-end-svc的新服务，以公开容器端口http<br>配置此服务，以通过在排定的节点上的NodePort来公开各个Pods</p></blockquote><p>猜测题目:</p><h2 id="18-调整现有名为front-end的deployment资源-添加名为http的端口-来公开当前容器nginx的TCP协议80端口-然后创建一个名为front-end-svc且类型为NodePort的服务来暴露这个80端口"><a href="#18-调整现有名为front-end的deployment资源-添加名为http的端口-来公开当前容器nginx的TCP协议80端口-然后创建一个名为front-end-svc且类型为NodePort的服务来暴露这个80端口" class="headerlink" title="18. 调整现有名为front-end的deployment资源, 添加名为http的端口,来公开当前容器nginx的TCP协议80端口, 然后创建一个名为front-end-svc且类型为NodePort的服务来暴露这个80端口"></a>18. 调整现有名为front-end的deployment资源, 添加名为http的端口,来公开当前容器nginx的TCP协议80端口, 然后创建一个名为front-end-svc且类型为NodePort的服务来暴露这个80端口</h2><pre><code class="yaml"># k edit deployment front-endcontainers:  ports:  - name: http    protocol: TCP    containerPort: 80</code></pre><pre><code class="bash">k expose --name=front-end-svc --type=NodePort depmoyment front-end --target-port=80 --port=80</code></pre><h1 id="题目十"><a href="#题目十" class="headerlink" title="题目十"></a>题目十</h1><p>Ingress</p><h2 id="19-在namespace-ing-internal下创建一个名为pong的nginx-ingress-将名为hello的service的5678端口暴露到-x2F-hello路径下"><a href="#19-在namespace-ing-internal下创建一个名为pong的nginx-ingress-将名为hello的service的5678端口暴露到-x2F-hello路径下" class="headerlink" title="19. 在namespace ing-internal下创建一个名为pong的nginx ingress, 将名为hello的service的5678端口暴露到&#x2F;hello路径下"></a>19. 在namespace ing-internal下创建一个名为pong的nginx ingress, 将名为hello的service的5678端口暴露到&#x2F;hello路径下</h2><pre><code class="bash">k create ingress pong --namespace=ing-internal --rule=&quot;/hello*=hello:5678&quot; --annotation=&quot;nginx.ingress.kubernetes.io/rewrite-target=/&quot; -oyaml --dry-run=client &gt;p19.yaml</code></pre><pre><code class="yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  annotations:    nginx.ingress.kubernetes.io/rewrite-target: /  name: pong  namespace: ing-internalspec:  rules:  - http:      paths:      - backend:          service:            name: hello            port:              number: 5678        path: /hello        pathType: Prefix</code></pre><h1 id="题目十一"><a href="#题目十一" class="headerlink" title="题目十一"></a>题目十一</h1><p>扩容Pod数量</p><h2 id="20-将deployment-loadbalancer扩展至5个pods"><a href="#20-将deployment-loadbalancer扩展至5个pods" class="headerlink" title="20. 将deployment loadbalancer扩展至5个pods"></a>20. 将deployment loadbalancer扩展至5个pods</h2><pre><code class="bash"># 编辑修改k edit deployment loadbalancer# 或命令行扩容k scale deployment loadbalancer --replicas=5</code></pre><h1 id="题目十二"><a href="#题目十二" class="headerlink" title="题目十二"></a>题目十二</h1><p>nodeSelector</p><h2 id="21-按如下要求调度一个Pod-名称：nginx-kusc00401-image-nginx-Node-selector-disk-x3D-ssd"><a href="#21-按如下要求调度一个Pod-名称：nginx-kusc00401-image-nginx-Node-selector-disk-x3D-ssd" class="headerlink" title="21. 按如下要求调度一个Pod , 名称：nginx-kusc00401, image: nginx, Node selector: disk&#x3D;ssd"></a>21. 按如下要求调度一个Pod , 名称：nginx-kusc00401, image: nginx, Node selector: disk&#x3D;ssd</h2><pre><code class="yaml">#k run nginx-kusc00401 --image=nginx --dry-run=client -oyaml &gt; p21.yaml# vim p21.yaml # 添加nodeselector字段apiVersion: v1kind: Podmetadata:  labels:    run: nginx-kusc00401  name: nginx-kusc00401spec:  containers:  - image: nginx    name: nginx-kusc00401    resources: &#123;&#125;  nodeSelector:    disk: ssd  dnsPolicy: ClusterFirst  restartPolicy: Alwaysstatus: &#123;&#125;</code></pre><h1 id="题目十三"><a href="#题目十三" class="headerlink" title="题目十三"></a>题目十三</h1><p>统计准备就绪节点数量</p><h2 id="22-检查有多少worker-nodes已准备就绪-不包括被打上Taint-NoSchedule的节点-，并将数量写入-x2F-opt-x2F-KUSC00402-x2F-kusc00402-txt"><a href="#22-检查有多少worker-nodes已准备就绪-不包括被打上Taint-NoSchedule的节点-，并将数量写入-x2F-opt-x2F-KUSC00402-x2F-kusc00402-txt" class="headerlink" title="22. 检查有多少worker nodes已准备就绪(不包括被打上Taint: NoSchedule的节点)，并将数量写入&#x2F;opt&#x2F;KUSC00402&#x2F;kusc00402.txt"></a>22. 检查有多少worker nodes已准备就绪(不包括被打上Taint: NoSchedule的节点)，并将数量写入&#x2F;opt&#x2F;KUSC00402&#x2F;kusc00402.txt</h2><pre><code class="bash">k describe nodes $(k get nodes|grep &quot;Ready&quot;|awk &#39;&#123;print $1&#125;&#39;) | grep Taints | grep -vc NoSchedule &gt; /opt/KUSC00402/kusc00402.txt</code></pre><h1 id="题目十四"><a href="#题目十四" class="headerlink" title="题目十四"></a>题目十四</h1><p>Pod配置多容器</p><h2 id="23-创建一个名为kucc4的pod，在pod里面分别为以下每个images单独运行一个app-container-可能会有1-4个images-：nginx-redis-memcached"><a href="#23-创建一个名为kucc4的pod，在pod里面分别为以下每个images单独运行一个app-container-可能会有1-4个images-：nginx-redis-memcached" class="headerlink" title="23. 创建一个名为kucc4的pod，在pod里面分别为以下每个images单独运行一个app container(可能会有1-4个images)：nginx+redis+memcached"></a>23. 创建一个名为kucc4的pod，在pod里面分别为以下每个images单独运行一个app container(可能会有1-4个images)：nginx+redis+memcached</h2><pre><code class="yaml"># k run kucc4 --image nginx --dry-run=client -oyaml &gt; p23.yaml# vim p23.yamlapiVersion: v1kind: Podmetadata:  creationTimestamp: null  labels:    run: kucc4  name: kucc4spec:  containers:  - image: nginx    name: nginx    resources: &#123;&#125;  - image: redis    name: redis    resources: &#123;&#125;  - image: memcached    name: memcached    resources: &#123;&#125;  dnsPolicy: ClusterFirst  restartPolicy: Alwaysstatus: &#123;&#125;</code></pre><h1 id="题目十五"><a href="#题目十五" class="headerlink" title="题目十五"></a>题目十五</h1><p>创建 PV</p><h2 id="24-创建名为app-data的persistent-volume，容量为2Gi，访问模式为ReadWriteOnce。volume类型为hostPath，位于-x2F-srv-x2F-app-data"><a href="#24-创建名为app-data的persistent-volume，容量为2Gi，访问模式为ReadWriteOnce。volume类型为hostPath，位于-x2F-srv-x2F-app-data" class="headerlink" title="24. 创建名为app-data的persistent volume，容量为2Gi，访问模式为ReadWriteOnce。volume类型为hostPath，位于&#x2F;srv&#x2F;app-data"></a>24. 创建名为app-data的persistent volume，容量为2Gi，访问模式为ReadWriteOnce。volume类型为hostPath，位于&#x2F;srv&#x2F;app-data</h2><pre><code class="yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: app-dataspec:  accessModes:    - ReadWriteOnce  capacity:    storage: 2Gi  hostPath:    path: &quot;/srv/appdata&quot;</code></pre><h1 id="题目十六"><a href="#题目十六" class="headerlink" title="题目十六"></a>题目十六</h1><p>Pod使用PVC</p><h2 id="25-创建一个新的PersistentVolumeClaim"><a href="#25-创建一个新的PersistentVolumeClaim" class="headerlink" title="25. 创建一个新的PersistentVolumeClaim"></a>25. 创建一个新的PersistentVolumeClaim</h2><blockquote><p>名称：pv-volume<br>Class：csi-hostpath-sc<br>容量：10Mi</p></blockquote><blockquote><p>创建一个新的Pod，此Pod将作为volume挂载到PersistenVolumeClaim<br>名称：web-server<br>image：nginx<br>挂载路径：&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</p></blockquote><blockquote><p>配置新的Pod，以对volume具有ReadWriteOnce权限<br>最后，使用kubectl edit或者kubectl patch将PersistenVolumeClaim的容量扩展为70Mi，并记录此更改。</p></blockquote><pre><code class="yaml">---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: pv-volumespec:  storageClassName: csi-hostpath-sc  resources:    requests:      storage: 10Mi  accessModes:    - ReadWriteOnce---apiVersion: v1kind: Podsmetadata:  name: web-serverspec:  containers:    - image: nginx      name: nginx      volumeMounts:        mountPath: /usr/share/nginx/html        name: data  volumes:    - name: data      persistentVolumeClaim:        claimName: pv-volume</code></pre><pre><code class="bash">kubectl edit pvc pv-volume --save-config</code></pre><h1 id="题目十七"><a href="#题目十七" class="headerlink" title="题目十七"></a>题目十七</h1><p>获取Pod错误日志</p><h2 id="26-监控pod-bar的日志并：提取与错误file-not-found相对应的日志行。将这些日志行写入-x2F-opt-x2F-KUTR00101-x2F-bar"><a href="#26-监控pod-bar的日志并：提取与错误file-not-found相对应的日志行。将这些日志行写入-x2F-opt-x2F-KUTR00101-x2F-bar" class="headerlink" title="26. 监控pod bar的日志并：提取与错误file-not-found相对应的日志行。将这些日志行写入&#x2F;opt&#x2F;KUTR00101&#x2F;bar"></a>26. 监控pod bar的日志并：提取与错误file-not-found相对应的日志行。将这些日志行写入&#x2F;opt&#x2F;KUTR00101&#x2F;bar</h2><pre><code class="bash">k logs bar | grep file-not-found &gt; /opt/KUTR00101/bar</code></pre><h1 id="题目十八"><a href="#题目十八" class="headerlink" title="题目十八"></a>题目十八</h1><p>给Pod增加一个容器(边车) sidecar</p><h2 id="27-用busybox-Image来将名为sidecar的sidecar容器添加到现有的Pod-legacy-app中。新的sidecar容器必须运行以下命令"><a href="#27-用busybox-Image来将名为sidecar的sidecar容器添加到现有的Pod-legacy-app中。新的sidecar容器必须运行以下命令" class="headerlink" title="27. 用busybox Image来将名为sidecar的sidecar容器添加到现有的Pod legacy-app中。新的sidecar容器必须运行以下命令:"></a>27. 用busybox Image来将名为sidecar的sidecar容器添加到现有的Pod legacy-app中。新的sidecar容器必须运行以下命令:</h2><blockquote><p>&#x2F;bin&#x2F;sh -c tail -n+1 -f &#x2F;var&#x2F;log&#x2F;legacy-app.log<br>使用安装在&#x2F;var&#x2F;log的Volume，使日志文件legacy-app.log可用于sidecar容器</p></blockquote><pre><code class="yaml"># k run legacy-app --image=busybox --dry-run=client -oyaml &gt; p27.yaml # 然后编辑apiVersion: v1kind: Podmetadata:  labels:    run: legacy-app  name: legacy-appspec:  containers:  - image: busybox    name: legacy-app    command:      - &quot;/bin/sh&quot;      - &quot;-c&quot;      - &gt;        while true;do echo $(date) &gt;&gt; /var/log/legacy-app.log;sleep 1;done    volumeMounts:    - name: logdir      mountPath: /var/log    resources: &#123;&#125;  - image: busybox    name: sidecar    command:      - &quot;/bin/sh&quot;      - &quot;-c&quot;      - &quot;tail -n+1 -f /var/log/legacy-app.log&quot;    volumeMounts:    - name: logdir      mountPath: /var/log    resources: &#123;&#125;  volumes:    - name: logdir      emptyDir: &#123;&#125;  dnsPolicy: ClusterFirst  restartPolicy: Always</code></pre><h1 id="题目十九"><a href="#题目十九" class="headerlink" title="题目十九"></a>题目十九</h1><p>统计使用CPU最高的Pod</p><blockquote><p>需要集群安装metrics-server</p></blockquote><h2 id="28-通过pod-label-app-x3D-hostnames，找到运行占用大量CPU的pod，并将占用CPU最高的pod名称写入文件-x2F-opt-x2F-KUTR00401-x2F-KUTR00401-txt（已存在）"><a href="#28-通过pod-label-app-x3D-hostnames，找到运行占用大量CPU的pod，并将占用CPU最高的pod名称写入文件-x2F-opt-x2F-KUTR00401-x2F-KUTR00401-txt（已存在）" class="headerlink" title="28. 通过pod label app&#x3D;hostnames，找到运行占用大量CPU的pod，并将占用CPU最高的pod名称写入文件&#x2F;opt&#x2F;KUTR00401&#x2F;KUTR00401.txt（已存在）"></a>28. 通过pod label app&#x3D;hostnames，找到运行占用大量CPU的pod，并将占用CPU最高的pod名称写入文件&#x2F;opt&#x2F;KUTR00401&#x2F;KUTR00401.txt（已存在）</h2><pre><code class="bash">k top pods -l app=hostnames --sort-by=cpu -A |head -2 | tail -1 | awk &#39;&#123;print $2&#125;&#39; &gt; /opt/KUTR00401/KUTR00401.txt</code></pre><h1 id="题目二十"><a href="#题目二十" class="headerlink" title="题目二十"></a>题目二十</h1><p>节点NotReady处理</p><h2 id="29-名为wk8s-node-0的Kubernetes-worker-node处于NotReady状态。调查发生这种情况的原因，并采取相应措施将node恢复为Ready状态，确保所做的任何更改永久有效"><a href="#29-名为wk8s-node-0的Kubernetes-worker-node处于NotReady状态。调查发生这种情况的原因，并采取相应措施将node恢复为Ready状态，确保所做的任何更改永久有效" class="headerlink" title="29. 名为wk8s-node-0的Kubernetes worker node处于NotReady状态。调查发生这种情况的原因，并采取相应措施将node恢复为Ready状态，确保所做的任何更改永久有效"></a>29. 名为wk8s-node-0的Kubernetes worker node处于NotReady状态。调查发生这种情况的原因，并采取相应措施将node恢复为Ready状态，确保所做的任何更改永久有效</h2><pre><code class="bash">k get nodesk describe node wk8s-node-0ssh wk8s-node-0systemctl status kubeletsystemctl enable --now kubelet</code></pre>]]></content>
      
      
      <categories>
          
          <category> 云原生 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> k8s </tag>
            
            <tag> cka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph日常运维</title>
      <link href="/2023/02/19/Ceph%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/"/>
      <url>/2023/02/19/Ceph%E6%97%A5%E5%B8%B8%E8%BF%90%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="OSD节点重启"><a href="#OSD节点重启" class="headerlink" title="OSD节点重启"></a>OSD节点重启</h2><p>&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;运维过程中可能由于内核更新、故障修复、raid配置、机柜位置调整等原因计划内重启osd物理机节点，需要进行以下操作安全地进行维护</p><pre><code class="bash"># 1、关闭自动重平衡ceph osd set noout# 2、关闭节点上的osd进程osds=$(systemctl |grep &quot;^  ceph-osd@&quot;|awk &#39;&#123;print $1&#125;&#39;|awk -F&#39;@&#39; &#39;&#123;print $NF&#125;&#39;|awk -F&quot;.&quot; &#39;&#123;print $1&#125;&#39;)for i in $osds;do ceph osd down $i;donesystemctl stop ceph-osd.target# 3、关闭节点shutdown -h now# 4、开始维护当你对失败域中OSD维护时，其中的PG将会变为degraded状态。# 5、维护完成启动守护进程systemctl start ceph-osd.target# 6、最后务必记得取消集群的noout状态ceph osd unset noout</code></pre>]]></content>
      
      
      <categories>
          
          <category> CEPH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph健康告警+钉钉</title>
      <link href="/2023/02/19/Ceph%E5%81%A5%E5%BA%B7%E5%91%8A%E8%AD%A6+%E9%92%89%E9%92%89/"/>
      <url>/2023/02/19/Ceph%E5%81%A5%E5%BA%B7%E5%91%8A%E8%AD%A6+%E9%92%89%E9%92%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>ceph-exporter没有带”up”之类的自身状态指标， 当ceph-exporter挂掉后， 是没办法第一时间知道的。如果此时ceph集群出现异常就不能及时收到告警，因此通过脚本定时去请求ceph的api获取当前集群状态</p></blockquote><ul><li>可以通过python脚本调取ceph dashboard api定期获取健康状态，结合钉钉机器人进行通知</li></ul><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><pre><code class="python"># %%#!/usr/bin/env python3# -*- coding: UTF-8 -*-import requestsimport jsonauth_path = &quot;/api/auth&quot;logout_path = &quot;/api/auth/logout&quot;health_path = &quot;/api/health/full&quot;ding_webhook = &quot;https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxxxxxxxx&quot;# %%def dingtalk(message):    header = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;    data = &#123;        &quot;msgtype&quot;: &quot;markdown&quot;,        &quot;markdown&quot;: &#123;            &quot;title&quot;: &quot;Ceph集群异常&quot;,            &quot;text&quot;: message        &#125;,        &quot;at&quot;: &#123;            &quot;isAtAll&quot;: False        &#125;    &#125;    res = requests.post(ding_webhook, headers=header, data=json.dumps(data))    return(res.json())# %%class Ceph(object):    def __init__(self, host, username, password):        self.host = host        self.username = username        self.password = password        header = &#123;            &quot;Content-Type&quot;: &quot;application/json&quot;,            &quot;Accept&quot;: &quot;application/vnd.ceph.api.v1.0+json&quot;        &#125;        data = &#123;            &quot;username&quot;: self.username,            &quot;password&quot;: self.password        &#125;        r = requests.post(self.host+auth_path, headers=header, data=json.dumps(data))        token = r.json().get(&quot;token&quot;)        authorization = f&quot;Bearer &#123;token&#125;&quot;        self.op_header = &#123;            &quot;Authorization&quot;: authorization        &#125;    def healthCheck(self):        r = requests.get(self.host+health_path, headers=self.op_header)        return r.json().get(&quot;health&quot;)[&quot;status&quot;]    def close(self):        r = requests.post(self.host+logout_path, headers=self.op_header)        return r.json()# %%host = &quot;http://dashboard.ohops.com&quot;username = &quot;ohmyuser&quot;password = &quot;xxxxxx&quot;ceph = Ceph(host, username, password)status = ceph.healthCheck()ceph.close()print(status)if status != &quot;HEALTH_OK&quot;:    message = f&quot;### Ceph集群异常 \n\n &gt; 当前状态: **&lt;font color=&#39;#FF0000&#39;&gt;&#123;status&#125;&lt;/font&gt;**\n\n[查看详情](http://grafana.ohops.com/d/xxxxxx)&quot;    send_msg = dingtalk(message)</code></pre><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p><img src="/image.png" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> CEPH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>摸鱼笔记</title>
      <link href="/2023/02/19/%E6%91%B8%E9%B1%BC%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/02/19/%E6%91%B8%E9%B1%BC%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="8d58d7fff33b25628538645c7b2d5b0bcb855a3511d0f1e1b6232b26cf4f9a06">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随手 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph运维常用命令整理</title>
      <link href="/2023/02/19/Ceph%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2023/02/19/Ceph%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="集群状态"><a href="#集群状态" class="headerlink" title="集群状态"></a>集群状态</h2><table><thead><tr><th><strong>command</strong></th><th><strong>example</strong></th><th><strong>description</strong></th></tr></thead><tbody><tr><td>ceph version</td><td></td><td>集群组件版本</td></tr><tr><td>ceph -s</td><td></td><td>集群健康状况、节点调度信息等</td></tr><tr><td>ceph health detail</td><td></td><td>集群健康状况细节</td></tr><tr><td>ceph df</td><td></td><td>集群中存储量的概览和细节</td></tr><tr><td>ceph osd stat</td><td></td><td>查看集群osd实例概览</td></tr><tr><td>ceph osd df</td><td></td><td>集群中每个osd的容量使用情况</td></tr><tr><td>rados df</td><td></td><td>查看每个pool使用情况</td></tr><tr><td>ceph osd pool ls</td><td></td><td>列出pool</td></tr><tr><td>ceph osd pool ls detail</td><td></td><td>列出pool及细节</td></tr><tr><td>ceph crash ls</td><td></td><td>列出所有崩溃问题</td></tr><tr><td>ceph crash ls-new</td><td></td><td>列出新出现的崩溃问题</td></tr><tr><td>ceph crash info <crash-id></td><td>ceph crash info 2023-02-01…..</td><td>展示具体崩溃问题信息</td></tr><tr><td>ceph crash archive <crash-id></td><td>ceph crash archive 2023-02-01..</td><td>归档指定崩溃问题</td></tr><tr><td>ceph crash archive-all</td><td></td><td>归档所有崩溃问题（解决WARN）</td></tr><tr><td>ceph daemon osd.<osd-id> config show</td><td>ceph daemon osd.3 config show</td><td>查看运行中的osd配置</td></tr></tbody></table><h2 id="osd"><a href="#osd" class="headerlink" title="osd"></a>osd</h2><table><thead><tr><th><strong>command</strong></th><th><strong>example</strong></th><th><strong>description</strong></th></tr></thead><tbody><tr><td>ceph osd tree</td><td></td><td></td></tr><tr><td>它提供了每个OSD的列表还包括类、权重、状态，OSD所在的节点，以及任何重新加权或优先级</td><td></td><td></td></tr><tr><td>ceph osd find <osd_id></td><td>ceph osd find 8</td><td>快速定位osd机器位置等信息</td></tr><tr><td>ceph osd down <osd_id></td><td>ceph osd down 0</td><td>下线osd，此osd不再接收读写操作，但是还活着</td></tr><tr><td>ceph osd up <osd_id></td><td>ceph osd up 0</td><td>拉起osd，该osd开始接收读写</td></tr><tr><td>ceph osd out <osd_id></td><td>ceph osd out 0</td><td>将osd驱逐出集群，此时可对此osd进行维护</td></tr><tr><td>ceph osd in <osd_id></td><td>ceph osd in 0</td><td>将osd加入集群</td></tr><tr><td>ceph osd rm <osd_id></td><td>ceph osd rm 0</td><td>在集群中删除一个 osd,可能需要先 stop 该 osd,即 systemctl stop osd.0</td></tr><tr><td>ceph osd crush rm <hostname></td><td>ceph osd crush rm node1</td><td>在集群中删除一个host节点</td></tr><tr><td>ceph osd getmaxosd</td><td></td><td></td></tr><tr><td>查看最大osd的个数及当前osd个数（如开启动态调整部署，则自适应无需设置）</td><td></td><td></td></tr><tr><td>ceph osd setmaxosd <max_num></td><td>ceph osd setmaxosd 20</td><td>设置最大osd个数</td></tr><tr><td>ceph osd pause</td><td></td><td></td></tr><tr><td>暂停osd服务，整个集群不再接收数据</td><td></td><td></td></tr><tr><td>ceph osd unpause</td><td></td><td></td></tr><tr><td>开启osd服务，开启后再次接收数据</td><td></td><td></td></tr></tbody></table><h2 id="pool"><a href="#pool" class="headerlink" title="pool"></a>pool</h2><table><thead><tr><th><strong>command</strong></th><th><strong>example</strong></th><th><strong>description</strong></th></tr></thead><tbody><tr><td>ceph osd pool create <pool-name> [pg_num]</td><td>ceph osd pool create  test_pool</td><td>创建pool, pg_num可选, 通常结合下面声明pool类型</td></tr><tr><td>ceph osd pool application enable <pool-name> <application-type></td><td>ceph osd pool application enable  test_pool rbd</td><td>受支持的类型为：cephfs、rbd、rgw</td></tr><tr><td>ceph osd pool set-quota <pool-name> max_bytes <quota_num></td><td>ceph osd pool set-quota test_pool max_bytes $((10 * 1024 * 1024))</td><td>设置pool配额</td></tr><tr><td>ceph osd pool get-quota <pool-name></td><td>ceph osd pool get-quota test_pool</td><td>查看pool配额</td></tr><tr><td>ceph osd pool set <pool-name> pg_num <pg-new-num></td><td>ceph osd pool set test_pool pg_num 64</td><td>修改pool pg_num</td></tr><tr><td>ceph osd pool set <pool-name> pgp_num <pgp-new-num></td><td>ceph osd pool set test_pool pgp_num 64</td><td>修改pool pgp_num</td></tr><tr><td>ceph osd pool set <pool-name> size <copy_num></td><td>ceph osd pool set test_pool size 2</td><td>修改副本数</td></tr><tr><td>ceph osd pool set <poolname> min_size <num></td><td>ceph osd pool set test_pool min_size 1</td><td>修改degrade副本数</td></tr><tr><td>ceph osd pool rename <current-pool-name> <new-pool-name></td><td>ceph osd pool rename test_pool test_new_pool</td><td>重命名pool</td></tr><tr><td>ceph osd pool delete <pool-name> <pool-name> –yes-i-really-really-mean-it</td><td>ceph osd pool delete test_new_pool test_new_pool –yes-i-really-really-mean-it</td><td>删除pool，需要临时设置mon_allow_pool_delete&#x3D;true才能成功删除</td></tr><tr><td>ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.<mon-nodename>.asok config set  mon_allow_pool_delete true</td><td>ceph –admin-daemon &#x2F;var&#x2F;run&#x2F;ceph&#x2F;ceph-mon.ceph-pre-01.asok config set  mon_allow_pool_delete true</td><td>临时设置mon_allow_pool_delete&#x3D;true</td></tr></tbody></table><h2 id="rgw"><a href="#rgw" class="headerlink" title="rgw"></a>rgw</h2><table><thead><tr><th><strong>command</strong></th><th><strong>example</strong></th><th><strong>description</strong></th></tr></thead><tbody><tr><td>radosgw-admin user create –uid&#x3D;<uid> –display-name&#x3D;”name” –email&#x3D;<email_addr></td><td>radosgw-admin user create –uid&#x3D;ohmyuser –display-name&#x3D;”ohmyuser” --email&#x3D;<a href="mailto:&#x6f;&#x68;&#x6d;&#121;&#x75;&#x73;&#x65;&#x72;&#64;&#111;&#104;&#111;&#112;&#x73;&#46;&#x6f;&#114;&#103;">&#x6f;&#x68;&#x6d;&#121;&#x75;&#x73;&#x65;&#x72;&#64;&#111;&#104;&#111;&#112;&#x73;&#46;&#x6f;&#114;&#103;</a></td><td>创建rgw用户</td></tr><tr><td>s3cmd mb s3:&#x2F;&#x2F;<bucket-name></td><td>s3cmd mb s3:&#x2F;&#x2F;test-backup</td><td>创建存储桶</td></tr><tr><td>radosgw-admin bucket link –uid&#x3D;<uid> –bucket&#x3D;<bucket-name></td><td>radosgw-admin bucket link –uid&#x3D;ohmyuser –bucket&#x3D;test-backup</td><td>绑定用户到存储桶</td></tr><tr><td>radosgw-admin quota set –uid&#x3D;<uid> –quota-scope&#x3D;user –max-size&#x3D;<quota-size></td><td>radosgw-admin quota set –uid&#x3D;ohmyuser –quota-scope&#x3D;user –max-size&#x3D;100TB</td><td>设置用户配额</td></tr><tr><td>radosgw-admin quota enable –quota-scope&#x3D;user –uid&#x3D;<uid></td><td>radosgw-admin quota enable –quota-scope&#x3D;user –uid&#x3D;ohmyuser</td><td>开启用户配额配置</td></tr><tr><td>adosgw-admin quota disable –quota-scope&#x3D;user –uid&#x3D;<uid></td><td>radosgw-admin quota disable –quota-scope&#x3D;user –uid&#x3D;ohmyuser</td><td>关闭用户配额设置</td></tr><tr><td>radosgw-admin quota set –uid&#x3D;<uid> –quota-scope&#x3D;bucket –max-size&#x3D;<quota-size></td><td>radosgw-admin quota set –uid&#x3D;ohmyuser –quota-scope&#x3D;bucket –max-size&#x3D;100TB</td><td>设置存储桶配额</td></tr><tr><td>radosgw-admin quota enable –quota-scope&#x3D;bucket –uid&#x3D;<uid></td><td>radosgw-admin quota enable –quota-scope&#x3D;bucket –uid&#x3D;ohmyuser</td><td>开启存储桶配额配置</td></tr><tr><td>radosgw-admin quota disable –quota-scope&#x3D;bucket –uid&#x3D;<uid></td><td>radosgw-admin quota disable –quota-scope&#x3D;bucket –uid&#x3D;ohmyuser</td><td>关闭存储桶配额设置</td></tr><tr><td>radosgw-admin user info –uid&#x3D;<uid></td><td>radosgw-admin user info –uid&#x3D;ohmyuser</td><td>查看rgw用户信息</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> CEPH </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch8小记</title>
      <link href="/2022/09/19/Elasticsearch8%E5%B0%8F%E8%AE%B0/"/>
      <url>/2022/09/19/Elasticsearch8%E5%B0%8F%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>ELK版本: 8.2.2</p><h2 id="列出索引"><a href="#列出索引" class="headerlink" title="列出索引"></a>列出索引</h2><pre><code class="bash">GET _cat/indices</code></pre><h2 id="获取当前生命周期策略"><a href="#获取当前生命周期策略" class="headerlink" title="获取当前生命周期策略"></a>获取当前生命周期策略</h2><pre><code>GET _ilm/policy</code></pre><h2 id="写入一条数据到logs-login-test数据流中"><a href="#写入一条数据到logs-login-test数据流中" class="headerlink" title="写入一条数据到logs-login-test数据流中"></a>写入一条数据到logs-login-test数据流中</h2><pre><code class="bash">POST /logs-login-test/_doc/&#123;    &quot;@timestamp&quot;: 1663567005424,    &quot;username&quot;: &quot;Jack&quot;,    &quot;title&quot;: &quot;ops&quot;,    &quot;level&quot;: &quot;info&quot;,    &quot;log_type&quot;: &quot;access_log&quot;,    &quot;logger&quot;: &quot;http.log.access.access_logger&quot;,    &quot;start_time&quot;: 1663567005&#125;</code></pre><h2 id="日志中时间戳转换为ISO日期-并调整为Date字段类型"><a href="#日志中时间戳转换为ISO日期-并调整为Date字段类型" class="headerlink" title="日志中时间戳转换为ISO日期, 并调整为Date字段类型"></a>日志中时间戳转换为ISO日期, 并调整为Date字段类型</h2><blockquote><p>需求是把已有str类型的start_time字段转换为ISO日期类型另存为新字段start_time_timestamp,并且调整为date类型字段</p></blockquote><h3 id="一-新增Ingest-Pipelines"><a href="#一-新增Ingest-Pipelines" class="headerlink" title="一. 新增Ingest Pipelines"></a>一. 新增Ingest Pipelines</h3><p>Kibana UI -&gt; Stack Management -&gt; Ingest Pipelines -&gt; Create pipeline -&gt; New Pipeline -&gt; Import Processor<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191553860.png"></p><pre><code class="json">&#123;  &quot;processors&quot;: [    &#123;      &quot;date&quot;: &#123;        &quot;field&quot;: &quot;start_time&quot;,        &quot;formats&quot;: [          &quot;UNIX&quot;        ],        &quot;target_field&quot;: &quot;start_time_timestamp&quot;      &#125;    &#125;  ]&#125;</code></pre><h3 id="二-对已存日志执行Pipelines"><a href="#二-对已存日志执行Pipelines" class="headerlink" title="二. 对已存日志执行Pipelines"></a>二. 对已存日志执行Pipelines</h3><pre><code class="bash">POST logs-login-test/_update_by_query?pipeline=logs-login-test&amp;wait_for_completion=false&#123;  &quot;query&quot;: &#123;    &quot;match_all&quot;: &#123;&#125;  &#125;&#125;</code></pre><p>可以看到已经添加上了新的字段”start_time_timestamp”, 但是还是keyword类型<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191815738.png"></p><h3 id="三-转换字段类型为data"><a href="#三-转换字段类型为data" class="headerlink" title="三. 转换字段类型为data"></a>三. 转换字段类型为data</h3><p>上面转换后的”start_time_timestamp”字段类型还是text,需要转换为时间类型后才可以在Dashboard等地方使用字段作为日期轴, 这就要说到es的template, 我们创建的索引是logs-login-test, 其实是符合es自带的”logs”模板的匹配, 这个模板并没有对任何字段进行mapping设置.<br>参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/data-streams-change-mappings-and-settings.html#:~:text=%22template%22%3A%20%22my%2Ddata%2Dstream%2Dtemplate%22%2C">Change mappings and settings for a data stream</a><br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191604415.png"></p><p>知道了数据流字段类型是通过模板设置的, 那么修改字段类型就的思路就可以是:</p><ol><li>新增模板logs-login-test,优先级高于自带的”logs”, mapping设置”start_time_timestamp”字段类型为”date_nanos”</li></ol><pre><code class="bash">PUT _index_template/logs-login&#123;  &quot;version&quot;: 2,  &quot;priority&quot;: 101,  &quot;template&quot;: &#123;    &quot;mappings&quot;: &#123;      &quot;properties&quot;: &#123;        &quot;start_time_timestamp&quot;: &#123;          &quot;index&quot;: true,          &quot;ignore_malformed&quot;: false,          &quot;store&quot;: false,          &quot;type&quot;: &quot;date_nanos&quot;,          &quot;doc_values&quot;: true        &#125;      &#125;    &#125;  &#125;,  &quot;index_patterns&quot;: [    &quot;logs-login-*&quot;,    &quot;logs-login-test&quot;  ],  &quot;data_stream&quot;: &#123;    &quot;hidden&quot;: false,    &quot;allow_custom_routing&quot;: false  &#125;,  &quot;composed_of&quot;: [    &quot;logs-mappings&quot;,    &quot;data-streams-mappings&quot;,    &quot;logs-settings&quot;  ],  &quot;_meta&quot;: &#123;    &quot;managed&quot;: true,    &quot;description&quot;: &quot;default logs template installed by x-pack&quot;  &#125;&#125;</code></pre><ol start="2"><li>新增符合自建template匹配规则的数据流</li></ol><pre><code class="bash">PUT /_data_stream/logs-login-new</code></pre><ol start="3"><li>获取当前数据流中的索引</li></ol><pre><code class="bash">GET /_resolve/index/logs-login-test</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191724488.png"></p><ol start="4"><li>执行reindex,复制老数据流中数据到新的数据流中</li></ol><pre><code class="bash">POST /_reindex?wait_for_completion=true&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;logs-login-test&quot;  &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;logs-login-new&quot;,    &quot;op_type&quot;: &quot;create&quot;  &#125;&#125;</code></pre><p>这个时候可以看到已经有两条一模一样的数据在两个数据流中,但是新的数据流中”start_time_timestamp”字段已经是”date_nanos”日期格式了.</p><blockquote><p>?wait_for_completion&#x3D;true参数异步执行,会返回一个taskid, 可以通过以下获取任务进度,数据量非常庞大的时候可以用<br>GET _tasks&#x2F;{taskid}</p></blockquote><pre><code class="bash">GET logs-login-new/_mapping</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191745710.png"></p><p>Data view中看到”start_time_timestamp”字段感叹号,是因为旧的数据流中字段类型冲突<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191746329.png"></p><ol start="5"><li>删除旧的数据流</li></ol><pre><code class="bash">DELETE /_data_stream/logs-login-test</code></pre><p>可以看到字段已经改为时间类型了<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191748829.png"></p><ol start="6"><li>测试新数据写入, 还是用logs-login-test数据流,这次应该会使用我们自定义的template ,默认mapping了字段的.</li></ol><pre><code class="bash">POST /logs-login-test/_doc/&#123;    &quot;@timestamp&quot;: 1663581020000,    &quot;username&quot;: &quot;Jack&quot;,    &quot;title&quot;: &quot;ops&quot;,    &quot;level&quot;: &quot;info&quot;,    &quot;log_type&quot;: &quot;access_log&quot;,    &quot;logger&quot;: &quot;http.log.access.access_logger&quot;,    &quot;start_time&quot;: 1663581020,    &quot;start_time_timestamp&quot;: &quot;2022-09-19T09:50:20.000Z&quot;&#125;</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191755736.png"></p><ol start="7"><li>这个时候去kibana创建图表,可以看到日期已经可以选择我们的自定义字段”start_time_timestamp”了<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209191846654.png"></li></ol><h2 id="other"><a href="#other" class="headerlink" title="other"></a>other</h2>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令集合</title>
      <link href="/2022/09/16/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"/>
      <url>/2022/09/16/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h3 id="autossh-反向代理写法"><a href="#autossh-反向代理写法" class="headerlink" title="autossh 反向代理写法"></a>autossh 反向代理写法</h3><pre><code class="bash">ssh -CfN -o TCPKeepAlive=yes -o ServerAliveInterval=10 -o ServerAliveCountMax=3 -L 0.0.0.0:8022:10.0.111.54:27017 oth-gov-nginx</code></pre><hr><h3 id="查看服务器CPU信息"><a href="#查看服务器CPU信息" class="headerlink" title="查看服务器CPU信息"></a>查看服务器CPU信息</h3><p><strong>CPU型号</strong></p><pre><code class="bash">dmidecode |grep -A 8 &quot;System Information&quot;</code></pre><p><strong>CPU核心个数</strong></p><pre><code class="bash">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</code></pre><hr><h3 id="ISO-8601-格式规范下的一年中第几周，以周一为每星期第一天-01-53"><a href="#ISO-8601-格式规范下的一年中第几周，以周一为每星期第一天-01-53" class="headerlink" title="ISO-8601 格式规范下的一年中第几周，以周一为每星期第一天(01-53)"></a>ISO-8601 格式规范下的一年中第几周，以周一为每星期第一天(01-53)</h3><pre><code class="powershell">#!/bin/bashdir=$(date --date= +%y%V)mkdir -p /tmp/test/$&#123;dir&#125;</code></pre><hr><h3 id="ssh远程执行本地脚本"><a href="#ssh远程执行本地脚本" class="headerlink" title="ssh远程执行本地脚本"></a>ssh远程执行本地脚本</h3><p><strong>在spare01服务器上执行本地的sed.sh脚本</strong></p><pre><code class="bash">ssh spare01 bash -s &lt; sed.sh</code></pre><hr><h3 id="Ubuntu解决公钥错误"><a href="#Ubuntu解决公钥错误" class="headerlink" title="Ubuntu解决公钥错误"></a>Ubuntu解决公钥错误</h3><pre><code class="bash">apt-key adv --keyserver keyserver.ubuntu.com --recv-keys B5B116B72D0F61F0# 或sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 76F1A20FF987672F</code></pre><hr><h3 id="查看当前用户公钥在git-xuhandsome-org上的状态"><a href="#查看当前用户公钥在git-xuhandsome-org上的状态" class="headerlink" title="查看当前用户公钥在git.xuhandsome.org上的状态"></a>查看当前用户公钥在git.xuhandsome.org上的状态</h3><pre><code class="bash">ssh -T git@git.xuhandsome.org</code></pre><hr><h3 id="查看磁盘读写状态"><a href="#查看磁盘读写状态" class="headerlink" title="查看磁盘读写状态"></a>查看磁盘读写状态</h3><pre><code class="bash">iotop -oP</code></pre><hr><h3 id="查看系统负载"><a href="#查看系统负载" class="headerlink" title="查看系统负载"></a>查看系统负载</h3><pre><code class="bash">htop</code></pre><hr><h3 id="查看网络负载"><a href="#查看网络负载" class="headerlink" title="查看网络负载"></a>查看网络负载</h3><pre><code class="bash">iftop</code></pre><hr><h3 id="查看文件过滤注释跟空行"><a href="#查看文件过滤注释跟空行" class="headerlink" title="查看文件过滤注释跟空行"></a>查看文件过滤注释跟空行</h3><pre><code class="bash">cat /etc/zabbix/zabbix_agentd.conf|grep -v &quot;#&quot;|grep -v &#39;^$&#39;</code></pre><hr><h3 id="alias设置带参数别名"><a href="#alias设置带参数别名" class="headerlink" title="alias设置带参数别名"></a>alias设置带参数别名</h3><pre><code class="bash">alias rm=&#39;test() &#123; echo Hello World;&#125;; test&#39;</code></pre><hr><h3 id="phpvbox设置控制台远程访问"><a href="#phpvbox设置控制台远程访问" class="headerlink" title="phpvbox设置控制台远程访问"></a>phpvbox设置控制台远程访问</h3><pre><code class="bash">VBoxManage modifyvm proxmox34 --vrdeaddress 0.0.0.0</code></pre><hr><h3 id="rsync-常用传输命令"><a href="#rsync-常用传输命令" class="headerlink" title="rsync 常用传输命令"></a>rsync 常用传输命令</h3><pre><code class="bash">rsync -avpP --log-file=/var/log/rsync-shnotary.log --exclude=README.md --exclude=logs /data/web1.www.sh-notary.gov.cn/* /data/web2.www.sh-notary.gov.cn/</code></pre><table><thead><tr><th>-avpP</th><th>前台显示缩略输出</th></tr></thead><tbody><tr><td>–log-file&#x3D;</td><td>保留输出日志</td></tr><tr><td>–exclude&#x3D;</td><td>排除文件或文件夹</td></tr></tbody></table><hr><h3 id="批量更新theme项目"><a href="#批量更新theme项目" class="headerlink" title="批量更新theme项目"></a>批量更新theme项目</h3><pre><code class="bash">for i in `cat 22.list`;do echo $i;ssh $i &quot;ls -ld /data/web1.$i.justice.org.cn/apps/theme/*&quot;;ssh $i &quot;mkdir /tmp/theme-bak&quot;;ssh $i &quot;mv /data/web1.$i.justice.org.cn/apps/theme/* /tmp/theme-bak/&quot;;ssh $i &quot;cd /data/web1.$i.justice.org.cn/apps/theme;pwd;unzip -q /tmp/theme-distribution-monarch.war&quot;;done</code></pre><hr><h3 id="Telnet-退出"><a href="#Telnet-退出" class="headerlink" title="Telnet 退出"></a>Telnet 退出</h3><p>使用ctrl+],然后quit。</p><pre><code class="bash">telnet 0.0.0.0 8022Trying 0.0.0.0...Connected to 0.0.0.0.Escape character is &#39;^]&#39;.^]telnet&gt; quitConnection closed.</code></pre><hr><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="Linux释放内存"><a href="#Linux释放内存" class="headerlink" title="Linux释放内存"></a>Linux释放内存</h3><pre><code class="bash">sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches</code></pre><hr><h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><h3 id="恢复elastic状态"><a href="#恢复elastic状态" class="headerlink" title="恢复elastic状态"></a>恢复elastic状态</h3><pre><code class="bash">cat &lt;&lt; EOF &gt;&gt; restore.json&#123;    &quot;commands&quot; : [        &#123;          &quot;allocate_stale_primary&quot; : &#123;              &quot;index&quot; : &quot;justice-mediation-sh&quot;, &quot;shard&quot; : 0, &quot;node&quot; : &quot;mXpgPza&quot;,        &quot;accept_data_loss&quot;:true          &#125;        &#125;    ]&#125;EOFcurl -vX POST 0.0.0.0:9200/_cluster/reroute -d @restore.json</code></pre><hr><h3 id="linux制作启动盘"><a href="#linux制作启动盘" class="headerlink" title="linux制作启动盘"></a>linux制作启动盘</h3><pre><code class="bash">$ dd if=~/Downloads/CentOS-7-x86_64-DVD-1511.iso of=/dev/disk2 bs=4</code></pre><blockquote><p>bs 表示写入块大小，可以设置为2m，但不要太大。</p></blockquote><hr><h3 id="ubuntu安装编译套件"><a href="#ubuntu安装编译套件" class="headerlink" title="ubuntu安装编译套件"></a>ubuntu安装编译套件</h3><blockquote><p>gcc, g++, make</p></blockquote><pre><code class="bash">apt-get install build-essential</code></pre><hr><h3 id="重启服务器"><a href="#重启服务器" class="headerlink" title="重启服务器"></a>重启服务器</h3><pre><code class="bash">echo 1 &gt; /proc/sys/kernel/sysrqecho b &gt; /proc/sysrq-trigger</code></pre><hr><h3 id="tmux中开启鼠标滚动"><a href="#tmux中开启鼠标滚动" class="headerlink" title="tmux中开启鼠标滚动"></a>tmux中开启鼠标滚动</h3><pre><code class="bash">tmux actrl+b:set -g mouse on</code></pre><hr><h3 id="rsync-精准传输指定目录"><a href="#rsync-精准传输指定目录" class="headerlink" title="rsync 精准传输指定目录"></a>rsync 精准传输指定目录</h3><blockquote><p>仅传输每个虚拟机id下的appdata目录到s3的&#x2F;data&#x2F;move目录下去</p></blockquote><pre><code class="bash">pwd/var/lib/vz/privatefor i in `ls`;do rsync -avpP --include &quot;appdata&quot; --exclude &quot;$i/*&quot; $i s3:/data/move/;done</code></pre><pre><code class="bash">ssh s3ls -l /data/move/5117/appdata</code></pre><hr><h3 id="apt获取可安装版本"><a href="#apt获取可安装版本" class="headerlink" title="apt获取可安装版本"></a>apt获取可安装版本</h3><pre><code class="bash">apt-cache madison docker-ce docker-ce | 5:19.03.2~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.1~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:19.03.0~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.9~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.8~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.7~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.6~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.5~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.4~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.3~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.2~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.1~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 5:18.09.0~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.06.3~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.06.2~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.06.1~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.06.0~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages docker-ce | 18.03.1~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesapt-get install docker-ce=5:18.09.9~3-0~ubuntu-bionic</code></pre><hr><h3 id="wget下载github的releases"><a href="#wget下载github的releases" class="headerlink" title="wget下载github的releases"></a>wget下载github的releases</h3><pre><code class="bash">wget --no-check-certificate --content-disposition https://github.com/be5invis/Sarasa-Gothic/releases/download/v0.12.5/sarasa-gothic-ttf-0.12.5.7z</code></pre><hr><h3 id="创建lvs的时候使用所有剩余空间"><a href="#创建lvs的时候使用所有剩余空间" class="headerlink" title="创建lvs的时候使用所有剩余空间"></a>创建lvs的时候使用所有剩余空间</h3><pre><code class="bash">lvcreate --name data -l 100%FREE vg_data</code></pre><hr><h3 id="linux设置主机名，同时配置hostname文件及hosts文件"><a href="#linux设置主机名，同时配置hostname文件及hosts文件" class="headerlink" title="linux设置主机名，同时配置hostname文件及hosts文件"></a>linux设置主机名，同时配置hostname文件及hosts文件</h3><pre><code class="bash">hostnamectl --static set-hostname &lt;HostName&gt;</code></pre><hr><h3 id="history历史命令开启上翻"><a href="#history历史命令开启上翻" class="headerlink" title="history历史命令开启上翻"></a>history历史命令开启上翻</h3><pre><code class="bash">cat &lt;&lt;EOF &gt;&gt;/etc/profile.d/bash_history.shif [ -t 1 ];then    # standard output is a tty    # do interactive initialization    bind &#39;&quot;\x1b\x5b\x41&quot;:history-search-backward&#39;    bind &#39;&quot;\x1b\x5b\x42&quot;:history-search-forward&#39;fiEOF</code></pre><hr><h3 id="MacOS中tar打包避免带有备份文件（”-”开头的文件）"><a href="#MacOS中tar打包避免带有备份文件（”-”开头的文件）" class="headerlink" title="MacOS中tar打包避免带有备份文件（”._”开头的文件）"></a>MacOS中tar打包避免带有备份文件（”._”开头的文件）</h3><pre><code class="bash">COPYFILE_DISABLE=1 tar czf taimei-local-deploy-2.0.x86_64.tar.gz taimei-local-deploy</code></pre><hr><h3 id="python快速启动ftp服务下载文件"><a href="#python快速启动ftp服务下载文件" class="headerlink" title="python快速启动ftp服务下载文件"></a>python快速启动ftp服务下载文件</h3><pre><code class="bash">python -m pyftpdlib -p 21</code></pre><hr><h3 id="linux下高压缩率压缩文件"><a href="#linux下高压缩率压缩文件" class="headerlink" title="linux下高压缩率压缩文件"></a>linux下高压缩率压缩文件</h3><pre><code class="bash"># 先打成tar包tar cvf filename.tar file# 再用xz压缩xz -z filename.tar# 解压xz -d filename.tar.xz</code></pre><hr><h3 id="dd生产大文件，可以测试监控告警"><a href="#dd生产大文件，可以测试监控告警" class="headerlink" title="dd生产大文件，可以测试监控告警"></a>dd生产大文件，可以测试监控告警</h3><pre><code class="bash">dd if=/dev/zero of=test bs=1M count=60000# 在当前目录生成一个60G的test文件</code></pre><hr><h3 id="不重启动态扩容EXT4分区类型硬盘（非LVM）"><a href="#不重启动态扩容EXT4分区类型硬盘（非LVM）" class="headerlink" title="不重启动态扩容EXT4分区类型硬盘（非LVM）"></a>不重启动态扩容EXT4分区类型硬盘（非LVM）</h3><pre><code class="bash">[root@registry ~]# lsblkNAME              MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda                 8:0    0   8G  0 disk└─sda1              8:1    0   8G  0 part /sdb                 8:16   0  50G  0 disk└─sdb1              8:17   0  50G  0 part  └─vgdata-lvdata 253:0    0  50G  0 lvm  /datasdc                 8:32   0   5G  0 disk└─sdc1              8:33   0   5G  0 part /taimeisr0                11:0    1   4M  0 rom[root@registry ~]# echo hello,sdc1 &gt; /taimei/test.txt[root@registry ~]# cat /taimei/test.txthello,sdc1# 现在在pve上对sdc硬盘进行扩容5G的操作，扩容后磁盘大小为10G</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209031314853.png"><br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209031315010.png"></p><pre><code class="bash">#现在开始扩容[root@registry ~]# umount /taimei/[root@registry ~]# fdisk /dev/sdc1 # 见下图，先删除现有分区1，新建分区1要和之前的分区1各项参数一模一样，（扩容之前也是全部默认的参数，所以这里也是一路回车）[root@registry ~]# partprobe /dev/sdc[root@registry ~]# e2fsck -f /dev/sdc1[root@registry ~]# resize2fs /dev/sdc1[root@registry ~]# lsblkNAME              MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda                 8:0    0   8G  0 disk└─sda1              8:1    0   8G  0 part /sdb                 8:16   0  50G  0 disk└─sdb1              8:17   0  50G  0 part  └─vgdata-lvdata 253:0    0  50G  0 lvm  /datasdc                 8:32   0  10G  0 disk└─sdc1              8:33   0  10G  0 partsr0                11:0    1   4M  0 rom[root@registry ~]# mount /dev/sdc1 /taimei/[root@registry ~]# cat /taimei/test.txthello,sdc1</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209031315982.png"><br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209031316584.png"></p><hr><h3 id="yum下载软件及依赖rmp包"><a href="#yum下载软件及依赖rmp包" class="headerlink" title="yum下载软件及依赖rmp包"></a>yum下载软件及依赖rmp包</h3><pre><code class="bash">yum install --downloadonly --downloaddir ./ bind-utils</code></pre><hr><h3 id="HDFS数据迁移"><a href="#HDFS数据迁移" class="headerlink" title="HDFS数据迁移"></a>HDFS数据迁移</h3><p>从141开发环境迁移dolphinscheduler目录到10.10.0.4测试环境的hdfs中</p><pre><code class="bash">./bin/hadoop distcp hdfs://192.168.96.141:8020/dolphinscheduler hdfs://10.10.0.4:8020/</code></pre><hr><h3 id="Python3简易http服务"><a href="#Python3简易http服务" class="headerlink" title="Python3简易http服务"></a>Python3简易http服务</h3><pre><code class="bash">python3 -m http.server --bind 0.0.0.0 9527</code></pre><hr><h3 id="tcpdump滚动抓包"><a href="#tcpdump滚动抓包" class="headerlink" title="tcpdump滚动抓包"></a>tcpdump滚动抓包</h3><pre><code class="bash">tcpdump -i any -n -w /tmp/tcpdump.pcap -C 200 -W 5-n 　　　指定将每个监听到数据包中的域名转换成IP地址后显示，不把网络地址转换成名字；-i      指定网卡-w      指定保存为wireshark数据文件pcap-W xx(文件个数)-C 250（大小限制单位M）</code></pre><hr><h3 id="使用mergecap将tcpdump滚动抓的包合并"><a href="#使用mergecap将tcpdump滚动抓的包合并" class="headerlink" title="使用mergecap将tcpdump滚动抓的包合并"></a>使用mergecap将tcpdump滚动抓的包合并</h3><pre><code class="bash">mergecap -w tcpdump.pcap tcpdump.pcap0 tcpdump.pcap1 tcpdump.pcap2 tcpdump.pcap3</code></pre><hr><h3 id="mtr网络链路追踪"><a href="#mtr网络链路追踪" class="headerlink" title="mtr网络链路追踪"></a>mtr网络链路追踪</h3><pre><code class="bash">mtr -c 50 -r www.baidu.com</code></pre><hr><h3 id="统计Linux系统中TCP连接情况"><a href="#统计Linux系统中TCP连接情况" class="headerlink" title="统计Linux系统中TCP连接情况"></a>统计Linux系统中TCP连接情况</h3><pre><code class="bash">netstat -n | awk &#39;/^tcp/ &#123;n=split($(NF-1),array,&quot;:&quot;);if(n&lt;=2)++S[array[(1)]];else++S[array[(4)]];++s[$NF];++N&#125; END &#123;for(a in S)&#123;printf(&quot;%-20s %s\n&quot;, a, S[a]);++I&#125;printf(&quot;%-20s %s\n&quot;,&quot;TOTAL_IP&quot;,I);for(a in s) printf(&quot;%-20s %s\n&quot;,a, s[a]);printf(&quot;%-20s %s\n&quot;,&quot;TOTAL_LINK&quot;,N);&#125;&#39;</code></pre><hr>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> Ops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Openldap-exporter in docker + prometheus operator</title>
      <link href="/2022/09/16/openldap-exporter-in-docker-prometheus-operator/"/>
      <url>/2022/09/16/openldap-exporter-in-docker-prometheus-operator/</url>
      
        <content type="html"><![CDATA[<h2 id="OpenLdap-Exporter-Docker镜像制作"><a href="#OpenLdap-Exporter-Docker镜像制作" class="headerlink" title="OpenLdap-Exporter Docker镜像制作"></a>OpenLdap-Exporter Docker镜像制作</h2><p>Dockerfile已上传<a href="https://github.com/XuHandsome/openldap-exporter-docker.git">GitHub</a></p><h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><pre><code class="Dockerfile">FROM golang:alpine3.16WORKDIR /opt/ENV LC_ALL en_US.utf8ENV EXPORTER_VERSION=v2.2.2RUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&#39; /etc/apk/repositories \    &amp;&amp; apk add gcc g++ make libffi-dev openssl-dev libtool gitRUN go env -w GOPROXY=https://goproxy.cn,direct \    &amp;&amp; git clone -b $EXPORTER_VERSION https://github.com/tomcz/openldap_exporter.git \    &amp;&amp; cd /opt/openldap_exporter \    &amp;&amp; makeFROM alpine:3.16.2WORKDIR /opt/COPY --from=0 /opt/openldap_exporter/target/openldap_exporter .EXPOSE 9330ENTRYPOINT [&quot;./openldap_exporter&quot;]</code></pre><h3 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h3><pre><code class="bash">TAG=&quot;handsomexu/openldap-exporter:v2.2.2&quot;docker build -t $&#123;TAG&#125; .</code></pre><blockquote><p>已推送DockerHub,可以直接使用我构建好的 docker pull handsomexu&#x2F;openldap-exporter:v2.2.2</p></blockquote><h2 id="部署到k8s集群-并接入prometheus-operator"><a href="#部署到k8s集群-并接入prometheus-operator" class="headerlink" title="部署到k8s集群,并接入prometheus operator"></a>部署到k8s集群,并接入prometheus operator</h2><blockquote><p>如果ldap server也是pod运行可以和ldap server以sidecar模式运行, 我这里使用独立pod</p></blockquote><pre><code class="yaml">---apiVersion: v1kind: Secretmetadata:    name: openldap-exporter-secret    namespace: monitoringtype: Opaquedata:  # echo -n &#39;password&#39; | base64 生成  ldapPass: cGFzc3dvcmQ=---apiVersion: apps/v1kind: Deploymentmetadata:  name: openldap-exporter  namespace: monitoring  labels:    k8s-app: openldap-exporterspec:  selector:    matchLabels:      k8s-app: openldap-exporter  strategy:    type: Recreate  replicas: 1  template:    metadata:      labels:        k8s-app: openldap-exporter    spec:      nodeSelector:        kubernetes.io/hostname: 10.22.19.34      dnsPolicy: ClusterFirst      containers:        - name: openldap-exporter          image: handsomexu/openldap-exporter:v2.2.2          imagePullPolicy: IfNotPresent          command: [&quot;/opt/openldap_exporter&quot;]          args: [ &quot;--ldapAddr&quot;, &quot;192.168.10.35:30389&quot;, &quot;--ldapUser&quot;, &quot;cn=root,dc=xuhandsome,dc=com&quot;, &quot;--ldapPass&quot;, &quot;$(LDAPPASS)&quot; ]          env:            - name: LDAPPASS              valueFrom:                secretKeyRef:                  name: openldap-exporter-secret                  key: ldapPass                  optional: false          resources:            limits:              cpu: 200m              memory: 500Mi            requests:              cpu: 200m              memory: 500Mi          securityContext:            runAsUser: 0          ports:            - name: ldap-exporter              containerPort: 9330              protocol: TCP          livenessProbe:            exec:              command:              - nc              - -zv              - 127.0.0.1              - &quot;9330&quot;            failureThreshold: 3            periodSeconds: 10            successThreshold: 1            timeoutSeconds: 1          readinessProbe:            exec:              command:              - nc              - -zv              - 127.0.0.1              - &quot;9330&quot;            failureThreshold: 3            periodSeconds: 10            successThreshold: 1            timeoutSeconds: 1---apiVersion: v1kind: Servicemetadata:  labels:    k8s-app: openldap-exporter  name: openldap-exporter  namespace: monitoringspec:  ports:  - name: metrics    port: 9330    protocol: TCP    targetPort: 9330  selector:    k8s-app: openldap-exporter  type: ClusterIP# 添加servicemonitor资源---apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: openldap-exporter  namespace: monitoring  labels:    app.kubernetes.io/name: openldap-exporterspec:  endpoints:  - interval: 30s    port: metrics    scheme: http  jobLabel: openldap-exporter  namespaceSelector:    matchNames:    - monitoring  selector:    matchLabels:      k8s-app: openldap-exporter</code></pre><h2 id="监控接入效果"><a href="#监控接入效果" class="headerlink" title="监控接入效果"></a>监控接入效果</h2><p>访问Prometheus UI可以看到openldap-exporter这个job里已经接进去了我们的target<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209161958186.png"></p>]]></content>
      
      
      <categories>
          
          <category> 云原生 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> openldap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch8操作踩坑记录</title>
      <link href="/2022/09/14/Elasticsearch8%E6%93%8D%E4%BD%9C%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
      <url>/2022/09/14/Elasticsearch8%E6%93%8D%E4%BD%9C%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="kibana创建pipeline后对索引执行发现500报错"><a href="#kibana创建pipeline后对索引执行发现500报错" class="headerlink" title="kibana创建pipeline后对索引执行发现500报错"></a>kibana创建pipeline后对索引执行发现500报错</h2><blockquote><p>reason: “There are no ingest nodes in this cluster, unable to forward request to an ingest node”<br>这是说es8集群中缺少ingest角色节点</p></blockquote><p>可以新增或者找一台数据节点添加”ingest”角色</p><pre><code class="bash">#修改配置文件cat /etc/elasticsearch/elasticsearch.yml | grep &quot;node.roles&quot;node.roles: [ data, ingest ]# 重启es节点systemctl restart elasticsearch</code></pre><h2 id="对索引执行删除-修改时发现403报错"><a href="#对索引执行删除-修改时发现403报错" class="headerlink" title="对索引执行删除\修改时发现403报错"></a>对索引执行删除\修改时发现403报错</h2><blockquote><p>message: “blocked by: [FORBIDDEN&#x2F;8&#x2F;index write (api)];: [cluster_block_exception] blocked by: [FORBIDDEN&#x2F;8&#x2F;index write (api)];”<br>由于集群生命周期策略配置了日志索引在一段时间后转存到冷节点上, 索引配置中会添加上写锁</p></blockquote><pre><code class="bash">GET app_access_log*/_settings&#123;  &quot;index.blocks.write&quot;: true&#125;</code></pre><p>可以去掉index.blocks.write锁</p><pre><code class="bash">PUT app_access_log*/_settings&#123;  &quot;index.blocks.write&quot;: null&#125;</code></pre><h2 id="对索引执行删除-修改时发现409报错"><a href="#对索引执行删除-修改时发现409报错" class="headerlink" title="对索引执行删除\修改时发现409报错"></a>对索引执行删除\修改时发现409报错</h2><blockquote><p>message: “version conflict, required seqNo [xxxxx0], primary term [1]. current document has seqNo [xxxxx1] and primary term [xxxxx0]”<br>出现这个的原因主要是因为使用随机_id，es内的乐观锁造成的索引修改不够完全，导致版本号冲突, 可以在操作索引的时候加上<strong>wait_for_completion&#x3D;false</strong>取消等待队列.</p></blockquote><pre><code class="bash">POST app_access_log*/_update_by_query?pipeline=logs-yunshu&amp;wait_for_completion=false&#123;  &quot;query&quot;: &#123;    &quot;match_all&quot;: &#123;&#125;  &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dell服务器通过Ansible批量配置Raid0</title>
      <link href="/2022/09/14/Dell%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%9A%E8%BF%87Ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AERaid0/"/>
      <url>/2022/09/14/Dell%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%9A%E8%BF%87Ansible%E6%89%B9%E9%87%8F%E9%85%8D%E7%BD%AERaid0/</url>
      
        <content type="html"><![CDATA[<h2 id="安装Dell-Ansible"><a href="#安装Dell-Ansible" class="headerlink" title="安装Dell-Ansible"></a>安装Dell-Ansible</h2><p>环境要求</p><blockquote><p>Ansible &gt;&#x3D; 2.13.0<br>Python &gt;&#x3D; 3.9.6</p></blockquote><pre><code class="bash">pip install omsdk --upgrade</code></pre><h2 id="playbook-example"><a href="#playbook-example" class="headerlink" title="playbook example"></a>playbook example</h2><blockquote><p>这里给192.168.40.1（idrac地址10.10.16.121:443）新增的12块HDD盘配置raid0</p></blockquote><h3 id="inventory文件-hosts"><a href="#inventory文件-hosts" class="headerlink" title="inventory文件 hosts"></a>inventory文件 hosts</h3><pre><code class="yaml">[dell]127.0.0.1[dell:vars]ansible_ssh_user=&#39;root&#39;ansible_ssh_port=22ansible_ssh_pass=&#39;xxxxxxxxxxxx&#39;idrac_username=&#39;root&#39;idrac_password=&#39;xxxxxxxxxxxx&#39;</code></pre><h3 id="playbook文件-makeraid-yml"><a href="#playbook文件-makeraid-yml" class="headerlink" title="playbook文件 makeraid.yml"></a>playbook文件 makeraid.yml</h3><pre><code class="yaml">## https://github.com/dell/dellemc-openmanage-ansible-modules/blob/collections/docs/modules/redfish_storage_volume.rst- hosts: dell  connection: local  name: Conversion Disk To RAID0  gather_facts: False  tasks:    - name: Create a volume with minimum options      dellemc.openmanage.redfish_storage_volume:        baseuri: &quot;10.10.16.121:443&quot;        username: &quot;&#123;&#123; idrac_username &#125;&#125;&quot;        password: &quot;&#123;&#123; idrac_password &#125;&#125;&quot;        validate_certs: False        state: &quot;present&quot;        # 每个逻辑硬盘的磁盘名称        name: &quot;Virtual-Disk-&#123;&#123; item|int - 1 &#125;&#125;&quot;        controller_id: &quot;RAID.SL.3-1&quot;        volume_type: &quot;NonRedundant&quot;        drives:          # 待制作raid0物理盘名称          - &quot;Disk.Bay.&#123;&#123; item &#125;&#125;:Enclosure.Internal.0-1:RAID.SL.3-1&quot;      # item范围从2-13，因为系统盘已经用磁盘0及磁盘1做了raid0了，待做raid的磁盘id就是2至13      with_sequence: 2-13      loop_control:        # 每次硬盘制作raid任务等待2分钟，因为idrac上的raid任务只能单线程        pause: 120      tags:        - convert-raid</code></pre><h3 id="执行playbook"><a href="#执行playbook" class="headerlink" title="执行playbook"></a>执行playbook</h3><pre><code class="bash">ansible -i hosts makeraid.yml</code></pre><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>运行过程中及运行完成后，可以在idrac任务列表中查看制作结果<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209141221546.png"></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> debian </tag>
            
            <tag> dell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Debian11安装DellRacadm+DCISM</title>
      <link href="/2022/09/14/Debian11-PVE7-1-%E5%AE%89%E8%A3%85DellRacadm-DCISM/"/>
      <url>/2022/09/14/Debian11-PVE7-1-%E5%AE%89%E8%A3%85DellRacadm-DCISM/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://www.dell.com/support/kbdoc/zh-cn/000126308/export-a-supportassist-collection-via-idrac9">通过 iDRAC9 导出 SupportAssist 收集</a></p><ol><li>通过Dell idrac控制台的技术支持发起检修Case的时候会提示需要安装ISM, 通过官方挂载的安装介质脚本安装是不支持Debian 11的<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209141130701.png"><br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209141132197.png"></li><li>Dell官仓未提供Debian11的apt安装源</li></ol><h2 id="配置Debian11-apt源"><a href="#配置Debian11-apt源" class="headerlink" title="配置Debian11 apt源"></a>配置Debian11 apt源</h2><pre><code class="bash">echo &quot;deb http://linux.dell.com/repo/community/openmanage/940/bionic bionic main&quot; &gt; /etc/apt/sources.list.d/dell.listapt-key adv --keyserver keyserver.ubuntu.com --recv-keys 1285491434D8786Fapt-get update</code></pre><h2 id="安装dcism-idracadm8"><a href="#安装dcism-idracadm8" class="headerlink" title="安装dcism+idracadm8"></a>安装dcism+idracadm8</h2><pre><code class="bash">apt-get install dcism srvadmin-idracadm8 srvadmin-hapi[[ ! -f /usr/lib/x86_64-linux-gnu/libssl.so ]] &amp;&amp; ln -s /usr/lib/x86_64-linux-gnu/libssl.so.1.1 /usr/lib/x86_64-linux-gnu/libssl.so</code></pre><h2 id="启动dcismeng服务"><a href="#启动dcismeng服务" class="headerlink" title="启动dcismeng服务"></a>启动dcismeng服务</h2><pre><code class="bash">systemctl start dcismeng.service</code></pre><h2 id="IDrac-GUI验证"><a href="#IDrac-GUI验证" class="headerlink" title="IDrac GUI验证"></a>IDrac GUI验证</h2><p>Maintenance → SupportAssist → 开始收集 ,即可开始收集系统日志<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209141139568.png"><br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209141140831.png"></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> debian </tag>
            
            <tag> dell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux网卡Bond后获取真实MAC地址</title>
      <link href="/2022/09/09/Linux%E7%BD%91%E5%8D%A1Bond%E5%90%8E%E8%8E%B7%E5%8F%96%E7%9C%9F%E5%AE%9EMAC%E5%9C%B0%E5%9D%80/"/>
      <url>/2022/09/09/Linux%E7%BD%91%E5%8D%A1Bond%E5%90%8E%E8%8E%B7%E5%8F%96%E7%9C%9F%E5%AE%9EMAC%E5%9C%B0%E5%9D%80/</url>
      
        <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>在生产环境中,一般采用双网卡组Bond的形式来保障网卡通讯的高可用性, 组完Bond网卡后,可以发现, slave网卡的mac地址变成了和bond卡一样的了,这样不便于后期运维工作中对网卡真实位置的溯源, 如下, 可以看到全都变成了与bond网卡mac地址一致的<strong>da:87:b2:1c:9f:f1</strong>了</p><blockquote><p>Master: bond0<br>Slave: enp23s0f0np0 + eno12399np0<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209091223174.png"></p></blockquote><h2 id="查看Slave网卡真实MAC地址的方法"><a href="#查看Slave网卡真实MAC地址的方法" class="headerlink" title="查看Slave网卡真实MAC地址的方法"></a>查看Slave网卡真实MAC地址的方法</h2><pre><code class="bash">nic=enp23s0f0np0cat /sys/class/net/$&#123;nic&#125;/bonding_slave/perm_hwaddrnic=eno12399np0cat /sys/class/net/$&#123;nic&#125;/bonding_slave/perm_hwaddr</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209091225941.png"></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> Ops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PVE嵌套虚拟化</title>
      <link href="/2022/09/06/PVE%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
      <url>/2022/09/06/PVE%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="PVE嵌套虚拟化"><a href="#PVE嵌套虚拟化" class="headerlink" title="PVE嵌套虚拟化"></a>PVE嵌套虚拟化</h2><h3 id="一、场景"><a href="#一、场景" class="headerlink" title="一、场景"></a>一、场景</h3><p>需要对pve集群级高危操作时，需要有一套测试场景随便折腾，又不担心集群崩掉，所以部署一套pve测试集群时可以用到。</p><ol><li><p>现有三节点物理机PVE集群<br>pve-01<br>pve-02<br>pve-03</p></li><li><p>现在三个节点上分别通过<a href="https://www.proxmox.com/en/downloads?task=callelement&format=raw&item_id=638&element=f85c494b-2b32-4109-b8c1-083cca2b7db6&method=download&args%5B0%5D=e916bdee8a960d7977b7cdf9439138dd">proxmox-ve_7.1-2.iso</a>安装 pve 系统并组成新的 pve 集群.</p></li><li><p>资源规划</p></li></ol><table><thead><tr><th>hostname</th><th>宿主机</th><th>vmid</th><th>ipaddr</th><th>gateway</th><th>prefix</th><th>dns</th><th>cpu</th><th>内存</th><th>系统盘</th><th>数据盘</th></tr></thead><tbody><tr><td>pve-qa-7-1</td><td>pve-01</td><td>200</td><td>192.168.7.1</td><td>192.168.7.254</td><td>22</td><td>192.168.7.250&#x2F;192.168.7.251</td><td>4C</td><td>8G</td><td>30G</td><td>40G</td></tr><tr><td>pve-qa-7-2</td><td>pve-02</td><td>201</td><td>192.168.7.2</td><td>192.168.7.254</td><td>22</td><td>192.168.7.250&#x2F;192.168.7.251</td><td>4C</td><td>8G</td><td>30G</td><td>40G</td></tr><tr><td>pve-qa-7-3</td><td>pve-03</td><td>202</td><td>192.168.7.3</td><td>192.168.7.254</td><td>22</td><td>192.168.7.250&#x2F;192.168.7.251</td><td>4C</td><td>8G</td><td>30G</td><td>40G</td></tr></tbody></table><h3 id="二、创建PVE-VM"><a href="#二、创建PVE-VM" class="headerlink" title="二、创建PVE VM"></a>二、创建PVE VM</h3><ol><li><p>上传<a href="https://www.proxmox.com/en/downloads?task=callelement&format=raw&item_id=638&element=f85c494b-2b32-4109-b8c1-083cca2b7db6&method=download&args%5B0%5D=e916bdee8a960d7977b7cdf9439138dd">proxmox-ve_7.1-2.iso</a>到物理pve节点&#x2F;var&#x2F;lib&#x2F;vz&#x2F;template&#x2F;iso目录下</p></li><li><p>物理pve控制台，分别创建三台pve vm</p></li></ol><ul><li>嵌套虚拟化需要注意的是: 创建vm时, cpu类型要选择”host”,直通宿主机,不然启动pve安装后会报错如下图<img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061215152.png" style="zoom:50%;" /></li></ul><p>我这里通过命令行创建vm及各自的vdisk</p><blockquote><p>create vm pve-qa-7-1 in pve-01</p></blockquote><pre><code class="bash"># 创建系统盘及数据盘逻辑卷lvcreate -L 30G -n vm-200-disk-0 local-lvm-MassStorage-pve-01lvcreate -L 40G -n vm-200-disk-1 local-lvm-MassStorage-pve-01cat &lt;&lt;EOF &gt;/etc/pve/qemu-server/200.confboot: order=scsi0;ide2;net0cores: 4cpu: hostide2: local:iso/proxmox-ve_7.1-2.iso,media=cdrommemory: 16384meta: creation-qemu=6.1.0,ctime=1662435658name: pve-qa-7-1net0: virtio=92:3F:01:75:2C:71,bridge=vmbr0,tag=2020numa: 0ostype: l26scsi0: local-lvm-MassStorage-pve-01:vm-200-disk-0,size=30Gscsi1: local-lvm-MassStorage-pve-01:vm-200-disk-1,size=40Gscsihw: virtio-scsi-pcismbios1: uuid=daf64489-a8c5-432b-9596-61cb58002ab1sockets: 1vmgenid: f5824f42-4c8f-472b-aee8-b1f12ac9ef31EOFqm start 200</code></pre><blockquote><p>create vm pve-qa-7-2 in pve-02</p></blockquote><pre><code class="bash"># 创建系统盘及数据盘逻辑卷lvcreate -L 30G -n vm-201-disk-0 local-lvm-MassStorage-pve-02lvcreate -L 40G -n vm-201-disk-1 local-lvm-MassStorage-pve-02cat &lt;&lt;EOF &gt;/etc/pve/qemu-server/201.confboot: order=scsi0;ide2;net0cores: 4cpu: hostide2: local:iso/proxmox-ve_7.1-2.iso,media=cdrommemory: 16384meta: creation-qemu=6.1.0,ctime=1662435658name: pve-qa-7-2net0: virtio=92:3F:01:75:2C:72,bridge=vmbr0,tag=2020numa: 0ostype: l26scsi0: local-lvm-MassStorage-pve-02:vm-201-disk-0,size=30Gscsi1: local-lvm-MassStorage-pve-02:vm-201-disk-1,size=40Gscsihw: virtio-scsi-pcismbios1: uuid=daf64489-a8c5-432b-9596-61cb58002ab2sockets: 1vmgenid: f5824f42-4c8f-472b-aee8-b1f12ac9ef32EOFqm start 201</code></pre><blockquote><p>create vm pve-qa-7-3 in pve-03</p></blockquote><pre><code class="bash"># 创建系统盘及数据盘逻辑卷lvcreate -L 30G -n vm-202-disk-0 local-lvm-MassStorage-pve-03lvcreate -L 40G -n vm-202-disk-1 local-lvm-MassStorage-pve-03cat &lt;&lt;EOF &gt;/etc/pve/qemu-server/202.confboot: order=scsi0;ide2;net0cores: 4cpu: hostide2: local:iso/proxmox-ve_7.1-2.iso,media=cdrommemory: 16384meta: creation-qemu=6.1.0,ctime=1662435658name: pve-qa-7-3net0: virtio=92:3F:01:75:2C:73,bridge=vmbr0,tag=2020numa: 0ostype: l26scsi0: local-lvm-MassStorage-pve-03:vm-202-disk-0,size=30Gscsi1: local-lvm-MassStorage-pve-03:vm-202-disk-1,size=40Gscsihw: virtio-scsi-pcismbios1: uuid=daf64489-a8c5-432b-9596-61cb58002ab3sockets: 1vmgenid: f5824f42-4c8f-472b-aee8-b1f12ac9ef33EOFqm start 202</code></pre><ol start="3"><li><p>VNC安装系统不在累述</p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061218483.png" style="zoom:50%;" /><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061218615.png" style="zoom:50%;" /><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061219000.png" style="zoom:50%;" /><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061220195.png" style="zoom:50%;" /><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061222969.png" style="zoom:50%;" /><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061223188.png" style="zoom:50%;" /></li><li><p>查看cpu是否支持虚拟化</p></li></ol><pre><code class="bash">#没有输出即不支持，否则会高亮显示vmx或者svm。egrep --color &#39;vmx|svm&#39; /proc/cpuinfo</code></pre><h3 id="三、PVE集群初始化"><a href="#三、PVE集群初始化" class="headerlink" title="三、PVE集群初始化"></a>三、PVE集群初始化</h3><p>使用<a href="https://github.com/XuHandsome/init-pve-cluster">init-pve-cluster</a>项目进行集群初始化即可</p><p>最终集群效果如下图:<br><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209061632255.png" style="zoom:50%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ops </tag>
            
            <tag> pve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>debian11清华大学源</title>
      <link href="/2022/09/05/debian11%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E6%BA%90/"/>
      <url>/2022/09/05/debian11%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E6%BA%90/</url>
      
        <content type="html"><![CDATA[<pre><code class="bash">cat &gt; /etc/apt/sources.list &lt;&lt; EOFdeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-freeEOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> Ops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows开启WinRM</title>
      <link href="/2022/09/02/Windows%E5%BC%80%E5%90%AFWinRM/"/>
      <url>/2022/09/02/Windows%E5%BC%80%E5%90%AFWinRM/</url>
      
        <content type="html"><![CDATA[<h2 id="新建powershell脚本"><a href="#新建powershell脚本" class="headerlink" title="新建powershell脚本"></a>新建powershell脚本</h2><blockquote><p>这里的powershell脚本配合阿里云批量分发执行,比较方便,可以批量配置windows server<br>桌面新建文本,内容以下:</p></blockquote><pre><code class="powershell"># powershell.exe -executionpolicy remotesigned -noexit c:\users\jack\desktop\openwinrm.ps1echo &quot;setp 1...&quot;set-executionpolicy remotesignedecho &quot;setp 2...&quot;winrm quickconfigecho &quot;setp 3...&quot;winrm set winrm/config/service/auth &#39;@&#123;Basic=&quot;true&quot;&#125;&#39;echo &quot;setp 4...&quot;winrm set winrm/config/service &#39;@&#123;AllowUnencrypted=&quot;true&quot;&#125;&#39;$now = Get-Date -Format &quot;yyyy-MM-dd HH:mm:ss&quot;&quot;[&#123;0&#125;][&#123;1&#125;] Success ....&quot; -f $now,$pidExit 0</code></pre><p>保存后,修改文件名为<strong>openwinrm.ps1</strong></p><h2 id="执行开启winrm"><a href="#执行开启winrm" class="headerlink" title="执行开启winrm"></a>执行开启winrm</h2><pre><code class="powershell"># powershell.exe -executionpolicy remotesigned -noexit c:\users\jack\desktop\openwinrm.ps1</code></pre><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><pre><code class="powershell">curl -vk -d &quot; &quot; -u &quot;jack:password&quot; https://192.168.1.206:5986/wsman</code></pre><p><img src="https://ohops.oss-cn-shanghai.aliyuncs.com/202209031045012.png"></p><pre><code class="bash">ansible -i hosts 192.168.1.206 -m win_ping</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Consul集群部署</title>
      <link href="/2022/09/02/Consul%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"/>
      <url>/2022/09/02/Consul%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>Consul在我这里的应用场景有两个:</p><ol><li>注册中心: Prometheus基于Consul的自动Target发现, 加上CMDB系统可实现监控自动录入</li><li>配置中心: Prometheus的rule以k&#x2F;v的形式存放在Consul注册中心中,然后通过<a href="https://github.com/kelseyhightower/confd">confd</a>实时监听,模板映射成rule文件,实现Prometheus告警规则统一界面管理.</li></ol><p>本文只写部署, 后面有时间再补充如何实现的自动化监控体系(在上家公司做的,已经好久了)</p><table><thead><tr><th>主机名</th><th>角色</th><th>内网IP</th><th>配置目录</th><th>数据目录</th></tr></thead><tbody><tr><td>shb-manager-mw-consul-node01</td><td>master、client</td><td>192.168.63.217</td><td>&#x2F;etc&#x2F;consul.d&#x2F;</td><td>&#x2F;data&#x2F;consul</td></tr><tr><td>shb-manager-mw-consul-node02</td><td>master、client</td><td>192.168.63.218</td><td>&#x2F;etc&#x2F;consul.d&#x2F;</td><td>&#x2F;data&#x2F;consul</td></tr><tr><td>shb-manager-mw-consul-node03</td><td>master、client</td><td>192.168.63.219</td><td>&#x2F;etc&#x2F;consul.d&#x2F;</td><td>&#x2F;data&#x2F;consul</td></tr></tbody></table><p>阿里云slb：<br>内网负载<br>192.168.63.205:8500<br>后端服务器组：<br>192.168.63.217:8500<br>192.168.63.218:8500<br>192.168.63.219:8500</p><h2 id="1-下载最新物料包"><a href="#1-下载最新物料包" class="headerlink" title="1. 下载最新物料包"></a>1. 下载最新物料包</h2><p><strong>所有主机</strong></p><pre><code class="bash">cd /optwget -c https://releases.hashicorp.com/consul/1.10.3/consul_1.10.3_linux_amd64.zip</code></pre><h2 id="2-解压安装"><a href="#2-解压安装" class="headerlink" title="2. 解压安装"></a>2. 解压安装</h2><p><strong>所有主机</strong></p><pre><code class="bash">unzip -q consul_1.10.3_linux_amd64.zipmv consul /usr/local/bin/consul -autocomplete-installcomplete -C /usr/local/bin/consul consul</code></pre><h2 id="3-创建配置文件、数据目录"><a href="#3-创建配置文件、数据目录" class="headerlink" title="3. 创建配置文件、数据目录"></a>3. 创建配置文件、数据目录</h2><p><strong>所有主机</strong></p><pre><code class="bash">mkdir -p /etc/consul.d/ /data/consuluseradd --system --home /etc/consul.d --shell /bin/false consulmkdir --parents /data/consulchown --recursive consul:consul /data/consul /etc/consul.d</code></pre><h2 id="4-创建配置文件"><a href="#4-创建配置文件" class="headerlink" title="4. 创建配置文件"></a>4. 创建配置文件</h2><p><strong>node1上执行,获取encrypt</strong></p><pre><code class="bash">consul keygenmVINgJxtdZGy8SMlKYNFFbaBEjVSHChlVPlLjvfjnII=</code></pre><p><strong>所有主机执行，注意node名改为各自的信息</strong></p><pre><code class="bash">cat &lt;&lt; EOF &gt; /etc/consul.d/consul.hcldatacenter = &quot;aliyun&quot;node_name = &quot;node01&quot;data_dir = &quot;/data/consul&quot;enable_syslog = truelog_level = &quot;INFO&quot;retry_join = [&quot;192.168.63.217&quot;, &quot;192.168.63.218&quot;, &quot;192.168.63.219&quot;]start_join = [&quot;192.168.63.217&quot;, &quot;192.168.63.218&quot;, &quot;192.168.63.219&quot;]retry_interval = &quot;30s&quot;rejoin_after_leave = trueclient_addr = &quot;0.0.0.0&quot;bind_addr = &quot;0.0.0.0&quot;encrypt = &quot;mVINgJxtdZGy8SMlKYNFFbaBEjVSHChlVPlLjvfjnII=&quot;performance &#123;  raft_multiplier = 1&#125;limits &#123;  http_max_conns_per_client = 2000&#125;EOF</code></pre><p><strong>所有主机执行</strong></p><pre><code class="bash">cat &lt;&lt; EOF &gt; /etc/consul.d/server.hclserver = truebootstrap_expect = 3ui = trueEOF</code></pre><h2 id="5-systemd守护进程"><a href="#5-systemd守护进程" class="headerlink" title="5. systemd守护进程"></a>5. systemd守护进程</h2><pre><code class="bash">cat &lt;&lt; EOF &gt; /etc/systemd/system/consul.service[Unit]Description=&quot;HashiCorp Consul - A service mesh solution&quot;Documentation=https://www.consul.io/Requires=network-online.targetAfter=network-online.targetConditionFileNotEmpty=/etc/consul.d/consul.hcl[Service]User=consulGroup=consulExecStart=/usr/local/bin/consul agent -config-dir=/etc/consul.d/ExecReload=/usr/local/bin/consul reloadKillMode=processRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF</code></pre><pre><code class="bash">systemctl daemon-reloadsystemctl enable consulsystemctl start consul</code></pre><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="6-常用命令"><a href="#6-常用命令" class="headerlink" title="6. 常用命令"></a>6. 常用命令</h2><pre><code class="bash"># 集群状态consul members# 获取集群节点curl &quot;192.168.63.217:8300/v1/status/peers&quot;# 获取leader节点curl &quot;192.168.63.217:8500/v1/status/leader&quot;# 注册servicecat &lt;&lt; EOF &gt; node_exporter.json&#123;  &quot;ID&quot;: &quot;node-exporter-192.168.1.36&quot;,  &quot;Name&quot;: &quot;node-exporter-master-36&quot;,  &quot;Tags&quot;: [    &quot;aliyun&quot;,    &quot;bendihua&quot;,    &quot;linux&quot;  ],  &quot;Address&quot;: &quot;192.168.1.36&quot;,  &quot;Port&quot;: 9100,  &quot;Meta&quot;: &#123;    &quot;env&quot;: &quot;test&quot;,    &quot;hostname&quot;: &quot;master-36&quot;,    &quot;instance&quot;: &quot;192.168.1.36&quot;,    &quot;os&quot;: &quot;centos7&quot;  &#125;,  &quot;EnableTagOverride&quot;: false,  &quot;Check&quot;: &#123;    &quot;HTTP&quot;: &quot;http://192.168.1.36:9100/metrics&quot;,    &quot;Interval&quot;: &quot;10s&quot;  &#125;,  &quot;Weights&quot;: &#123;    &quot;Passing&quot;: 10,    &quot;Warning&quot;: 1  &#125;&#125;EOFcurl --request PUT --data @node_exporter.json http://192.168.63.205:8500/v1/agent/service/register# 删除servicecurl -X PUT http://192.168.63.205:8500/v1/agent/service/deregister/node-exporter-192.168.1.36# 更新service与注册方式一样，相同的json文件中修改信息后put提交即为更新`# 获取所有serviceidcurl http://192.168.63.205:8500/v1/health/state/any | python -m json.tool | grep ServiceID | awk &#39;&#123;print $2&#125;&#39; |sed &#39;s/&quot;//g&#39; | sed &#39;s/,//g&#39;# 批量删除不健康的serviceservicelist=$(curl http://192.168.63.205:8500/v1/health/state/critical | python -m json.tool | grep ServiceID | awk &#39;&#123;print $2&#125;&#39; |sed &#39;s/&quot;//g&#39; | sed &#39;s/,//g&#39;|grep jvm)for i in $servicelist ;do    echo $i;  curl -X PUT http://192.168.63.205:8500/v1/agent/service/deregister/$&#123;i&#125;done# 批量删除所有状态的serviceservicelist=$(curl http://192.168.63.205:8500/v1/health/state/any | python -m json.tool | grep ServiceID | awk &#39;&#123;print $2&#125;&#39; |sed &#39;s/&quot;//g&#39; | sed &#39;s/,//g&#39;|grep jvm)for i in $servicelist ;do    echo $i;  curl -X PUT http://192.168.63.205:8500/v1/agent/service/deregister/$&#123;i&#125;done# 用命令行方式获取所有services列表consul catalog services|grep jvm &gt; /tmp/jvm-list# 用命令行方式批量删除servicesfor i in $(cat /tmp/jvm-list) ;do    echo $i;  curl -X PUT http://127.0.0.1:8500/v1/agent/service/deregister/$&#123;i&#125;done</code></pre><h2 id="7-访问入口"><a href="#7-访问入口" class="headerlink" title="7. 访问入口"></a>7. 访问入口</h2><p><a href="http://192.168.63.205:8500/">http://192.168.63.205:8500/</a></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>使用agent接口注册与删除服务有一个问题,就是前面挂了一个slb到三个agent节点,注册的service也是分散再各节点的, 发现还有一个catalog接口注册的办法,参考<br><a href="https://blog.51cto.com/l0vesql/2489813">https://blog.51cto.com/l0vesql/2489813</a><br><a href="https://edgar615.github.io/consul-service-register.html">https://edgar615.github.io/consul-service-register.html</a><br><a href="https://www.cnblogs.com/Qing-840/p/10144184.html">https://www.cnblogs.com/Qing-840/p/10144184.html</a><br><a href="https://edgar615.github.io/consul-service-register.html">https://edgar615.github.io/consul-service-register.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> consul </tag>
            
            <tag> 服务注册中心 </tag>
            
            <tag> 配置中心 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bind9主从+实时同步阿里云PrivateZone</title>
      <link href="/2022/09/02/Bind9%E4%B8%BB%E4%BB%8E-%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E9%98%BF%E9%87%8C%E4%BA%91PrivateZone/"/>
      <url>/2022/09/02/Bind9%E4%B8%BB%E4%BB%8E-%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5%E9%98%BF%E9%87%8C%E4%BA%91PrivateZone/</url>
      
        <content type="html"><![CDATA[<h2 id="一、节点规划"><a href="#一、节点规划" class="headerlink" title="一、节点规划"></a>一、节点规划</h2><table><thead><tr><th>角色</th><th>IP</th><th>服务及版本号</th><th>配置文件</th></tr></thead><tbody><tr><td>Master</td><td>192.168.1.250</td><td>bind v9.11.4</td><td>&#x2F;etc&#x2F;named.conf、&#x2F;etc&#x2F;named.rfc1912.zones</td></tr><tr><td>Slave</td><td>192.168.1.251</td><td>bind v9.11.4</td><td>&#x2F;etc&#x2F;named.conf、&#x2F;etc&#x2F;named.rfc1912.zones</td></tr><tr><td>Client</td><td>192.168.1.5</td><td>bind-utils</td><td></td></tr></tbody></table><h2 id="二、服务安装"><a href="#二、服务安装" class="headerlink" title="二、服务安装"></a>二、服务安装</h2><p>主从节点都需要通过yum安装bind软件包，通过<strong>named</strong>守护进程维护</p><pre><code class="bash">yum install -y bindsystemctl enable --now named</code></pre><p>关闭IPV6传输，避免出现network unreachable resolving报错</p><pre><code class="bash">echo OPTIONS=\&quot;-4\&quot; &gt;&gt; /etc/sysconfig/namedsystemctl restart named</code></pre><p>服务配置文件  &#x2F;etc&#x2F;named.conf 中 incloud 了 &#x2F;etc&#x2F;named.rfc1912.zones<br>一般域配置文件放在 &#x2F;var&#x2F;named目录下</p><h2 id="三、主DNS服务器配置"><a href="#三、主DNS服务器配置" class="headerlink" title="三、主DNS服务器配置"></a>三、主DNS服务器配置</h2><ol><li>&#x2F;etc&#x2F;named.rfc1912.zones中新增正向域配置, master模式<blockquote><p>添加一个正向解析的区域，当需要查询的域名的根域名为xuhandsome.org时均会查询该区域，后面阿里云PrivateZone会同步到这个域。</p></blockquote></li></ol><pre><code class="bash">cat &lt;&lt;EOF&gt;&gt; /etc/named.rfc1912.zoneszone &quot;xuhandsome.org&quot; IN &#123;        type master;        // 这里的zone文件在相对路径/var/named目录下        file &quot;xuhandsome.org.zone&quot;;        allow-update &#123; 127.0.0.1; &#125;;        allow-transfer &#123; 192.168.1.251; &#125;;        notify yes;        also-notify &#123; 192.168.1.251; &#125;;&#125;;EOF</code></pre><ol start="2"><li>主配置文件&#x2F;etc&#x2F;named.conf</li></ol><pre><code class="json">options &#123;        // 监听内网ip 53端口        listen-on port 53 &#123; 127.0.0.1;192.168.1.250; &#125;;        ...        // 配置开放DNS服务器给所有主机（可以设置特定主机）        allow-query     &#123; any; &#125;;        ...        // 配置客户端并发数量，不设置的话默认是100，可以根据使用情况评估        tcp-clients 50;        // 开启查询日志        querylog yes;&#125;;...// 日志配置, 为后期接入elk做准备logging &#123;        // 查询日志 ,绝对路径是/var/named/data/query.log        channel query_log &#123;                file &quot;data/query.log&quot; versions 5 size 100m;                print-time yes;                severity info;        &#125;;        channel query_syslog &#123;                syslog local0;                print-time yes;                print-category yes;                print-severity yes;                severity info;        &#125;;        category queries &#123; query_log;query_syslog; &#125;;        // 查询错误日志        channel query-errors_log &#123;                file &quot;data/query_error.log&quot; versions 10 size 100m;                print-time yes;                print-category yes;                print-severity yes;                severity debug 2;        &#125;;        category query-errors &#123; query-errors_log; &#125;;        // 所有等级日志设置        channel general_log &#123;                file &quot;data/access.log&quot; versions 5 size 100m;                print-time yes;                print-category yes;                print-severity yes;                severity info;        &#125;;        category default  &#123; general_log; &#125;;        category general  &#123; general_log; &#125;;        channel notify_log &#123;                file &quot;data/notify.log&quot; versions 2 size 20m;                print-time yes;                print-category yes;                print-severity yes;                severity dynamic;        &#125;;        category notify &#123; notify_log; &#125;;&#125;;...// 开启服务监控状态统计, 后期配合bind9-exporter + prometheus进行监控statistics-channels &#123;  inet 127.0.0.1 port 8053 allow &#123; 127.0.0.1; &#125;;&#125;;</code></pre><ol start="3"><li>安装阿里云PrivateZone同步工具<blockquote><p>更多用法参考<a href="https://help.aliyun.com/document_detail/102718.html?spm=5176.12818093.help.dexternal.57ea16d0TSEpR3&scm=20140722.S_help@@%E6%96%87%E6%A1%A3@@102718.S_os+hot.ID_102718-RL_bind9-LOC_consoleUNDhelp-OR_ser-V_2-P0_0">如何PrivateZone同步至自建DNS</a></p></blockquote></li></ol><pre><code class="bash">cd /optwget -O tools.zip -c &quot;https://dns-tool.oss-cn-beijing.aliyuncs.com/pvzone-sync-record/tools.zip?spm=a2c4g.11186623.0.0.146e6fddvxwgz8&amp;file=tools.zip&quot;unzip -q tools.zipchmod +x /opt/release/Zone_file_synccp /opt/release/Zone_file_sync /usr/bin/Zone_file_synccat &gt;/root/.bind9_sync_privatezone_config.json &lt;&lt; EOF&#123;  &quot;accessKeyId&quot;: &quot;xxxxxxx&quot;,  &quot;accessKeySecret&quot;: &quot;xxxxxxx&quot;,  &quot;zone&quot;: [    &#123;      &quot;zoneName&quot;: &quot;xuhandsome.org&quot;,      &quot;zoneId&quot;: &quot;52ac2clex4c6175e7a906b1f2a6i3917&quot;,      &quot;filePath&quot;: &quot;/var/named/xuhandsome.org.zone&quot;    &#125;  ]&#125;EOFchmod 400 /root/.bind9_sync_privatezone_config.json</code></pre><ol start="4"><li>编写实时同步脚本</li></ol><pre><code class="bash">#while true无限循环,每隔10s执行一次cat &gt;/app/scripts/sync_privatezone.sh &lt;&lt; EOF#!/bin/bashwhile true;do    /usr/sbin/rndc reload    /usr/sbin/rndc freeze xuhandsome.org    /usr/bin/Zone_file_sync -c /root/.bind9_sync_privatezone_config.json    /usr/sbin/rndc thaw xuhandsome.org    sleep 10doneEOFchmod +x /app/scripts/sync_privatezone.sh</code></pre><ol start="5"><li>添加实时同步守护进程sync_privatezone.service</li></ol><pre><code class="bash">cat &gt;/etc/systemd/system/sync_privatezone.service &lt;&lt;EOF[Unit]Description=Auto Sync Zone Configure From PrivateZoneWants=network-online.targetAfter=network-online.target[Service]Type=simpleExecStart=/app/scripts/sync_privatezone.shRestart=alwaysRestartSec=60[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable --now sync_privatezonesystemctl status sync_privatezone</code></pre><ol start="6"><li>zone文件同步内容检查<blockquote><p>可以看到已经同步下来了,由于内容敏感就不截图了</p></blockquote></li></ol><pre><code class="bash">wc -l /var/named/xuhandsome.org.zone32 /var/named/xuhandsome.org.zone</code></pre><h2 id="四、从DNS服务器配置"><a href="#四、从DNS服务器配置" class="headerlink" title="四、从DNS服务器配置"></a>四、从DNS服务器配置</h2><ol><li>&#x2F;etc&#x2F;named.rfc1912.zones中新增正向域配置, slave模式</li></ol><pre><code class="bash">cat &lt;&lt;EOF&gt;&gt; /etc/named.rfc1912.zoneszone &quot;xuhandsome.org&quot; IN &#123;        type slave;        file &quot;slaves/xuhandsome.org.zone&quot;;        masters &#123; 192.168.1.250; &#125;;&#125;;</code></pre><ol start="2"><li>主配置文件&#x2F;etc&#x2F;named.conf</li></ol><pre><code class="json">options &#123;        listen-on port 53 &#123; 127.0.0.1;192.168.1.251; &#125;;        ...        allow-query     &#123; any; &#125;;        ...        // 避免同步过来的zone文件内容乱码        masterfile-format    text;        ...        tcp-clients 50;        ...&#125;;statistics-channels &#123;  inet 127.0.0.1 port 8053 allow &#123; 127.0.0.1; &#125;;&#125;;</code></pre><ol start="3"><li>开启同步</li></ol><pre><code class="bash">named-checkconf /etc/named.conf ## 检查配置文件语法/usr/sbin/rndc reload  ##配置文件重载ls -l /var/named/slaves/xuhandsome.org.zone</code></pre><h2 id="五、客户端验证"><a href="#五、客户端验证" class="headerlink" title="五、客户端验证"></a>五、客户端验证</h2><ol><li>在阿里云PrivateZone xuhandsome.org下添加子域名ops.xuhandsome.org A记录解析到192.168.1.21</li><li>在客户端通过nslookup分别指定DNS Server为上面搭建的主从服务器解析ops.xuhandsome.org.<blockquote><p>nslookup domain DNSServer</p></blockquote></li></ol><pre><code class="bash"># 使用主DNS server解析nslookup ops.xuhandsome.org 192.168.1.250Server:192.168.1.250Address:192.168.1.250#53Name:ops.xuhandsome.orgAddress: 192.168.1.21# 使用主DNS server解析nslookup ops.xuhandsome.org 192.168.1.251Server:192.168.1.251Address:192.168.1.251#53Name:ops.xuhandsome.orgAddress: 192.168.1.21</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> centos </tag>
            
            <tag> bind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自定义Centos7.9镜像-默认5.15内核</title>
      <link href="/2022/08/31/%E8%87%AA%E5%AE%9A%E4%B9%89Centos7-9%E9%95%9C%E5%83%8F-%E9%BB%98%E8%AE%A45-15%E5%86%85%E6%A0%B8/"/>
      <url>/2022/08/31/%E8%87%AA%E5%AE%9A%E4%B9%89Centos7-9%E9%95%9C%E5%83%8F-%E9%BB%98%E8%AE%A45-15%E5%86%85%E6%A0%B8/</url>
      
        <content type="html"><![CDATA[<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><pre><code class="bash">yum install -y createrepo  genisoimage \  syslinux isomd5sum ncurses-devel bc flex</code></pre><h2 id="下载原版镜像，挂载"><a href="#下载原版镜像，挂载" class="headerlink" title="下载原版镜像，挂载"></a>下载原版镜像，挂载</h2><pre><code class="bash">cd /tmpwget -c https://mirrors.cloud.tencent.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.isomkdir -p /mnt/cdrommount -o loop /tmp/CentOS-7-x86_64-Minimal-2009.iso /mnt/cdrom</code></pre><h2 id="复制光盘内容"><a href="#复制光盘内容" class="headerlink" title="复制光盘内容"></a>复制光盘内容</h2><pre><code class="bash">mkdir /ISOcp -raf /mnt/cdrom/* /ISO</code></pre><h2 id="准备5-15-47内核及依赖包"><a href="#准备5-15-47内核及依赖包" class="headerlink" title="准备5.15.47内核及依赖包"></a>准备5.15.47内核及依赖包</h2><pre><code class="bash">cd /ISO/Packagesrm -rf kernel-*.rpmwget -c https://dl.lamp.sh/kernel/el7/kernel-ml-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/kernel-ml-devel-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/kernel-ml-headers-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/kernel-ml-tools-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/kernel-ml-tools-libs-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/kernel-ml-tools-libs-devel-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/perf-5.15.47-1.el7.x86_64.rpm \  https://dl.lamp.sh/kernel/el7/python-perf-5.15.47-1.el7.x86_64.rpmyum reinstall --downloadonly --downloaddir=./ $(rpm -qa|grep ^perl)</code></pre><h2 id="编辑bios引导文件"><a href="#编辑bios引导文件" class="headerlink" title="编辑bios引导文件"></a>编辑bios引导文件</h2><pre><code class="nginx"># 只需要修改label linux部分，menu default唯一，所以要删除check下的menu default# 忽略未修改部分label linux  menu label ^Install CentOS 7.9 (Kernel-5.15.47-1)  menu default  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS7label check  menu label Test this ^media &amp; install CentOS 7  kernel vmlinuz  append initrd=initrd.img inst.stage2=hd:LABEL=CentOS\x207\x20x86_64 rd.live.check quietmenu separator # insert an empty line# 忽略未修改部分...</code></pre><h2 id="编辑EFI启动配置文件"><a href="#编辑EFI启动配置文件" class="headerlink" title="编辑EFI启动配置文件"></a>编辑EFI启动配置文件</h2><pre><code class="nginx"># 忽略未修改部分...# 只修改menuentry 安装选项中Install CentOS这一段### BEGIN /etc/grub.d/10_linux ###menuentry &#39;Install CentOS 7.9 (Kernel-5.15.47-1)&#39; --class fedora --class gnu-linux --class gnu --class os &#123;        linuxefi /images/pxeboot/vmlinuz inst.stage2=hd:LABEL=CentOS7 quiet        initrdefi /images/pxeboot/initrd.img&#125;# 忽略未修改部分...</code></pre><h2 id="准备comps-xml"><a href="#准备comps-xml" class="headerlink" title="准备comps.xml"></a>准备comps.xml</h2><p>拷贝一个模板过来</p><pre><code class="bash">cp /mnt/cdrom/repodata/*-c7-minimal-x86_64-comps.xml /ISO/comps.xml</code></pre><p>编辑拷贝过来的comps.xml，安装自定义5.15.47内核，去掉原本的kernel-tools</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE comps PUBLIC &quot;-//CentOS//DTD Comps info//EN&quot; &quot;comps.dtd&quot;&gt;&lt;comps&gt;  &lt;group&gt;    &lt;id&gt;core&lt;/id&gt;    &lt;name&gt;Core&lt;/name&gt;    &lt;name xml:lang=&quot;af&quot;&gt;Kern&lt;/name&gt;    &lt;name xml:lang=&quot;am&quot;&gt;ማዕከላዊ ቦታ&lt;/name&gt;    &lt;name xml:lang=&quot;ar&quot;&gt;اللبّ&lt;/name&gt;    &lt;name xml:lang=&quot;as&quot;&gt;ভিত্তি&lt;/name&gt;    &lt;name xml:lang=&quot;bal&quot;&gt;هستگ&lt;/name&gt;    &lt;name xml:lang=&quot;be&quot;&gt;Падмурак&lt;/name&gt;    &lt;name xml:lang=&quot;bg&quot;&gt;Основа&lt;/name&gt;    &lt;name xml:lang=&quot;bn&quot;&gt;কোর&lt;/name&gt;    &lt;name xml:lang=&quot;bn_IN&quot;&gt;কোর&lt;/name&gt;    &lt;name xml:lang=&quot;bs&quot;&gt;Jezgra&lt;/name&gt;    &lt;name xml:lang=&quot;ca&quot;&gt;Nucli&lt;/name&gt;    &lt;name xml:lang=&quot;cs&quot;&gt;Úplný základ&lt;/name&gt;    &lt;name xml:lang=&quot;cy&quot;&gt;Craidd&lt;/name&gt;    &lt;name xml:lang=&quot;da&quot;&gt;Grundlæggende&lt;/name&gt;    &lt;name xml:lang=&quot;de&quot;&gt;Kern&lt;/name&gt;    &lt;name xml:lang=&quot;el&quot;&gt;Πυρήνας&lt;/name&gt;    &lt;name xml:lang=&quot;en_GB&quot;&gt;Core&lt;/name&gt;    &lt;name xml:lang=&quot;es&quot;&gt;Núcleo&lt;/name&gt;    &lt;name xml:lang=&quot;et&quot;&gt;Tuum&lt;/name&gt;    &lt;name xml:lang=&quot;fa&quot;&gt;اصل&lt;/name&gt;    &lt;name xml:lang=&quot;fi&quot;&gt;Keskeiset&lt;/name&gt;    &lt;name xml:lang=&quot;fr&quot;&gt;Core&lt;/name&gt;    &lt;name xml:lang=&quot;gl&quot;&gt;Núcleo&lt;/name&gt;    &lt;name xml:lang=&quot;gu&quot;&gt;મૂળ&lt;/name&gt;    &lt;name xml:lang=&quot;he&quot;&gt;ליבה&lt;/name&gt;    &lt;name xml:lang=&quot;hi&quot;&gt;कोर&lt;/name&gt;    &lt;name xml:lang=&quot;hr&quot;&gt;Jezgra&lt;/name&gt;    &lt;name xml:lang=&quot;hu&quot;&gt;Mag&lt;/name&gt;    &lt;name xml:lang=&quot;hy&quot;&gt;Հիմք&lt;/name&gt;    &lt;name xml:lang=&quot;ia&quot;&gt;Nucleo&lt;/name&gt;    &lt;name xml:lang=&quot;id&quot;&gt;Inti&lt;/name&gt;    &lt;name xml:lang=&quot;ilo&quot;&gt;Bugas&lt;/name&gt;    &lt;name xml:lang=&quot;is&quot;&gt;Lágmarkskerfi&lt;/name&gt;    &lt;name xml:lang=&quot;it&quot;&gt;Principale&lt;/name&gt;    &lt;name xml:lang=&quot;ja&quot;&gt;コア&lt;/name&gt;    &lt;name xml:lang=&quot;ka&quot;&gt;ბირთვი&lt;/name&gt;    &lt;name xml:lang=&quot;kn&quot;&gt;ಅಂತಸ್ಸಾರ&lt;/name&gt;    &lt;name xml:lang=&quot;ko&quot;&gt;핵심&lt;/name&gt;    &lt;name xml:lang=&quot;lv&quot;&gt;Pamatsistēma&lt;/name&gt;    &lt;name xml:lang=&quot;mai&quot;&gt;कोर&lt;/name&gt;    &lt;name xml:lang=&quot;mk&quot;&gt;Основни&lt;/name&gt;    &lt;name xml:lang=&quot;ml&quot;&gt;കോറ്‍&lt;/name&gt;    &lt;name xml:lang=&quot;mr&quot;&gt;कोर&lt;/name&gt;    &lt;name xml:lang=&quot;ms&quot;&gt;Teras&lt;/name&gt;    &lt;name xml:lang=&quot;nb&quot;&gt;Kjerne&lt;/name&gt;    &lt;name xml:lang=&quot;ne&quot;&gt;कोर&lt;/name&gt;    &lt;name xml:lang=&quot;nl&quot;&gt;Kern&lt;/name&gt;    &lt;name xml:lang=&quot;no&quot;&gt;Kjerne&lt;/name&gt;    &lt;name xml:lang=&quot;nso&quot;&gt;Bogare&lt;/name&gt;    &lt;name xml:lang=&quot;or&quot;&gt;ପ୍ରମୂଖ&lt;/name&gt;    &lt;name xml:lang=&quot;pa&quot;&gt;ਮੂਲ&lt;/name&gt;    &lt;name xml:lang=&quot;pl&quot;&gt;Rdzeń&lt;/name&gt;    &lt;name xml:lang=&quot;pt&quot;&gt;Núcleo&lt;/name&gt;    &lt;name xml:lang=&quot;pt_BR&quot;&gt;Núcleo&lt;/name&gt;    &lt;name xml:lang=&quot;ro&quot;&gt;Nucleu&lt;/name&gt;    &lt;name xml:lang=&quot;ru&quot;&gt;Основа&lt;/name&gt;    &lt;name xml:lang=&quot;si&quot;&gt;න්‍යෂ්ඨිය&lt;/name&gt;    &lt;name xml:lang=&quot;sk&quot;&gt;Jadro&lt;/name&gt;    &lt;name xml:lang=&quot;sl&quot;&gt;Jedro&lt;/name&gt;    &lt;name xml:lang=&quot;sq&quot;&gt;Bërthama&lt;/name&gt;    &lt;name xml:lang=&quot;sr&quot;&gt;Срж&lt;/name&gt;    &lt;name xml:lang=&quot;sr@latin&quot;&gt;Srž&lt;/name&gt;    &lt;name xml:lang=&quot;sr@Latn&quot;&gt;Srž&lt;/name&gt;    &lt;name xml:lang=&quot;sv&quot;&gt;Grund&lt;/name&gt;    &lt;name xml:lang=&quot;ta&quot;&gt;கோர்&lt;/name&gt;    &lt;name xml:lang=&quot;te&quot;&gt;అంతర్భాగం&lt;/name&gt;    &lt;name xml:lang=&quot;tg&quot;&gt;Система&lt;/name&gt;    &lt;name xml:lang=&quot;th&quot;&gt;แกนหลัก&lt;/name&gt;    &lt;name xml:lang=&quot;tr&quot;&gt;Çekirdek&lt;/name&gt;    &lt;name xml:lang=&quot;uk&quot;&gt;Основа&lt;/name&gt;    &lt;name xml:lang=&quot;ur&quot;&gt;مرکز&lt;/name&gt;    &lt;name xml:lang=&quot;vi&quot;&gt;Lõi&lt;/name&gt;    &lt;name xml:lang=&quot;zh_CN&quot;&gt;核心&lt;/name&gt;    &lt;name xml:lang=&quot;zh_TW&quot;&gt;核心&lt;/name&gt;    &lt;name xml:lang=&quot;zu&quot;&gt;Okuyikhona&lt;/name&gt;    &lt;description&gt;Smallest possible installation.&lt;/description&gt;    &lt;description xml:lang=&quot;as&quot;&gt;ন্যূনতম ইনস্টল।&lt;/description&gt;    &lt;description xml:lang=&quot;bn&quot;&gt;ন্যূনতম ইনস্টলেশন।&lt;/description&gt;    &lt;description xml:lang=&quot;bn_IN&quot;&gt;ন্যূনতম ইনস্টলেশন।&lt;/description&gt;    &lt;description xml:lang=&quot;cs&quot;&gt;Nejmenší možná instalace.&lt;/description&gt;    &lt;description xml:lang=&quot;de&quot;&gt;Kleinstmögliche Installation.&lt;/description&gt;    &lt;description xml:lang=&quot;es&quot;&gt;La instalación más pequeña posible.&lt;/description&gt;    &lt;description xml:lang=&quot;fr&quot;&gt;Plus petite installation possible.&lt;/description&gt;    &lt;description xml:lang=&quot;gu&quot;&gt;નાનામાં નાના શક્ય સ્થાપન.&lt;/description&gt;    &lt;description xml:lang=&quot;hi&quot;&gt;लघुतम संभावित संस्थापन.&lt;/description&gt;    &lt;description xml:lang=&quot;ia&quot;&gt;Le minime possibile installation.&lt;/description&gt;    &lt;description xml:lang=&quot;it&quot;&gt;Minima installazione possibile.&lt;/description&gt;    &lt;description xml:lang=&quot;ja&quot;&gt;最小限のインストール&lt;/description&gt;    &lt;description xml:lang=&quot;kn&quot;&gt;ಅತ್ಯಲ್ಪಸಾಧ್ಯ ಅನುಸ್ಥಾಪನೆ.&lt;/description&gt;    &lt;description xml:lang=&quot;ko&quot;&gt;가능한 최소 설치&lt;/description&gt;    &lt;description xml:lang=&quot;ml&quot;&gt;സാധ്യമായ ഏറ്റവും ചെറിയ ഇന്‍സ്റ്റലേഷന്‍.&lt;/description&gt;    &lt;description xml:lang=&quot;mr&quot;&gt;शक्यतया सर्वात लहान प्रतिष्ठापन.&lt;/description&gt;    &lt;description xml:lang=&quot;or&quot;&gt;କ୍ଷୁଦ୍ରତମ ସମ୍ଭାବ୍ଯ ସ୍ଥାପନା।&lt;/description&gt;    &lt;description xml:lang=&quot;pa&quot;&gt;ਘੱਟੋ-ਘੱਟ ਸੰਭਵ ਇੰਸਟਾਲੇਸ਼ਨ।&lt;/description&gt;    &lt;description xml:lang=&quot;pl&quot;&gt;Najmniejsza możliwa instalacja.&lt;/description&gt;    &lt;description xml:lang=&quot;pt_BR&quot;&gt;Menor instalação possível&lt;/description&gt;    &lt;description xml:lang=&quot;ru&quot;&gt;Минимально возможная установка&lt;/description&gt;    &lt;description xml:lang=&quot;sv&quot;&gt;Minsta möjliga installation&lt;/description&gt;    &lt;description xml:lang=&quot;ta&quot;&gt;மிகச் சிறிய செயல்படுத்தக்கூடிய நிறுவல்.&lt;/description&gt;    &lt;description xml:lang=&quot;te&quot;&gt;సాధ్యమగు అతిచిన్న సంస్థాపన.&lt;/description&gt;    &lt;description xml:lang=&quot;uk&quot;&gt;Мінімально можливе встановлення.&lt;/description&gt;    &lt;description xml:lang=&quot;zh_CN&quot;&gt;最小可能安装。&lt;/description&gt;    &lt;description xml:lang=&quot;zh_TW&quot;&gt;最小型安裝。&lt;/description&gt;    &lt;default&gt;false&lt;/default&gt;    &lt;uservisible&gt;false&lt;/uservisible&gt;    &lt;packagelist&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;audit&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;basesystem&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;bash&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;biosdevname&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;btrfs-progs&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;coreutils&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;cronie&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;curl&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;dhclient&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;e2fsprogs&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;filesystem&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;firewalld&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;glibc&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;hostname&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;initscripts&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;iproute&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;iprutils&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;iptables&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;iputils&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;irqbalance&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;kbd&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;kexec-tools&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;less&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;man-db&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;ncurses&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;openssh-clients&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;openssh-server&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;parted&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;passwd&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;plymouth&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;policycoreutils&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;procps-ng&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;rootfiles&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;rpm&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;rsyslog&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;selinux-policy-targeted&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;setup&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;shadow-utils&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;sudo&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;systemd&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;tar&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;tuned&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;util-linux&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;vim-minimal&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;xfsprogs&lt;/packagereq&gt;      &lt;packagereq type=&quot;mandatory&quot;&gt;yum&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;aic94xx-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;alsa-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;bfa-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;dracut-config-rescue&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;ivtv-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl100-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl1000-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl105-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl135-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl2000-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl2030-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl3160-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl3945-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl4965-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl5000-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl5150-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl6000-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl6000g2a-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl6000g2b-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl6050-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl7260-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;iwl7265-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;kernel-ml&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;kernel-ml-devel&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;kernel-ml-headers&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;kernel-ml-tools&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;libertas-sd8686-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;libertas-sd8787-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;libertas-usb8388-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;libsysfs&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;linux-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;microcode_ctl&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;NetworkManager&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;NetworkManager-team&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;NetworkManager-tui&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;NetworkManager-wifi&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;postfix&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;ql2100-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;ql2200-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;ql23xx-firmware&lt;/packagereq&gt;      &lt;packagereq type=&quot;default&quot;&gt;rdma&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;dracut-config-generic&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;dracut-fips&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;dracut-fips-aesni&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;dracut-network&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;openssh-keycat&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;selinux-policy-mls&lt;/packagereq&gt;      &lt;packagereq type=&quot;optional&quot;&gt;tboot&lt;/packagereq&gt;    &lt;/packagelist&gt;  &lt;/group&gt;  &lt;environment&gt;    &lt;id&gt;minimal&lt;/id&gt;    &lt;name&gt;Minimal Install&lt;/name&gt;    &lt;name xml:lang=&quot;as&quot;&gt;নূন্যতম ইনস্টল&lt;/name&gt;    &lt;name xml:lang=&quot;bn_IN&quot;&gt;ন্যূনতম ইনস্টল&lt;/name&gt;    &lt;name xml:lang=&quot;cs&quot;&gt;Minimální instalace&lt;/name&gt;    &lt;name xml:lang=&quot;de&quot;&gt;Minimale Installation&lt;/name&gt;    &lt;name xml:lang=&quot;es&quot;&gt;Instalación mínima&lt;/name&gt;    &lt;name xml:lang=&quot;fr&quot;&gt;Installation minimale&lt;/name&gt;    &lt;name xml:lang=&quot;gu&quot;&gt;ન્યૂનતમ સ્થાપન&lt;/name&gt;    &lt;name xml:lang=&quot;hi&quot;&gt;न्यूनतम संस्थापन&lt;/name&gt;    &lt;name xml:lang=&quot;it&quot;&gt;Installazione minima&lt;/name&gt;    &lt;name xml:lang=&quot;ja&quot;&gt;最小限のインストール&lt;/name&gt;    &lt;name xml:lang=&quot;kn&quot;&gt;ಕನಿಷ್ಟ ಅನುಸ್ಥಾಪನೆ&lt;/name&gt;    &lt;name xml:lang=&quot;ko&quot;&gt;최소 설치&lt;/name&gt;    &lt;name xml:lang=&quot;ml&quot;&gt;ഏറ്റവും കുറഞ്ഞ ഇന്‍സ്റ്റോള്‍&lt;/name&gt;    &lt;name xml:lang=&quot;mr&quot;&gt;किमान इंस्टॉल&lt;/name&gt;    &lt;name xml:lang=&quot;or&quot;&gt;ସର୍ବନିମ୍ନ ସ୍ଥାପନ&lt;/name&gt;    &lt;name xml:lang=&quot;pa&quot;&gt;ਘੱਟ ਤੋਂ ਘੱਟ ਇੰਸਟਾਲ&lt;/name&gt;    &lt;name xml:lang=&quot;pl&quot;&gt;Minimalna instalacja&lt;/name&gt;    &lt;name xml:lang=&quot;pt_BR&quot;&gt;Instalações Mínimas&lt;/name&gt;    &lt;name xml:lang=&quot;ru&quot;&gt;Минимальная установка&lt;/name&gt;    &lt;name xml:lang=&quot;ta&quot;&gt;குறைந்தபட்ச நிறுவல்&lt;/name&gt;    &lt;name xml:lang=&quot;te&quot;&gt;కనీసపు సంస్థాపన&lt;/name&gt;    &lt;name xml:lang=&quot;uk&quot;&gt;Мінімальна система&lt;/name&gt;    &lt;name xml:lang=&quot;zh_CN&quot;&gt;最小安装&lt;/name&gt;    &lt;name xml:lang=&quot;zh_TW&quot;&gt;最小型安裝&lt;/name&gt;    &lt;description&gt;Basic functionality.&lt;/description&gt;    &lt;description xml:lang=&quot;as&quot;&gt;মৌলি কাৰ্য্যকৰীতা।&lt;/description&gt;    &lt;description xml:lang=&quot;bn_IN&quot;&gt;প্রাথমিক বৈশিষ্ট্য।&lt;/description&gt;    &lt;description xml:lang=&quot;cs&quot;&gt;Základní funkcionalita.&lt;/description&gt;    &lt;description xml:lang=&quot;de&quot;&gt;Grundlegende Funktionalität.&lt;/description&gt;    &lt;description xml:lang=&quot;es&quot;&gt;Funcionalidad básica.&lt;/description&gt;    &lt;description xml:lang=&quot;fr&quot;&gt;Fonctionnalité de base.&lt;/description&gt;    &lt;description xml:lang=&quot;gu&quot;&gt;મૂળભૂત વિધેય.&lt;/description&gt;    &lt;description xml:lang=&quot;hi&quot;&gt;मौलिक प्रकार्यात्मकता.&lt;/description&gt;    &lt;description xml:lang=&quot;it&quot;&gt;Funzione di base.&lt;/description&gt;    &lt;description xml:lang=&quot;ja&quot;&gt;基本的な機能です。&lt;/description&gt;    &lt;description xml:lang=&quot;kn&quot;&gt;ಮೂಲಭೂತ ಕ್ರಿಯಾಶೀಲತೆ.&lt;/description&gt;    &lt;description xml:lang=&quot;ko&quot;&gt;기본적인 기능입니다.&lt;/description&gt;    &lt;description xml:lang=&quot;ml&quot;&gt;അടിസ്ഥാന പ്രവൃത്തിവിശേഷണം.&lt;/description&gt;    &lt;description xml:lang=&quot;mr&quot;&gt;मूळ कार्यक्षमता.&lt;/description&gt;    &lt;description xml:lang=&quot;or&quot;&gt;ସାଧାରଣ କାର୍ଯ୍ୟକାରିତା।&lt;/description&gt;    &lt;description xml:lang=&quot;pa&quot;&gt;ਮੁੱਢਲੀ ਕਾਰਜਸ਼ੀਲਤਾ।&lt;/description&gt;    &lt;description xml:lang=&quot;pl&quot;&gt;Podstawowa funkcjonalność.&lt;/description&gt;    &lt;description xml:lang=&quot;pt_BR&quot;&gt;Função básica&lt;/description&gt;    &lt;description xml:lang=&quot;ru&quot;&gt;Базовая функциональность.&lt;/description&gt;    &lt;description xml:lang=&quot;ta&quot;&gt;அடிப்படை செயலம்சம்.&lt;/description&gt;    &lt;description xml:lang=&quot;te&quot;&gt;ప్రాథమిక ఫంక్షనాలిటి.&lt;/description&gt;    &lt;description xml:lang=&quot;uk&quot;&gt;Основні можливості.&lt;/description&gt;    &lt;description xml:lang=&quot;zh_CN&quot;&gt;基本功能。&lt;/description&gt;    &lt;description xml:lang=&quot;zh_TW&quot;&gt;基本功能。&lt;/description&gt;    &lt;display_order&gt;5&lt;/display_order&gt;    &lt;grouplist&gt;      &lt;groupid&gt;core&lt;/groupid&gt;      &lt;groupid&gt;core&lt;/groupid&gt;    &lt;/grouplist&gt;  &lt;/environment&gt;&lt;/comps&gt;</code></pre><h2 id="生成repodata"><a href="#生成repodata" class="headerlink" title="生成repodata"></a>生成repodata</h2><pre><code class="bash">cd /ISOcreaterepo -g comps.xml .</code></pre><h2 id="打包ISO镜像"><a href="#打包ISO镜像" class="headerlink" title="打包ISO镜像"></a>打包ISO镜像</h2><pre><code class="bash">genisoimage -joliet-long -V CentOS7 -o CentOS-7.9-kernel-5.15.47.iso -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -R -J -v -cache-inodes -T -eltorito-alt-boot -e images/efiboot.img -no-emul-boot /ISO</code></pre><pre><code class="bash">cd /ISOisohybrid -v CentOS-7.9-kernel-5.15.47.isoisohybrid: Warning: more than 1024 cylinders: 1054isohybrid: Not all BIOSes will be able to boot this device</code></pre><pre><code class="bash">implantisomd5 /ISO/CentOS-7.9-kernel-5.15.47.iso</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> kernel </tag>
            
            <tag> centos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7.9编译5.15.47版本内核</title>
      <link href="/2022/08/31/5-15-47/"/>
      <url>/2022/08/31/5-15-47/</url>
      
        <content type="html"><![CDATA[<h2 id="gcc更新"><a href="#gcc更新" class="headerlink" title="gcc更新"></a>gcc更新</h2><pre><code class="bash">yum install centos-release-scl -yyum install devtoolset-7-gcc* -yscl enable devtoolset-7 bashgcc --version</code></pre><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><pre><code class="bash">mkdir -p /root/kernelcd /root/kernelwget -c https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.15.47.tar.xztar xvJf linux-5.15.47.tar.xzcd linux-5.15.47make mrpropercp /boot/config-5.15.45-1.el7.x86_64  ./.configmake menuconfigmake oldconfigmake bzImage -j8 &amp;&amp; make modules -j8 &amp;&amp; make modules_install INSTALL_MOD_PATH=/root/modinstall/ INSTALL_MOD_STRIP=1 -j8</code></pre><pre><code class="bash">make rpm-pkg -j8ll /root/rpmbuild/RPMS/x86_64/kernel*.rpm-rw-r--r--. 1 root root 1186206312 Jun 16 04:52 /root/rpmbuild/RPMS/x86_64/kernel-5.15.47-1.x86_64.rpm-rw-r--r--. 1 root root  169605404 Jun 16 04:53 /root/rpmbuild/RPMS/x86_64/kernel-devel-5.15.47-1.x86_64.rpm-rw-r--r--. 1 root root    1465684 Jun 16 04:52 /root/rpmbuild/RPMS/x86_64/kernel-headers-5.15.47-1.x86_64.rpm</code></pre><h2 id="更新内核"><a href="#更新内核" class="headerlink" title="更新内核"></a>更新内核</h2><pre><code class="bash">rpm -ivh kernel-5.15.47-1.x86_64.rpmawk -F\&#39; &#39;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&#39; /boot/grub2/grub.cfg #查看新内核启动序号grub2-set-default 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> kernel </tag>
            
            <tag> centos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Elasticsearch精确索引权限管理</title>
      <link href="/2022/08/31/Elasticsearch%E7%B2%BE%E7%A1%AE%E7%B4%A2%E5%BC%95%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"/>
      <url>/2022/08/31/Elasticsearch%E7%B2%BE%E7%A1%AE%E7%B4%A2%E5%BC%95%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="权限及用户"><a href="#权限及用户" class="headerlink" title="权限及用户"></a>权限及用户</h2><h3 id="创建“read-only”role"><a href="#创建“read-only”role" class="headerlink" title="创建“read_only”role"></a>创建“read_only”role</h3><blockquote><p>这里的role具有list indices及get indices内容的权限，如果只给一个read，是不能list的</p></blockquote><pre><code class="bash">curl -XPOST -H &#39;Content-Type: application/json&#39; -u elastic:xxx http://10.0.2.1:9200/_security/role/read_only -d &#39;&#123;    &quot;cluster&quot;: [        &quot;cluster:monitor/health&quot;,        &quot;monitor&quot;    ],    &quot;indices&quot;: [        &#123;            &quot;names&quot;: [                &quot;*&quot;            ],            &quot;privileges&quot;: [                &quot;read&quot;,                &quot;monitor&quot;,                &quot;indices:admin/get&quot;,                &quot;indices:monitor/settings/get&quot;,                &quot;indices:monitor/stats&quot;,                &quot;indices:admin/aliases/get&quot;            ]        &#125;    ]&#125;&#39;</code></pre><h3 id="创建esread账号，绑定read-only-role"><a href="#创建esread账号，绑定read-only-role" class="headerlink" title="创建esread账号，绑定read_only role"></a>创建esread账号，绑定read_only role</h3><pre><code class="bash">curl -XPOST -H &#39;Content-Type: application/json&#39; -u elastic:xxx http://10.0.2.1:9200/_security/user/esread -d &#39;&#123;  &quot;password&quot;: &quot;GfWVqGWo0gU3PdHz&quot;,  &quot;roles&quot;: [&quot;read_only&quot;]&#125;&#39;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker运维常用命令集合</title>
      <link href="/2022/08/31/Docker%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/"/>
      <url>/2022/08/31/Docker%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h3 id="清理"><a href="#清理" class="headerlink" title="清理"></a>清理</h3><p><strong>删除所有dangling数据卷(即无用的volume)：</strong></p><pre><code class="bash">docker volume prune</code></pre><p><strong>删除已停止的容器、dangling 镜像、未被容器引用的 network 和构建过程中的 cache</strong></p><pre><code class="bash">docker system prune</code></pre><p><strong>删除所有dangling镜像(即无tag的镜像)：</strong></p><pre><code class="bash">for i in $(docker images|grep &quot;none&quot;|awk &#39;&#123;print $3&#125;&#39;);do docker rmi -f $i;done</code></pre><p><strong>删除所有关闭的容器:</strong></p><pre><code class="bash">docker ps -a | grep Exit | cut -d &#39; &#39; -f 1 | xargs docker rm</code></pre><p><strong>清空容器日志:</strong></p><pre><code class="powershell">#!/bin/bashfor container_id in $(docker ps -a --filter=&quot;name=$name&quot; -q); do    file=$(docker inspect -f &#39;&#123;&#123;.LogPath&#125;&#125;&#39; $container_id)    sudo ls -lh $file    if [ -f $file ]; then        echo $file        cat /dev/null &gt;$file    fidone</code></pre><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><p><strong>对应docker run –ip&#x3D;,获取所有容器设置的IP地址</strong></p><pre><code class="powershell">for i in $(docker ps | grep -v CON | awk &#39;&#123;print $1&#125;&#39;); do    echo $i    docker inspect $i | grep IPAddress | tail -n 1done</code></pre><h3 id="docker进程日志"><a href="#docker进程日志" class="headerlink" title="docker进程日志"></a>docker进程日志</h3><pre><code class="bash">journalctl /usr/bin/dockerd</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7修改网卡名称</title>
      <link href="/2022/08/31/Centos7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E5%90%8D%E7%A7%B0/"/>
      <url>/2022/08/31/Centos7%E4%BF%AE%E6%94%B9%E7%BD%91%E5%8D%A1%E5%90%8D%E7%A7%B0/</url>
      
        <content type="html"><![CDATA[<p><a href="https://developer.aliyun.com/article/609587">Centos7网卡名称怎么来的</a><br>修改网卡名<a href="https://cloud.tencent.com/developer/article/1721108">参考</a></p><pre><code class="bash"># 当前网卡名 ens18[root@localhost ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: ens18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 46:23:2a:29:37:4e brd ff:ff:ff:ff:ff:ff    inet 10.22.13.80/24 brd 10.22.13.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::4423:2aff:fe29:374e/64 scope link       valid_lft forever preferred_lft forever</code></pre><pre><code class="bash">cd /etc/sysconfig/network-scriptsmv ifcfg-ens18 ifcfg-eth0sed -i &#39;s/ens18/eth0/g&#39; ifcfg-eth0sed -i &#39;s/GRUB_CMDLINE_LINUX=\&quot;/GRUB_CMDLINE_LINUX=\&quot;net.ifnames=0 /g&#39; /etc/default/grub#重新生成GRUB配置并更新内核参数grub2-mkconfig -o /boot/grub2/grub.cfgreboot</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> centos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Centos7.9 + Ceph-ansible安装Ceph Octopus</title>
      <link href="/2022/08/31/Octopus/"/>
      <url>/2022/08/31/Octopus/</url>
      
        <content type="html"><![CDATA[<h1 id="一、部署信息"><a href="#一、部署信息" class="headerlink" title="一、部署信息"></a>一、部署信息</h1><ul><li>Ceph版本号：Octopus &#x2F; 15.2.15</li><li>Ceph-Ansible版本：origin&#x2F;stable-5.0</li><li>Centos7.9 2009 + 5.15 Kernel</li><li>所有节点配置业务、集群通信双网口bond0.2049 + bond1.2060</li><li>所有osd节点12块2.2TB裸盘</li></ul><h1 id="二、节点规划"><a href="#二、节点规划" class="headerlink" title="二、节点规划"></a>二、节点规划</h1><table><thead><tr><th>主机地址（业务口）</th><th>网关</th><th>集群内地址</th><th>主机名称</th><th>主机角色</th></tr></thead><tbody><tr><td>IP：10.10.0.1&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.1&#x2F;22</td><td>ceph-mgr01</td><td>mgr，mon，osd，rgw，rbd，cephfs，mds</td></tr><tr><td>IP：10.10.0.2&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.2&#x2F;22</td><td>ceph-mgr02</td><td>mgr，mon，osd，rgw，rbd，cephfs，mds</td></tr><tr><td>IP：10.10.0.3&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.3&#x2F;22</td><td>ceph-mgr03</td><td>mgr，mon，osd，rgw，rbd，cephfs，mds</td></tr><tr><td>IP：10.10.0.4&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.4&#x2F;22</td><td>ceph-node01</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.5&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.5&#x2F;22</td><td>ceph-node02</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.6&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.6&#x2F;22</td><td>ceph-node03</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.7&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.7&#x2F;22</td><td>ceph-node04</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.8&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.8&#x2F;22</td><td>ceph-node05</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.9&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.9&#x2F;22</td><td>ceph-node06</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>IP：10.10.0.10&#x2F;22</td><td>10.10.3.254</td><td>IP：10.60.0.10&#x2F;22</td><td>ceph-node07</td><td>osd，rgw，rbd，cephfs</td></tr><tr><td>VIP：10.10.0.250&#x2F;22</td><td>10.10.3.254</td><td></td><td></td><td></td></tr><tr><td>ceph-mgr01-3、ceph-node01-7</td><td>keepalived，haproxy</td><td></td><td></td><td></td></tr></tbody></table><h1 id="三、执行部署"><a href="#三、执行部署" class="headerlink" title="三、执行部署"></a>三、执行部署</h1><h2 id="1-基础准备"><a href="#1-基础准备" class="headerlink" title="1.  基础准备"></a>1.  基础准备</h2><pre><code class="bash"># disable selinux，所有节点都要执行setenforce 0sed -i -e &#39;s/SELINUX=.*/SELINUX=disabled/g&#39; /etc/selinux/config# stop firewalld，所有节点都要执行systemctl disable --now firewalldsystemctl disable --now NetworkManager# hosts，所有节点都要执行cat &gt; /etc/hosts &lt;&lt; HOSTS127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain610.10.0.1  ceph110.10.0.2  ceph210.10.0.3  ceph310.10.0.4  ceph410.10.0.5  ceph510.10.0.6  ceph610.10.0.7  ceph710.10.0.8  ceph810.10.0.9  ceph910.10.0.10 ceph1010.10.0.250 oss.xuhandsome.org #rgw存储桶访问入口HOSTS# 基础rpm包安装，所有节点都要执行yum install -y -q epel-releaseyum install -y -q python3 python3-pip python36-PyYAML python36-six python36-netaddrpip3 install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simplepython3 -m pip install ansible==2.9 -i https://pypi.tuna.tsinghua.edu.cn/simplepython3 -m pip install six pecan werkzeug -i https://pypi.tuna.tsinghua.edu.cn/simple# 配置ssh免密登录，部署机执行ssh-keygenfor i in &#123;1..10&#125;;do ssh-copy-id ceph$i;done# 下载ceph-ansible项目代码，部署机执行git -b stable-5.0 clone https://github.com/ceph/ceph-ansible.gitcd ceph-ansiblecp site.yml.sample site.yml</code></pre><h2 id="2-部署配置文件准备"><a href="#2-部署配置文件准备" class="headerlink" title="2. 部署配置文件准备"></a>2. 部署配置文件准备</h2><h3 id="ceph-ansible-x2F-group-vars-x2F-all-yml"><a href="#ceph-ansible-x2F-group-vars-x2F-all-yml" class="headerlink" title="ceph-ansible&#x2F;group_vars&#x2F;all.yml"></a>ceph-ansible&#x2F;group_vars&#x2F;all.yml</h3><pre><code class="yaml">cat ceph-ansible/group_vars/all.yml | grep -vE &#39;(^#|^$)&#39;---dummy:configure_firewall: Falseceph_origin: repositoryceph_repository: communityceph_mirror: http://mirrors.163.com/cephceph_stable_key: &quot;&#123;&#123; ceph_mirror &#125;&#125;/keys/release.asc&quot;ceph_stable_release: octopusceph_stable_repo: &quot;&#123;&#123; ceph_mirror &#125;&#125;/rpm-&#123;&#123; ceph_stable_release &#125;&#125;&quot;cephx: truemonitor_interface: bond0.2049ip_version: ipv4pg_autoscale_mode: Truepublic_network: 10.10.0.0/22cluster_network: 10.60.0.0/24osd_objectstore: bluestoreosd_auto_discovery: trueradosgw_civetweb_port: 8080radosgw_interface: bond0.2049radosgw_address: &quot;&#123;&#123; inventory_hostname &#125;&#125;&quot;rgw_multisite: truergw_multisite_proto: httpradosgw_frontend_port: 8081rgw_instances:  - instance_name: xuhandsome    rgw_zone: huishan    rgw_zonemaster: true    rgw_zonesecondary: false    rgw_zonegroup: jiangsu # should be set by the user    rgw_zonegroupmaster: true    rgw_zone_user: root    rgw_zone_user_display_name: &quot;Root&quot;    rgw_realm: cn # should be set by the user    rgw_multisite_proto: &quot;&#123;&#123; rgw_multisite_proto &#125;&#125;&quot;    radosgw_address: &quot;&#123;&#123; radosgw_address &#125;&#125;&quot;    radosgw_frontend_port: &quot;&#123;&#123; radosgw_frontend_port &#125;&#125;&quot;    system_access_key: p9wq0vsnRi7sTSz0Ls6L # should be re-created by the user    system_secret_key: 4w4I3bOX0bFZ6YqCWOoDhJ03eTHIAUMohf6wDZC6 # should be re-created by the user    # Multi-site remote pull URL variables    #rgw_pull_port: &quot;&#123;&#123; radosgw_frontend_port &#125;&#125;&quot;    #rgw_pull_proto: &quot;http&quot; # should be the same as rgw_multisite_proto for the master zone cluster    #rgw_pullhost: localhost # rgw_pullhost only needs to be declared if there is a zone secondary.dashboard_enabled: falsegrafana_admin_password: admin</code></pre><h3 id="ceph-ansible-x2F-group-vars-x2F-osds-yml"><a href="#ceph-ansible-x2F-group-vars-x2F-osds-yml" class="headerlink" title="ceph-ansible&#x2F;group_vars&#x2F;osds.yml"></a>ceph-ansible&#x2F;group_vars&#x2F;osds.yml</h3><pre><code class="yaml">---dummy:devices:  - /dev/sdb  - /dev/sdc  - /dev/sdd  - /dev/sde  - /dev/sdf  - /dev/sdg  - /dev/sdh  - /dev/sdi  - /dev/sdj  - /dev/sdk  - /dev/sdl  - /dev/sdm</code></pre><h3 id="ceph-ansible-x2F-group-vars-x2F-mgrs-yml"><a href="#ceph-ansible-x2F-group-vars-x2F-mgrs-yml" class="headerlink" title="ceph-ansible&#x2F;group_vars&#x2F;mgrs.yml"></a>ceph-ansible&#x2F;group_vars&#x2F;mgrs.yml</h3><pre><code class="yaml">cat ceph-ansible/group_vars/mgrs.yml | grep -vE &#39;(^#|^$)&#39;---dummy:copy_admin_key: true</code></pre><h3 id="ceph-ansible-x2F-group-vars-x2F-rgwloadbalancers-yml"><a href="#ceph-ansible-x2F-group-vars-x2F-rgwloadbalancers-yml" class="headerlink" title="ceph-ansible&#x2F;group_vars&#x2F;rgwloadbalancers.yml"></a>ceph-ansible&#x2F;group_vars&#x2F;rgwloadbalancers.yml</h3><pre><code class="yaml">cat ceph-ansible/group_vars/rgwloadbalancers.yml | grep -vE &#39;(^#|^$)&#39;---dummy:haproxy_frontend_port: 80haproxy_frontend_ssl_port: 443haproxy_ssl_dh_param: 4096haproxy_ssl_ciphers: - EECDH+AESGCM - EDH+AESGCMhaproxy_ssl_options: - no-sslv3 - no-tlsv10 - no-tlsv11 - no-tls-ticketsvirtual_ips:  - 10.10.0.250virtual_ip_netmask: 22virtual_ip_interface: bond0.2049</code></pre><h3 id="ceph-ansible-x2F-hosts"><a href="#ceph-ansible-x2F-hosts" class="headerlink" title="ceph-ansible&#x2F;hosts"></a>ceph-ansible&#x2F;hosts</h3><pre><code class="bash">cat ceph-ansible/hosts[mons]10.10.0.110.10.0.210.10.0.3[mgrs]10.10.0.110.10.0.210.10.0.3[mdss]10.10.0.110.10.0.210.10.0.3[osds]10.10.0.110.10.0.210.10.0.310.10.0.410.10.0.510.10.0.610.10.0.710.10.0.810.10.0.910.10.0.10[rgws]10.10.0.110.10.0.210.10.0.310.10.0.410.10.0.510.10.0.610.10.0.710.10.0.810.10.0.910.10.0.10[rbdmirrors]10.10.0.110.10.0.210.10.0.310.10.0.410.10.0.510.10.0.610.10.0.710.10.0.810.10.0.910.10.0.10[nfss]10.10.0.110.10.0.210.10.0.310.10.0.410.10.0.510.10.0.610.10.0.710.10.0.810.10.0.910.10.0.10[grafana-server]10.10.0.1[clients]10.10.0.110.10.0.210.10.0.3[rgwloadbalancers]10.10.0.110.10.0.210.10.0.310.10.0.410.10.0.510.10.0.610.10.0.710.10.0.810.10.0.910.10.0.10</code></pre><h2 id="3-执行部署"><a href="#3-执行部署" class="headerlink" title="3.执行部署"></a>3.执行部署</h2><pre><code class="bash">cd ceph-ansibleansible-playbook -i hosts site.yml</code></pre><h2 id="四、部署后验证"><a href="#四、部署后验证" class="headerlink" title="四、部署后验证"></a>四、部署后验证</h2><h3 id="1-集群健康及常见WARN处理"><a href="#1-集群健康及常见WARN处理" class="headerlink" title="1. 集群健康及常见WARN处理"></a>1. 集群健康及常见WARN处理</h3><pre><code class="bash">ceph -s  cluster:    id:     6d380cdd-3d1b-4400-a718-ad6d43ec5edd    health: HEALTH_OK  services:    mon:        3 daemons, quorum Ceph-01,Ceph-02,Ceph-03 (age 47h)    mgr:        Ceph-03(active, since 43h), standbys: Ceph-02, Ceph-01    mds:        cephfs:1 &#123;0=Ceph-01=up:active&#125; 2 up:standby    osd:        108 osds: 108 up (since 2d), 108 in (since 3d)    rbd-mirror: 9 daemons active (65808, 69285, 69303, 69312, 75487, 78568, 78604, 85223, 88472)    rgw:        18 daemons active (Ceph-01.xuhandsome, Ceph-01.rgw0, Ceph-02.xuhandsome, Ceph-02.rgw0, Ceph-03.xuhandsome, Ceph-03.rgw0, Ceph-04.xuhandsome, Ceph-04.rgw0, Ceph-05.xuhandsome, Ceph-05.rgw0, Ceph-06.xuhandsome, Ceph-06.rgw0, Ceph-07.xuhandsome, Ceph-07.rgw0, Ceph-09.xuhandsome, Ceph-09.rgw0, Ceph-10.xuhandsome, Ceph-10.rgw0)    rgw-nfs:    9 daemons active (Ceph-01, Ceph-02, Ceph-03, Ceph-04, Ceph-05, Ceph-06, Ceph-07, Ceph-09, Ceph-10)  task status:  data:    pools:   16 pools, 385 pgs    objects: 825 objects, 634 MiB    usage:   118 GiB used, 236 TiB / 236 TiB avail    pgs:     385 active+clean</code></pre><h4 id="a-HEALTH-WARN-clock-skew-detected-on-mon-ceph-02"><a href="#a-HEALTH-WARN-clock-skew-detected-on-mon-ceph-02" class="headerlink" title="a. HEALTH_WARN: clock skew detected on mon.ceph-02"></a>a. HEALTH_WARN: clock skew detected on mon.ceph-02</h4><p>这是集群时间不同步，前面ceph-ansible安装时已经安装了chronyd客户端，修改为同步阿里云ntp服务器</p><ol><li>修改配置文件</li></ol><pre><code class="bash">cat /etc/chrony.conf |grep -vE &#39;(^#|^$)&#39;server ntp.aliyun.com iburstserver ntp1.aliyun.com iburstserver ntp2.aliyun.com iburstserver ntp3.aliyun.com iburstserver ntp4.aliyun.com iburstserver ntp5.aliyun.com iburstserver ntp6.aliyun.com iburstserver ntp7.aliyun.com iburstserver ntp.cloud.aliyuncs.com iburstserver ntp7.cloud.aliyuncs.com iburstserver ntp8.cloud.aliyuncs.com iburstserver ntp9.cloud.aliyuncs.com iburstserver ntp10.cloud.aliyuncs.com iburstserver ntp11.cloud.aliyuncs.com iburstserver ntp12.cloud.aliyuncs.com iburstdriftfile /var/lib/chrony/driftmakestep 1.0 3rtcsyncstratumweight 0allow 0.0.0.0/0logdir /var/log/chronylogchange 1</code></pre><ol start="2"><li>重启chronyd服务</li></ol><pre><code class="bash">systemctl restart chronydchronyc sources</code></pre><ol start="3"><li>重启ceph-mon服务</li></ol><pre><code class="bash">systemctl restart ceph-mon@&#123;nodename&#125;systemctl restart ceph-mon.target</code></pre><h4 id="b-HEALTH-WARN-mons-are-allowing-insecure-global-id-reclaim"><a href="#b-HEALTH-WARN-mons-are-allowing-insecure-global-id-reclaim" class="headerlink" title="b. HEALTH_WARN:  mons are allowing insecure global_id reclaim"></a>b. HEALTH_WARN:  mons are allowing insecure global_id reclaim</h4><p>查询相关文档在此<a href="https://ceph.io/releases/v14-2-20-nautilus-released/">文档</a>中发现，14.2.20修复了一个ceph身份验证框架中的安全漏洞，增加了相关警告。相关信息，请参考<a href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/">CVE-2021-20288</a>。可以通过设置将其禁用，文档建议升级到O版后在禁用此设置</p><pre><code class="bash">ceph config set mon mon_warn_on_insecure_global_id_reclaim_allowed falseceph config set mon auth_allow_insecure_global_id_reclaim false</code></pre><h3 id="2-Bucket及VIP验证"><a href="#2-Bucket及VIP验证" class="headerlink" title="2. Bucket及VIP验证"></a>2. Bucket及VIP验证</h3><h4 id="a-VIP连通性"><a href="#a-VIP连通性" class="headerlink" title="a. VIP连通性"></a>a. VIP连通性</h4><pre><code class="bash">ping 10.10.0.250curl http://10.10.0.250&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</code></pre><h4 id="b-安装S3客户端"><a href="#b-安装S3客户端" class="headerlink" title="b. 安装S3客户端"></a>b. 安装S3客户端</h4><pre><code class="bash">yum install s3cmd -y</code></pre><pre><code class="bash">cat &lt;&lt; EOF &gt; ~/.s3cfg[default]access_key = p9wq3vsnRg7xTSzoLs6Laccess_token =add_encoding_exts =add_headers =bucket_location = USca_certs_file =cache_file =check_ssl_certificate = Truecheck_ssl_hostname = Truecloudfront_host = oss.xuhandsome.orgconnection_max_age = 5connection_pooling = Truecontent_disposition =content_type =default_mime_type = binary/octet-streamdelay_updates = Falsedelete_after = Falsedelete_after_fetch = Falsedelete_removed = Falsedry_run = Falseenable_multipart = Trueencrypt = Falseexpiry_date =expiry_days =expiry_prefix =follow_symlinks = Falseforce = Falseget_continue = Falsegpg_command = /usr/bin/gpggpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_passphrase =guess_mime_type = Truehost_base = oss.xuhandsome.org#host_bucket = %(bucket)s.s3.amazonaws.comhost_bucket = oss.xuhandsome.org/%(bucket)human_readable_sizes = Falseinvalidate_default_index_on_cf = Falseinvalidate_default_index_root_on_cf = Trueinvalidate_on_cf = Falsekms_key =limit = -1limitrate = 0list_md5 = Falselog_target_prefix =long_listing = Falsemax_delete = -1mime_type =multipart_chunk_size_mb = 15multipart_copy_chunk_size_mb = 1024multipart_max_chunks = 10000preserve_attrs = Trueprogress_meter = Trueproxy_host =proxy_port = 0public_url_use_https = Falseput_continue = Falserecursive = Falserecv_chunk = 65536reduced_redundancy = Falserequester_pays = Falserestore_days = 1restore_priority = Standardsecret_key = 4w4I3bOX0bFZ6YqCWOoDhJ03eTHIAUMohf6wDZC6send_chunk = 65536server_side_encryption = Falsesignature_v2 = Falsesignurl_use_https = Falsesimpledb_host = sdb.amazonaws.comskip_existing = Falsesocket_timeout = 300ssl_client_cert_file =ssl_client_key_file =stats = Falsestop_on_error = Falsestorage_class =throttle_max = 100upload_id =urlencoding_mode = normaluse_http_expect = Falseuse_https = Falseuse_mime_magic = Trueverbosity = WARNINGwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/website_error =website_index = index.htmlEOF</code></pre><h4 id="c-创建bucket"><a href="#c-创建bucket" class="headerlink" title="c. 创建bucket"></a>c. 创建bucket</h4><pre><code class="bash">s3cmd mb s3://test-bucketBucket &#39;s3://test-bucket/&#39; createds3cmd ls2022-07-17 10:16  s3://test-bucket</code></pre><h4 id="d-上传测试"><a href="#d-上传测试" class="headerlink" title="d. 上传测试"></a>d. 上传测试</h4><pre><code class="bash"># 们ceph-ansible安装配置文件上传上去s3cmd put --recursive /opt/ansible s3://test-bucket/</code></pre><h4 id="c-下载文件"><a href="#c-下载文件" class="headerlink" title="c. 下载文件"></a>c. 下载文件</h4><pre><code class="bash">s3cmd get s3://test-bucket/ceph-ansible/hostsdownload: &#39;s3://test-bucket/ceph-ansible/hosts&#39; -&gt; &#39;./hosts&#39;  [1 of 1] 448 of 448   100% in    0s    37.97 KB/s  done</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
